{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"*K* nearest neighbors\"\n",
        "subtitle: \"IN2004B: Generation of Value with Data Analytics\"\n",
        "author: \n",
        "  - name: Alan R. Vazquez\n",
        "    affiliations:\n",
        "      - name: Department of Industrial Engineering\n",
        "format: \n",
        "  revealjs:\n",
        "    chalkboard: false\n",
        "    multiplex: false\n",
        "    footer: \"Tecnologico de Monterrey\"\n",
        "    logo: IN2004B_logo.png\n",
        "    css: style.css\n",
        "    slide-number: True\n",
        "    html-math-method: mathjax\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "## Load the libraries\n",
        "\n",
        "Before we start, let's import the data science libraries into Python.\n"
      ],
      "id": "7fe34587"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "c13760d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we use specific functions from the **pandas**, **matplotlib**, **seaborn** and **sklearn** libraries in Python.\n",
        "\n",
        "## *K*-nearest neighbors (KNN)\n",
        "\n",
        "</br>\n",
        "\n",
        "KNN a supervised learning algorithm that uses proximity to make classifications or predictions about the clustering of a single data point.\n",
        "\n",
        "-   **Basic idea**: Predict a new observation using the *K* closest observations in the training dataset.\n",
        "\n",
        "To predict the response for a new observation, KNN uses the *K* nearest neighbors (observations) in [***terms of the predictors!***]{style=\"color:brown;\"}\n",
        "\n",
        "The predicted response for the new observation is the most common response among the *K* nearest neighbors.\n",
        "\n",
        "## The algorithm has 3 steps:\n",
        "\n",
        "</br></br>\n",
        "\n",
        "::: incremental\n",
        "1.  Choose the number of nearest neighbors (*K*).\n",
        "\n",
        "2.  For a new observation, find the *K* closest observations in the training data (ignoring the response).\n",
        "\n",
        "3.  For the new observation, the algorithm predicts the value of the most common response among the *K* nearest observations.\n",
        ":::\n",
        "\n",
        "## Nearest neighbour\n",
        "\n",
        "Suppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n",
        "\n",
        "![](images/KNNsequence1.png){fig-align=\"center\"}\n",
        "\n",
        "[A new observation arrives, and we don't know which group it belongs to. If we had chosen $K=3$, then the three nearest neighbors would vote on which group the new observation belongs to.]{style=\"color:white;\"}\n",
        "\n",
        "## Nearest neighbour\n",
        "\n",
        "Suppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n",
        "\n",
        "![](images/KNNsequence2.png){fig-align=\"center\"}\n",
        "\n",
        "A new observation arrives, and we don't know which group it belongs to. [If we had chosen $K=3$, then the three nearest neighbors would vote on which group the new observation belongs to.]{style=\"color:white;\"}\n",
        "\n",
        "## Nearest neighbour\n",
        "\n",
        "Suppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n",
        "\n",
        "![](images/KNNsequence3.png){fig-align=\"center\"}\n",
        "\n",
        "A new observation arrives, and we don't know which group it belongs to. If we had chosen $K=3$, then the three nearest neighbors would vote on which group the new observation belongs to.\n",
        "\n",
        "## Nearest neighbour\n",
        "\n",
        "Suppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n",
        "\n",
        "![](images/KNNsequence4.png){fig-align=\"center\"}\n",
        "\n",
        "A new observation arrives, and we don't know which group it belongs to. If we had chosen $K=3$, then the three nearest neighbors would vote on which group the new observation belongs to.\n",
        "\n",
        "## Banknote data\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"70%\"}"
      ],
      "id": "13b0145a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "bank_data = pd.read_excel(\"banknotes.xlsx\")\n",
        "# Set response variable as categorical.\n",
        "bank_data['Status'] = pd.Categorical(bank_data['Status'])\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6.3, 6.3))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "d9fa5ec7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"30%\"}\n",
        ":::\n",
        ":::::\n",
        "\n",
        "Using $K = 3$, that's 2 votes for \"genuine\" and 2 for \"fake.\" So we classify it as \"genius.\"\n",
        "\n",
        "## Banknote data\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"70%\"}"
      ],
      "id": "ffa81085"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "bank_data = pd.read_excel(\"banknotes.xlsx\")\n",
        "# Set response variable as categorical.\n",
        "bank_data['Status'] = pd.Categorical(bank_data['Status'])\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6.3, 6.3))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add star point at (10, 10)\n",
        "plt.plot(10, 10, marker='*', markersize=15, color='red', label='Special Point')\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "99bbd4fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"30%\"}\n",
        ":::\n",
        ":::::\n",
        "\n",
        "Using $K = 3$, that's 2 votes for \"genuine\" and 2 for \"fake.\" So we classify it as \"genius.\"\n",
        "\n",
        "## Banknote data\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"70%\"}"
      ],
      "id": "df07e227"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "bank_data = pd.read_excel(\"banknotes.xlsx\")\n",
        "# Set response variable as categorical.\n",
        "bank_data['Status'] = pd.Categorical(bank_data['Status'])\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6.3, 6.3))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add star point at (10, 10)\n",
        "plt.plot(10, 10, marker='*', markersize=15, color='red', label='Special Point')\n",
        "\n",
        "# Add circle around the point with diameter = 1 unit (radius = 0.5)\n",
        "circle = Circle((10, 10), 0.5, edgecolor='black', facecolor='none', linewidth=2)\n",
        "plt.gca().add_patch(circle)\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "eff60510",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"30%\"}\n",
        "</br>\n",
        "\n",
        "Using $K = 3$, that's 3 votes for \"counterfeit\" and 0 for \"genuine.\" So we classify it as \"counterfeit.\"\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## Banknote data\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"70%\"}"
      ],
      "id": "4585f599"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "bank_data = pd.read_excel(\"banknotes.xlsx\")\n",
        "# Set response variable as categorical.\n",
        "bank_data['Status'] = pd.Categorical(bank_data['Status'])\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6.3, 6.3))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add star point at (10, 10)\n",
        "plt.plot(10, 10, marker='*', markersize=15, color='red', label='Special Point')\n",
        "\n",
        "# Add circle around the point with diameter = 1 unit (radius = 0.5)\n",
        "circle = Circle((10, 10), 0.5, edgecolor='black', facecolor='none', linewidth=2)\n",
        "plt.gca().add_patch(circle)\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "96e52b93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"30%\"}\n",
        "</br>\n",
        "\n",
        "Using $K = 3$, that's 3 votes for \"counterfeit\" and 0 for \"genuine.\" So we classify it as \"counterfeit.\"\n",
        "\n",
        "[Closeness is based on Euclidean distance.]{style=\"color:darkblue;\"}\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "**Ties**\n",
        "\n",
        "-   If there are more than *K* nearest neighbors, include them all.\n",
        "\n",
        "-   If there is a tie in the vote, set a rule to break the tie. For example, randomly select the class.\n",
        "\n",
        "## \n",
        "\n",
        "[KNN uses the Euclidean distance between points]{style=\"color:darkblue;\"}. So it ignores units.\n",
        "\n",
        "-   Example: two predictors: height in cm and arm span in feet. Compare two people: (152.4, 1.52) and (182.88, 1.85).\n",
        "\n",
        "-   These people are separated by 30.48 units of distance in the first variable, but only by 0.33 units in the second.\n",
        "\n",
        "-   Therefore, the first predictor plays a much more important role in classification and can bias the results to the point where the second variable becomes useless.\n",
        "\n",
        ". . .\n",
        "\n",
        "**Therefore, as a first step, we must transform the predictors so that they have the same units!**\n",
        "\n",
        "## Standardization\n",
        "\n",
        "</br>\n",
        "\n",
        "Standardization refers to *centering* and *scaling* each numerical predictor individually. This places all predictors on the same scale.\n",
        "\n",
        "In mathematical terms, we standardize a predictor $\\mathbf{X}$ as:\n",
        "\n",
        "$${\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2}},$$\n",
        "\n",
        "with $\\bar{X} = \\sum_{i=1}^n \\frac{x_i}{n}$.\n",
        "\n",
        "## Example\n",
        "\n",
        "The data is located in the file \"banknotes.xlsx\".\n"
      ],
      "id": "7cb13c6e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "bank_data = pd.read_excel(\"banknotes.xlsx\")\n",
        "# Set response variable as categorical.\n",
        "bank_data['Status'] = pd.Categorical(bank_data['Status'])\n",
        "bank_data.head()"
      ],
      "id": "b99aabed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the predictor matrix and response column\n",
        "\n",
        "Let's create the predictor matrix or response column\n"
      ],
      "id": "d4a04e96"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Set full matrix of predictors.\n",
        "X_full = bank_data.drop(columns = ['Status']) \n",
        "\n",
        "# Vector with responses\n",
        "Y_full = bank_data.filter(['Status'])"
      ],
      "id": "f3308a09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To set the target category in the response we use the `get_dummies()` function.\n"
      ],
      "id": "4a5fdef8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Create dummy variables.\n",
        "Y_dummies = pd.get_dummies(Y_full, dtype = 'int')\n",
        "\n",
        "# Select target variable.\n",
        "Y_target_full = Y_dummies['Status_counterfeit']"
      ],
      "id": "1043f2b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's partition the dataset\n",
        "\n",
        "</br>\n",
        "\n",
        "We use 70% for training and the rest for validation.\n"
      ],
      "id": "1cb5c92f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Split the dataset into training and validation.\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n",
        "                                                      test_size = 0.3)"
      ],
      "id": "d6803be3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standardization in Python\n",
        "\n",
        "</br>\n",
        "\n",
        "To standardize **numeric** predictors, we use the `StandardScaler()` function. We also apply the function to variables using the `fit_transform()` function.\n",
        "\n",
        "</br>\n"
      ],
      "id": "5dab7774"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "scaler = StandardScaler()\n",
        "Xs_train = scaler.fit_transform(X_train)"
      ],
      "id": "fcbcee1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN in Python\n",
        "\n",
        "</br>\n",
        "\n",
        "In Python, we can use the `KNeighborsClassifier()` and `fit()` from **scikit-learn** to train a KNN.\n",
        "\n",
        "In the `KNeighborsClassifier` function, we can define the number of nearest neighbors using the `n_neighbors` parameter.\n"
      ],
      "id": "33c5586a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# For example, let's use KNN with three neighbours\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Now, we train the algorithm.\n",
        "knn.fit(Xs_train, Y_train)"
      ],
      "id": "b859daa2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "</br>\n",
        "\n",
        "To evaluate KNN, we make predictions on the validation data (not used to train the KNN). To do this, we must first perform standardization operations on the predictors in the validation dataset.\n"
      ],
      "id": "02f09670"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "Xs_valid = scaler.fit_transform(X_valid)"
      ],
      "id": "82ca43e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</br>\n",
        "\n",
        "Next, we make predictions.\n"
      ],
      "id": "4b6c6282"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "Y_pred_knn = knn.predict(Xs_valid)"
      ],
      "id": "c86ab3b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion matrix\n"
      ],
      "id": "fbd01045"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Calcular matriz de confusión.\n",
        "cm = confusion_matrix(Y_valid, Y_pred_knn)\n",
        "\n",
        "# Mostrar matriz de confusión.\n",
        "ConfusionMatrixDisplay(cm).plot()"
      ],
      "id": "e09c9cb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finding the best value of *K*\n",
        "\n",
        "We can determine the best value of *K* for the KNN algorithm. To this end, we evaluate the performance of the KNN for different values of $K$ in terms of accuracy on the validation dataset.\n"
      ],
      "id": "d743969d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "best_k = 1\n",
        "best_accuracy = 0\n",
        "k_values = range(1, 50)  # Test k values from 1 to 50\n",
        "validation_accuracies = []\n",
        "\n",
        "for k in k_values:\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(Xs_train, Y_train)\n",
        "    val_accuracy = accuracy_score(Y_valid, model.predict(Xs_valid))\n",
        "    validation_accuracies.append(val_accuracy)\n",
        "\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        best_k = k"
      ],
      "id": "a2df1b52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize\n",
        "\n",
        "We can then visualize the accuracy for different values of $K$ using the following graph and code.\n"
      ],
      "id": "816c2287"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "#| code-fold: true\n",
        "\n",
        "plt.figure(figsize=(6.3, 4.3))\n",
        "plt.plot(k_values, validation_accuracies, marker=\"o\", linestyle=\"-\")\n",
        "plt.xlabel(\"Number of Neighbors (k)\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Choosing the Best k for KNN\")\n",
        "plt.show()"
      ],
      "id": "078c76c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "Finally, we select the best number of nearest neighbors contained in the `best_k` object.\n"
      ],
      "id": "4585340b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "KNN_final = KNeighborsClassifier(n_neighbors = best_k)\n",
        "KNN_final.fit(Xs_train, Y_train)"
      ],
      "id": "010915a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</br>\n",
        "\n",
        "The accuracy of the best KNN is\n"
      ],
      "id": "0bbe1548"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "Y_pred_KNNfinal = KNN_final.predict(Xs_valid)\n",
        "valid_accuracy = accuracy_score(Y_valid, Y_pred_KNNfinal)\n",
        "print(valid_accuracy)"
      ],
      "id": "53759d6d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "\n",
        "</br>\n",
        "\n",
        "KNN is intuitive and simple and can produce decent predictions. However, KNN has some disadvantages:\n",
        "\n",
        "-   When the training dataset is very large, KNN is computationally expensive. This is because, to predict an observation, we need to calculate the distance between that observation and all the others in the dataset. (\"*Lazy learner*\").\n",
        "\n",
        "-   In this case, a decision tree is more advantageous because it is easy to build, store, and make predictions with.\n",
        "\n",
        "## \n",
        "\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "-   The predictive performance of KNN deteriorates as the number of predictors increases.\n",
        "\n",
        "-   This is because the expected distance to the nearest neighbor increases dramatically with the number of predictors, unless the size of the dataset increases exponentially with this number.\n",
        "\n",
        "-   This is known as the ***curse of dimensionality***.\n",
        ":::\n",
        "\n",
        "![](images/clipboard-72810347.png){fig-align=\"center\"}\n",
        "\n",
        "::: {style=\"font-size: 50%;\"}\n",
        "<https://aiaspirant.com/curse-of-dimensionality/>\n",
        ":::\n",
        "\n",
        "# [Return to main page](https://alanrvazquez.github.io/TEC-IN2004B/)"
      ],
      "id": "7f426995"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}