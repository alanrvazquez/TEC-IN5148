{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Ensemble Methods\"\n",
        "subtitle: \"IN2004B: Generation of Value with Data Analytics\"\n",
        "author: \n",
        "  - name: Alan R. Vazquez\n",
        "    affiliations:\n",
        "      - name: Department of Industrial Engineering\n",
        "format: \n",
        "  revealjs:\n",
        "    chalkboard: false\n",
        "    multiplex: false\n",
        "    footer: \"Tecnologico de Monterrey\"\n",
        "    logo: IN2004B_logo.png\n",
        "    css: style.css\n",
        "    slide-number: True\n",
        "    html-math-method: mathjax\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "## Agenda\n",
        "\n",
        "</br>\n",
        "\n",
        "1.  Introduction to Ensemble Methods\n",
        "2.  Bagging\n",
        "3.  Random Forests\n",
        "\n",
        "# Ensamble Methods\n",
        "\n",
        "## Load the libraries\n",
        "\n",
        "Before we start, let's import the data science libraries into Python.\n"
      ],
      "id": "57c99b42"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score"
      ],
      "id": "83eed877",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we use specific functions from the **pandas**, **matplotlib**, **seaborn** and **sklearn** libraries in Python.\n",
        "\n",
        "## Decision trees\n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: columns\n",
        ":::: {.column width=\"45%\"}\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "-   Simple and useful for interpretations.\n",
        "\n",
        "-   Can handle continuous and categorical predictors and responses. So, they can be applied to both [**classification**]{style=\"color:blue;\"} and [**regression**]{style=\"color:green;\"} problems.\n",
        "\n",
        "-   Computationally efficient.\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {.column width=\"55%\"}\n",
        "</br>\n",
        "\n",
        "![](images/clipboard-2280895928.png){fig-align=\"center\"}\n",
        ":::\n",
        "::::::\n",
        "\n",
        "## Limitations of decision trees\n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "-   In general, decision trees do not work well for classification and regression problems.\n",
        "\n",
        "-   However, decision trees can be combined to build effective algorithms for these problems.\n",
        "\n",
        "## Ensamble methods\n",
        "\n",
        "</br>\n",
        "\n",
        "Ensemble methods refer to frameworks to combine decision trees.\n",
        "\n",
        "Here, we will cover a popular ensamble method:\n",
        "\n",
        "-   [**Bagging**]{style=\"color:purple;\"}. Ensemble many deep trees.\n",
        "\n",
        "    -   Quintessential method: [Random Forests]{style=\"color:purple;\"}.\n",
        "\n",
        "# Bagging\n",
        "\n",
        "## Bootstrap samples\n",
        "\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "Bootstrap samples are samples obtained *with replacement* from the original sample. So, an observation can occur more than one in a bootstrap sample.\n",
        "\n",
        "Bootstrap samples are the building block of the bootstrap method, which is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples.\n",
        ":::\n",
        "\n",
        "![](images/clipboard-2003681619.png){fig-align=\"center\"}\n",
        "\n",
        "## Bagging\n",
        "\n",
        "Given a training dataset, [**bagging**]{style=\"color:purple;\"} averages the predictions from decision trees over a collection of ***bootstrap*** samples.\n",
        "\n",
        "![](images/bagging_scheme.png){fig-align=\"center\"}\n",
        "\n",
        "## Predictions\n",
        "\n",
        "</br></br>\n",
        "\n",
        "Let $\\mathbf{x} = (x_1, x_2, \\ldots, x_p)$ be a vector of new predictor values. For classification problems with 2 classes:\n",
        "\n",
        "1.  Each classification tree outputs the probability for class 1 and 2 depending on the region $\\mathbf{x}$ falls in.\n",
        "\n",
        "2.  For the *b*-th tree, we denote the probabilities as $\\hat{p}^{b}_0(\\boldsymbol{x})$ and $\\hat{p}^{b}_1(\\boldsymbol{x})$ for class 0 and 1, respectively.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "3.  Using the probabilities, a standard tree follows the Bayes Classifier to output the actual class:\n",
        "\n",
        "$$\\hat{T}_{b}(\\boldsymbol{x}) =\n",
        "    \\begin{cases}\n",
        "      1, & \\hat{p}^{b}_1(\\boldsymbol{x}) > 0.5 \\\\\n",
        "      0, & \\hat{p}^{b}_1(\\boldsymbol{x}) \\leq 0.5\n",
        "    \\end{cases}$$\n",
        "\n",
        "## \n",
        "\n",
        "4.  Compute the proportion of trees that output a 0 as\n",
        "\n",
        "$$p_{bag, 0} = \\frac{1}{B} \\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 0).$$\n",
        "\n",
        "5.  Compute the proportion of trees that output a 1 as:\n",
        "\n",
        "$$p_{bag, 1} = \\frac{1}{B}\\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 1).$$\n",
        "\n",
        "## \n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "6.  Classify according to a majority vote between $p_{bag, 0}$ and $p_{bag, 1}$.\n",
        "\n",
        "## Implementation\n",
        "\n",
        "</br></br>\n",
        "\n",
        "-   How many trees? No risk of overfitting, so use plenty.\n",
        "\n",
        "-   No pruning necessary to build the trees. However, one can still decide to apply some pruning or early stopping mechanism.\n",
        "\n",
        "-   The size of bootstrap samples is the same as the size of the training dataset, but we can use a different size.\n",
        "\n",
        "## Example 1\n",
        "\n",
        "</br>\n",
        "\n",
        "The data “AdultReduced.xlsx” comes from the UCI Machine Learning Repository and is derived from US census records. In this data, the goal is to predict whether a person's income was high (defined in 1994 as more than \\$50,000) or low.\n",
        "\n",
        "Predictors include education level, job type (e.g., never worked and local government), capital gains/losses, hours worked per week, country of origin, etc. The data contains 7,508 records.\n",
        "\n",
        "## Read the dataset\n"
      ],
      "id": "766e7554"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Load the data\n",
        "Adult_data = pd.read_excel('AdultReduced.xlsx')\n",
        "\n",
        "# Preview the data.\n",
        "Adult_data.head(3)"
      ],
      "id": "0fb4f1ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selected predictors.\n",
        "\n",
        "-   age: Age of the individual.\n",
        "-   sex: Sex of the individual (male or female).\n",
        "-   race: Race of the individual (W, B, Amer-Indian, Amer-Pacific, or Other).\n",
        "-   education.num: number of years of education.\n",
        "-   hours.per.week: Number of work hours per week.\n"
      ],
      "id": "98115a2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Choose the predictors.\n",
        "X_full = Adult_data.filter(['age', 'sex', 'race', 'education.num', \n",
        "                            'hours.per.week'])"
      ],
      "id": "542a5bc3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-processing for categorical predictors\n",
        "\n",
        "</br></br>\n",
        "\n",
        "Unfortunately, bagging does not work with categorical predictors. We must transform them into dummy variables using the code below.\n"
      ],
      "id": "69148997"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Turn categorical predictors into dummy variables.\n",
        "X_dummies = pd.get_dummies(X_full[['sex', 'race']])\n",
        "\n",
        "# Drop original predictors from the test.\n",
        "X_other = X_full.drop(['sex', 'race'], axis=1)\n",
        "\n",
        "# Update the predictor matrix.\n",
        "X_full = pd.concat([X_other, X_dummies], axis=1)"
      ],
      "id": "bd2a6d03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set the target class\n",
        "\n",
        "Let's set the **target class** and the **reference class** using the `get_dummies()` function.\n"
      ],
      "id": "2f3bba7e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Select answer\n",
        "Y = Adult_data.filter(['income'])\n",
        "\n",
        "# Create dummy variables.\n",
        "Y_dummies = pd.get_dummies(Y, dtype = 'int')\n",
        "\n",
        "# Show.\n",
        "Y_dummies.head(4)"
      ],
      "id": "df27112b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "Here we'll use the **large** target class. So, let's use the corresponding column as our response variable.\n"
      ],
      "id": "af54dc62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Choose target category.\n",
        "Y_target = Y_dummies['income_large']\n",
        "\n",
        "Y_target.head()"
      ],
      "id": "fdea47ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and validation datasets\n",
        "\n",
        "</br>\n",
        "\n",
        "To evaluate a model's performance on unobserved data, we split the current dataset into training and validation datasets. To do this, we use `train_test_split()` from **scikit-learn**.\n"
      ],
      "id": "94f4e333"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Split into training and validation\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target, \n",
        "                                                      test_size = 0.2)"
      ],
      "id": "08e82b36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use 80% of the dataset for training and the rest for validation.\n",
        "\n",
        "## Bagging in Python\n",
        "\n",
        "</br>\n",
        "\n",
        "We define a bagging algorithm for classification using the `BaggingClassifier` function from **scikit-learn**.\n",
        "\n",
        "The `n_estimators` argument is the number of decision trees to generate in bagging. Ideally, it should be high, around 500.\n"
      ],
      "id": "a7de5cfa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Set the bagging algorithm.\n",
        "Baggingalgorithm = BaggingClassifier(n_estimators = 500, \n",
        "                                     random_state = 59227)\n",
        "\n",
        "# Train the bagging algorithm.\n",
        "Baggingalgorithm.fit(X_train, Y_train)"
      ],
      "id": "0073e630",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`random_state` allows us to obtain the same bagging algorithm in different runs of the algorithm.\n",
        "\n",
        "## Predictions\n",
        "\n",
        "Predict the classes using bagging.\n"
      ],
      "id": "a0d1bc17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "predicted_class = Baggingalgorithm.predict(X_valid)\n",
        "\n",
        "predicted_class"
      ],
      "id": "78c99e6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion matrix\n"
      ],
      "id": "e8e9bf81"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Compute confusion matrix.\n",
        "cm = confusion_matrix(Y_valid, predicted_class)\n",
        "\n",
        "# Visualize the matrix.\n",
        "ConfusionMatrixDisplay(cm).plot()"
      ],
      "id": "d5afd37a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy\n",
        "\n",
        "</br></br>\n",
        "\n",
        "The accuracy of the bagging classifier is 78%.\n"
      ],
      "id": "7b634f2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Compute accuracy.\n",
        "accuracy = accuracy_score(Y_valid, predicted_class)\n",
        "\n",
        "# Show accuracy.\n",
        "print( round(accuracy, 2) )"
      ],
      "id": "e9b2c8f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A single deep tree\n",
        "\n",
        "To compare the bagging, let's use a single deep tree.\n"
      ],
      "id": "c716f30e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "#| code-fold: true\n",
        "\n",
        "# We tell Python that we want a classification tree\n",
        "clf_simple = DecisionTreeClassifier(ccp_alpha=0.0,\n",
        "                                    random_state=507134)\n",
        "\n",
        "# We train the classification tree using the training data.\n",
        "clf_simple.fit(X_train, Y_train)"
      ],
      "id": "0c25e4e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compute the accuracy of the pruned tree.\n"
      ],
      "id": "22d3cfc5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "single_tree_Y_pred = clf_simple.predict(X_valid)\n",
        "accuracy = accuracy_score(Y_valid, single_tree_Y_pred)\n",
        "print( round(accuracy, 2) )"
      ],
      "id": "4914af17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advantages\n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "-   Bagging will have lower prediction errors than a single classification tree.\n",
        "\n",
        "-   The fact that, for each tree, not all of the original observations were used, can be exploited to produce an estimate of the accuracy for classification.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "</br></br>\n",
        "\n",
        "-   *Loss of interpretability*: the final bagged classifier is [not a tree]{style=\"color:darkred;\"}, and so we forfeit the clear interpretative ability of a classification tree.\n",
        "\n",
        "-   *Computational complexity*: we are essentially multiplying the work of growing (and possibly pruning) a single tree by B.\n",
        "\n",
        "-   *Fundamental issue*: bagging a good model can improve predictive accuracy, but bagging a bad one can seriously degrade predictive accuracy.\n",
        "\n",
        "## Other issues\n",
        "\n",
        "</br></br>\n",
        "\n",
        "-   Suppose a variable is very important and decisive.\n",
        "\n",
        "    -   It will probably appear near the top of a large number of trees.\n",
        "\n",
        "    -   And these trees will tend to vote the same way.\n",
        "\n",
        "    -   In some sense, then, many of the trees are “correlated”.\n",
        "\n",
        "    -   This will degrade the performance of bagging.\n",
        "\n",
        "## \n",
        "\n",
        "-   Bagging is unable to capture simple decision boundaries\n",
        "\n",
        "![](images/bagging_decision_boundary.png){fig-align=\"center\"}\n",
        "\n",
        "# Random Forest\n",
        "\n",
        "## Random Forest\n",
        "\n",
        "</br>\n",
        "\n",
        "Exactly as bagging, but...\n",
        "\n",
        "-   When splitting the nodes using the CART algorithm, instead of going through all possible splits for all possible variables, we go through all possible splits on a [*random sample of a small number of variables* $m$]{style=\"color:brown;\"}, where $m < p$.\n",
        "\n",
        "Random forests can reduce variability further.\n",
        "\n",
        "## Why does it work?\n",
        "\n",
        "</br>\n",
        "\n",
        "-   Not so dominant predictors will get a chance to appear by themselves and show “their stuff”.\n",
        "\n",
        "-   This adds more diversity to the trees.\n",
        "\n",
        "-   The fact that the trees in the forest are not (strongly) correlated means lower variability in the predictions and so, a bettter performance overall.\n",
        "\n",
        "## Tuning parameter\n",
        "\n",
        "</br>\n",
        "\n",
        "How do we set $m$?\n",
        "\n",
        "-   For classification, use $m = \\lfloor \\sqrt{p} \\rfloor$ and the minimum node size is 1.\n",
        "\n",
        "In practice, sometimes the best values for these parameters will depend on the problem. So, we can treat $m$ as a tuning parameter.\n",
        "\n",
        "> Note that if $m = p$, we get bagging.\n",
        "\n",
        "## The final product is a black box\n",
        "\n",
        "![](images/clipboard-2376484681.png){fig-align=\"center\"}\n",
        "\n",
        "-   A black box. Inside the box are several hundred trees, each slightly different.\n",
        "\n",
        "-   You put an observation into the black box, and the black box classifies it or predicts it for you.\n",
        "\n",
        "## Random Forest in Python\n",
        "\n",
        "</br>\n",
        "\n",
        "In Python, we define a RandomForest algorithm for classification using the `RandomForestClassifier` function from **scikit-learn**. The `n_estimators` argument is the number of decision trees to generate in the RandomForest, and `random_state` allows you to reprudce the results.\n"
      ],
      "id": "7be14584"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Set the bagging algorithm.\n",
        "RFalgorithm = RandomForestClassifier(n_estimators = 500,\n",
        "                                     random_state = 59227)\n",
        "\n",
        "# Train the bagging algorithm.\n",
        "RFalgorithm.fit(X_train, Y_train)"
      ],
      "id": "64791c77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion matrix\n",
        "\n",
        "Evaluate the performance of random forest.\n"
      ],
      "id": "6f238e23"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Predict class.\n",
        "RF_predicted = RFalgorithm.predict(X_valid)\n",
        "\n",
        "# Compute confusion matrix.\n",
        "cm = confusion_matrix(Y_valid, RF_predicted)\n",
        "\n",
        "# Visualize the matrix.\n",
        "ConfusionMatrixDisplay(cm).plot()"
      ],
      "id": "db0acaa0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy\n",
        "\n",
        "</br></br>\n",
        "\n",
        "The accuracy of the random forest classifier is 79%.\n"
      ],
      "id": "2ecf82dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Compute accuracy.\n",
        "accuracy = accuracy_score(Y_valid, RF_predicted)\n",
        "\n",
        "# Show accuracy.\n",
        "print( round(accuracy, 2) )"
      ],
      "id": "cbb521e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [Return to main page](https://alanrvazquez.github.io/TEC-IN2004B/)"
      ],
      "id": "babbd9d0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}