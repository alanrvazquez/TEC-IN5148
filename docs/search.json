[
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#agenda",
    "href": "PreProcessing/PreProcessing.slides.html#agenda",
    "title": "Data preprocessing",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nDealing with missing values\nTransforming predictors\nReducing the number of predictors\nStandardizing predictors"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#data-preprocessing",
    "href": "PreProcessing/PreProcessing.slides.html#data-preprocessing",
    "title": "Data preprocessing",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nData pre-processing techniques generally refer to the addition, deletion, or transformation of data.\n\n\nIt can make or break a model’s predictive ability.\n\n\nFor example, linear regression models (to be discussed later) are relatively insensitive to the characteristics of the predictor data, but advanced methods like K-nearest neighbors, principal component regression, and LASSO are not."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section",
    "href": "PreProcessing/PreProcessing.slides.html#section",
    "title": "Data preprocessing",
    "section": "",
    "text": "We will review some common strategies for processing predictors from the data, without considering how they might be related to the response.\n\nIn particular, we will review:\n\nDealing with missing values.\nTransforming predictors.\nReducing the number of predictors.\nStandardizing the units of the predictors."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#scikit-learn-library",
    "href": "PreProcessing/PreProcessing.slides.html#scikit-learn-library",
    "title": "Data preprocessing",
    "section": "scikit-learn library",
    "text": "scikit-learn library\n\nscikit-learn is a robust and popular library for machine learning in Python\nIt provides simple, efficient tools for data mining and data analysis\nIt is built on top of libraries such as NumPy, SciPy, and Matplotlib\nhttps://scikit-learn.org/stable/"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-1",
    "href": "PreProcessing/PreProcessing.slides.html#section-1",
    "title": "Data preprocessing",
    "section": "",
    "text": "Let’s import scikit-learn into Python together with the other relevant libraries.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler \n\nWe will not use all the functions from the scikit-learn library. Instead, we will use specific functions from the sub-libraries preprocessing and impute."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#missing-values",
    "href": "PreProcessing/PreProcessing.slides.html#missing-values",
    "title": "Data preprocessing",
    "section": "Missing values",
    "text": "Missing values\nIn many cases, some predictors have no values for a given observation. It is important to understand why the values are missing.\n\n\nThere four main types of missing data:\n\nStructurally missing data is data that is missing for a logical reason or because it should not exist.\nMissing completely at random assumes that the fact that the data is missing is unrelated to the other information in the data."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-2",
    "href": "PreProcessing/PreProcessing.slides.html#section-2",
    "title": "Data preprocessing",
    "section": "",
    "text": "Missing at random assumes that we can predict the value that is missing based on the other available data.\nMissing not at random assumes that there is a mechanism that generates the missing values, which may include observed and unobserved predictors."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-3",
    "href": "PreProcessing/PreProcessing.slides.html#section-3",
    "title": "Data preprocessing",
    "section": "",
    "text": "For large data sets, removal of observations based on missing values is not a problem, assuming that the type of missing data is completely at random.\n\nIn a smaller data sets, there is a high price in removing observations. To overcome this issue, we can use methods of imputation, which try to estimate the missing values of a predictor variable using the other predictors’ values.\n\nHere, we will introduce some simple methods for imputing missing values in categorical and numerical variables."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#example-1",
    "href": "PreProcessing/PreProcessing.slides.html#example-1",
    "title": "Data preprocessing",
    "section": "Example 1",
    "text": "Example 1\n\nLet’s use the penguins dataset available in the file “penguins.xlsx”.\n\n# Load the Excel file into a pandas DataFrame.\npenguins_data = pd.read_excel(\"penguins.xlsx\")\n\n# Set categorical variables.\npenguins_data['sex'] = pd.Categorical(penguins_data['sex'])\npenguins_data['species'] = pd.Categorical(penguins_data['species'])\npenguins_data['island'] = pd.Categorical(penguins_data['island'])"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#predictor-data",
    "href": "PreProcessing/PreProcessing.slides.html#predictor-data",
    "title": "Data preprocessing",
    "section": "Predictor data",
    "text": "Predictor data\nFor illustrative purposes, we will use the predictors bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, and sex. We create the predictor matrix.\n\n# Set full matrix of predictors.\nX_p = (penguins_data\n      .filter(['bill_length_mm', 'bill_depth_mm', \n           'flipper_length_mm', 'body_mass_g', 'sex']))\n# Preview dataset.\nX_p.head(3)\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\n39.1\n18.7\n181.0\n3750.0\nmale\n\n\n1\n39.5\n17.4\n186.0\n3800.0\nfemale\n\n\n2\n40.3\n18.0\n195.0\n3250.0\nfemale"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-4",
    "href": "PreProcessing/PreProcessing.slides.html#section-4",
    "title": "Data preprocessing",
    "section": "",
    "text": "Let’s check if the dataset has missing observations using the function .info() from pandas.\n\nX_p.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     342 non-null    float64 \n 1   bill_depth_mm      342 non-null    float64 \n 2   flipper_length_mm  342 non-null    float64 \n 3   body_mass_g        342 non-null    float64 \n 4   sex                333 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 11.3 KB"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-5",
    "href": "PreProcessing/PreProcessing.slides.html#section-5",
    "title": "Data preprocessing",
    "section": "",
    "text": "In the output of the function, “non-null” refers to the number of entries in a column that have actual values. That is, the number of entries where there are not NaN.\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     342 non-null    float64 \n 1   bill_depth_mm      342 non-null    float64 \n 2   flipper_length_mm  342 non-null    float64 \n 3   body_mass_g        342 non-null    float64 \n 4   sex                333 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 11.3 KB\n\n\nThe “Index” section shows the number of observations in the dataset."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-6",
    "href": "PreProcessing/PreProcessing.slides.html#section-6",
    "title": "Data preprocessing",
    "section": "",
    "text": "The output below shows that there are 344 entries but bill_length_mm has 342 that are “non-null”. Therefore, this column has two missing observations.\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     342 non-null    float64 \n 1   bill_depth_mm      342 non-null    float64 \n 2   flipper_length_mm  342 non-null    float64 \n 3   body_mass_g        342 non-null    float64 \n 4   sex                333 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 11.3 KB\n\n\nThe same is true for the other predictors except for sex that has 11 missing values."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-7",
    "href": "PreProcessing/PreProcessing.slides.html#section-7",
    "title": "Data preprocessing",
    "section": "",
    "text": "Alternatively, we can use the function .isnull() together with sum() to determine the number of missing values for each column in the dataset.\n\n\nmissing_values = X_p.isnull().sum()\nmissing_values\n\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#removing-missing-values",
    "href": "PreProcessing/PreProcessing.slides.html#removing-missing-values",
    "title": "Data preprocessing",
    "section": "Removing missing values",
    "text": "Removing missing values\nIf we want to remove all rows in the dataset that have at least one missing value, we use the function .dropna().\n\ncomplete_predictors = X_p.dropna()\ncomplete_predictors.head()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\n39.1\n18.7\n181.0\n3750.0\nmale\n\n\n1\n39.5\n17.4\n186.0\n3800.0\nfemale\n\n\n2\n40.3\n18.0\n195.0\n3250.0\nfemale\n\n\n4\n36.7\n19.3\n193.0\n3450.0\nfemale\n\n\n5\n39.3\n20.6\n190.0\n3650.0\nmale"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-8",
    "href": "PreProcessing/PreProcessing.slides.html#section-8",
    "title": "Data preprocessing",
    "section": "",
    "text": "complete_predictors.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 333 entries, 0 to 343\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     333 non-null    float64 \n 1   bill_depth_mm      333 non-null    float64 \n 2   flipper_length_mm  333 non-null    float64 \n 3   body_mass_g        333 non-null    float64 \n 4   sex                333 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 13.5 KB\n\n\nThe new data is complete because each column has 333 “non-null” values; the total number of observations in complete_predictors.\nHowever, note that we have lost eight of the original observations in X_train_p!"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#imputation-using-the-mean",
    "href": "PreProcessing/PreProcessing.slides.html#imputation-using-the-mean",
    "title": "Data preprocessing",
    "section": "Imputation using the mean",
    "text": "Imputation using the mean\nWe can impute the missing values of a numeric variable using the mean or median of its available values. For example, consider the variable bill_length_mm that has two missing values.\n\nX_p['bill_length_mm'].info()\n\n&lt;class 'pandas.core.series.Series'&gt;\nRangeIndex: 344 entries, 0 to 343\nSeries name: bill_length_mm\nNon-Null Count  Dtype  \n--------------  -----  \n342 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB\n\n\nIn scikit-learn, we use the function SimpleImputer() to define the method of imputation of missing values."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-9",
    "href": "PreProcessing/PreProcessing.slides.html#section-9",
    "title": "Data preprocessing",
    "section": "",
    "text": "Using SimpleImputer(), we set the method to impute missing values using the mean.\nWe also use the function fit_transform() to apply the imputation method to the variable.\n\n# Imputation for numerical variables (using the mean)\nnum_imputer = SimpleImputer(strategy = 'mean')\n\n# Replace the original variable with new version.\nX_p['bill_length_mm'] = num_imputer.fit_transform(X_p[ ['bill_length_mm'] ] )"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-10",
    "href": "PreProcessing/PreProcessing.slides.html#section-10",
    "title": "Data preprocessing",
    "section": "",
    "text": "After imputation, the information of the predictors in the dataset looks like this.\n\nX_p.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     344 non-null    float64 \n 1   bill_depth_mm      342 non-null    float64 \n 2   flipper_length_mm  342 non-null    float64 \n 3   body_mass_g        342 non-null    float64 \n 4   sex                333 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 11.3 KB\n\n\nNow, bill_length_mm has 344 complete values."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-11",
    "href": "PreProcessing/PreProcessing.slides.html#section-11",
    "title": "Data preprocessing",
    "section": "",
    "text": "To impute the missing values using the median, we simply set this method in SimpleImputer(). For example, let’s impute the missing values of bill_depth_mm.\n\n# Imputation for numerical variables (using the mean)\nnum_imputer = SimpleImputer(strategy = 'median')\n\n# Replace the original variable with new version.\nX_p['bill_depth_mm'] = num_imputer.fit_transform(X_p[ ['bill_depth_mm'] ] )\n\n# Show the information of the predictor.\nX_p['bill_depth_mm'].info()\n\n&lt;class 'pandas.core.series.Series'&gt;\nRangeIndex: 344 entries, 0 to 343\nSeries name: bill_depth_mm\nNon-Null Count  Dtype  \n--------------  -----  \n344 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.8 KB"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#using-the-mean-or-the-median",
    "href": "PreProcessing/PreProcessing.slides.html#using-the-mean-or-the-median",
    "title": "Data preprocessing",
    "section": "Using the mean or the median?",
    "text": "Using the mean or the median?\n\nWe use the sample mean when the data distribution is roughly symmetrical.\n\nPros: Simple and easy to implement.\nCons: Sensitive to outliers; may not be accurate for skewed distributions"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-12",
    "href": "PreProcessing/PreProcessing.slides.html#section-12",
    "title": "Data preprocessing",
    "section": "",
    "text": "We use the sample median when the data is skewed (e.g., incomes, prices).\n\nPros: Less sensitive to outliers; robust for skewed distributions.\nCons: May reduce variability in the data."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#imputation-method-for-a-categorical-variable",
    "href": "PreProcessing/PreProcessing.slides.html#imputation-method-for-a-categorical-variable",
    "title": "Data preprocessing",
    "section": "Imputation method for a categorical variable",
    "text": "Imputation method for a categorical variable\nIf a categorical variable has missing values, we can use the most frequent of the available values to replace the missing values. To this end, we use similar commands as before.\nFor example, let’s impute the missing values of sex using this strategy.\n\n# Imputation for categorical variables (using the most frequent value)\ncat_imputer = SimpleImputer(strategy = 'most_frequent')\n\n# Apply imputation strategy for categorical variables.\nX_p['sex'] = cat_imputer.fit_transform(X_p[ ['sex'] ]).ravel()"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-13",
    "href": "PreProcessing/PreProcessing.slides.html#section-13",
    "title": "Data preprocessing",
    "section": "",
    "text": "Let’s now have a look at the information of the dataset.\n\n# Apply imputation strategy for categorical variables.\nX_p.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   bill_length_mm     344 non-null    float64\n 1   bill_depth_mm      344 non-null    float64\n 2   flipper_length_mm  342 non-null    float64\n 3   body_mass_g        342 non-null    float64\n 4   sex                344 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 13.6+ KB\n\n\nThe columns bill_length_mm, bill_depth_mm, and sex have 344 complete values."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-14",
    "href": "PreProcessing/PreProcessing.slides.html#section-14",
    "title": "Data preprocessing",
    "section": "",
    "text": "Unfortunately, after applying cat_imputer to the dataset, the variable sex is an object. To change it to categorical, we use the function pd.Categorical again.\n\n# Apply imputation strategy for categorical variables.\nX_p['sex'] = pd.Categorical(X_p['sex'])\nX_p.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     344 non-null    float64 \n 1   bill_depth_mm      344 non-null    float64 \n 2   flipper_length_mm  342 non-null    float64 \n 3   body_mass_g        342 non-null    float64 \n 4   sex                344 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 11.3 KB"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#categorical-predictors",
    "href": "PreProcessing/PreProcessing.slides.html#categorical-predictors",
    "title": "Data preprocessing",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nA categorical predictor takes on values that are categories or groups.\n\nFor example:\n\nType of school: Public or private.\nTreatment: New or placebo.\nGrade: Passed or not passed.\n\n\n\nThe categories can be represented by names, labels or even numbers. Their use in regression requires dummy variables, which are numeric."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#dummy-variables",
    "href": "PreProcessing/PreProcessing.slides.html#dummy-variables",
    "title": "Data preprocessing",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\nThe traditional choice for a dummy variable is a binary variable, which can only take the values 0 and 1.\n\n\nInitially, a categorical variable with \\(k\\) categories requires \\(k\\) dummy variables."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#example-2",
    "href": "PreProcessing/PreProcessing.slides.html#example-2",
    "title": "Data preprocessing",
    "section": "Example 2",
    "text": "Example 2\nA market analyst is studying quality characteristics of cars. Specifically, the analyst is investigating the miles per gallon (mpg) of cars can be predicted using:\n\n\\(X_1:\\) cylinders. Number of cylinders between 4 and 8\n\\(X_2:\\) displacement. Engine displacement (cu. inches)\n\\(X_3:\\) horsepower. Engine horsepower\n\\(X_4:\\) weight. Vehicle weight (lbs.)\n\\(X_5:\\) acceleration. Time to accelerate from 0 to 60 mph (sec.)\n\\(X_6:\\) origin. Origin of car (American, European, Japanese)"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-15",
    "href": "PreProcessing/PreProcessing.slides.html#section-15",
    "title": "Data preprocessing",
    "section": "",
    "text": "The dataset is in the file “auto.xlsx”. Let’s read the data using pandas.\n\n# Load the Excel file into a pandas DataFrame.\nauto_data = pd.read_excel(\"auto.xlsx\")\n\n# Set categorical variables.\nauto_data['origin'] = pd.Categorical(auto_data['origin'])"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#predictor-data-1",
    "href": "PreProcessing/PreProcessing.slides.html#predictor-data-1",
    "title": "Data preprocessing",
    "section": "Predictor data",
    "text": "Predictor data\nFor illustrative purposes, we use the six predictors, \\(X_1, \\ldots, X_6\\). We create the predictor matrix.\n\n# Set full matrix of predictors.\nX_c = (auto_data\n      .filter(['cylinders', 'displacement', \n              'horsepower', 'weight', 'acceleration', 'origin']))\n# Preview first rows.\nX_c.head(3)\n\n\n\n\n\n\n\n\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\norigin\n\n\n\n\n0\n8\n307.0\n130\n3504\n12.0\nAmerican\n\n\n1\n8\n350.0\n165\n3693\n11.5\nAmerican\n\n\n2\n8\n318.0\n150\n3436\n11.0\nAmerican"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#dealing-with-missing-values-1",
    "href": "PreProcessing/PreProcessing.slides.html#dealing-with-missing-values-1",
    "title": "Data preprocessing",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\n\nThe dataset has missing values. In this example, we remove each row with at least one missing value.\n\n\n# Remove rows with missing values.\ncomplete_Auto = X_c.dropna()"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#example-2-cont.",
    "href": "PreProcessing/PreProcessing.slides.html#example-2-cont.",
    "title": "Data preprocessing",
    "section": "Example 2 (cont.)",
    "text": "Example 2 (cont.)\nCategorical predictor: Origin of a car. Three categories: American, European and Japanese.\nInitially, 3 dummy variables are required:\n\\[d_1 =\n\\begin{cases}\n1 \\text{ if observation is from an American car}\\\\\n0 \\text{ otherwise}\n\\end{cases}\\] \\[d_2 =\n\\begin{cases}\n1 \\text{ if observation is from an European car}\\\\\n0 \\text{ otherwise}\n\\end{cases}\\] \\[d_3 =\n\\begin{cases}\n1 \\text{ if observation is from a Japanese car}\\\\\n0 \\text{ otherwise}\n\\end{cases}\\]"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-16",
    "href": "PreProcessing/PreProcessing.slides.html#section-16",
    "title": "Data preprocessing",
    "section": "",
    "text": "The variable Origin would then be replaced by the three dummy variables\n\n\n\nOrigin (\\(X\\))\n\\(d_1\\)\n\\(d_2\\)\n\\(d_3\\)\n\n\n\n\nAmerican\n1\n0\n0\n\n\nAmerican\n1\n0\n0\n\n\nEuropean\n0\n1\n0\n\n\nEuropean\n0\n1\n0\n\n\nAmerican\n1\n0\n0\n\n\nJapanese\n0\n0\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#a-drawback",
    "href": "PreProcessing/PreProcessing.slides.html#a-drawback",
    "title": "Data preprocessing",
    "section": "A drawback",
    "text": "A drawback\n\n\nA drawback with the initial dummy variables is that they are linearly dependent. That is, \\(d_1 + d_2 + d_3 = 1\\).\nTherefore, we can determine the value of \\(d_1 = 1- d_2 - d_3.\\)\nPredictive models such as linear regression are sensitive to linear dependencies among predictors.\nThe solution is to drop one of the predictor, say, \\(d_1\\), from the data."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-17",
    "href": "PreProcessing/PreProcessing.slides.html#section-17",
    "title": "Data preprocessing",
    "section": "",
    "text": "The variable Origin would then be replaced by the three dummy variables.\n\n\n\nOrigin (\\(X\\))\n\\(d_2\\)\n\\(d_3\\)\n\n\n\n\nAmerican\n0\n0\n\n\nAmerican\n0\n0\n\n\nEuropean\n1\n0\n\n\nEuropean\n1\n0\n\n\nAmerican\n0\n0\n\n\nJapanese\n0\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#dummy-variables-in-python",
    "href": "PreProcessing/PreProcessing.slides.html#dummy-variables-in-python",
    "title": "Data preprocessing",
    "section": "Dummy variables in Python",
    "text": "Dummy variables in Python\n\nWe can get the dummy variables of a categorical variable using the function pd.get_dummies() from pandas.\nThe input of the function is the categorical variable.\nThe function has an extra argument called drop_first to drop the first dummy variable. It also has the argument dtype to show the values as integers.\n\ndummy_data = pd.get_dummies(complete_Auto['origin'], drop_first = True, \n                            dtype = 'int')"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-18",
    "href": "PreProcessing/PreProcessing.slides.html#section-18",
    "title": "Data preprocessing",
    "section": "",
    "text": "dummy_data.head()\n\n\n\n\n\n\n\n\nEuropean\nJapanese\n\n\n\n\n0\n0\n0\n\n\n1\n0\n0\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n0\n0"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-19",
    "href": "PreProcessing/PreProcessing.slides.html#section-19",
    "title": "Data preprocessing",
    "section": "",
    "text": "Now, to add the dummy variables to the dataset, we use the function concat() from pandas.\n\naugmented_auto = pd.concat([complete_Auto, dummy_data], axis = 1)\naugmented_auto.head()\n\n\n\n\n\n\n\n\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\norigin\nEuropean\nJapanese\n\n\n\n\n0\n8\n307.0\n130\n3504\n12.0\nAmerican\n0\n0\n\n\n1\n8\n350.0\n165\n3693\n11.5\nAmerican\n0\n0\n\n\n2\n8\n318.0\n150\n3436\n11.0\nAmerican\n0\n0\n\n\n3\n8\n304.0\n150\n3433\n12.0\nAmerican\n0\n0\n\n\n4\n8\n302.0\n140\n3449\n10.5\nAmerican\n0\n0"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#removing-predictors",
    "href": "PreProcessing/PreProcessing.slides.html#removing-predictors",
    "title": "Data preprocessing",
    "section": "Removing predictors",
    "text": "Removing predictors\n\nThere are potential advantages to removing predictors prior to modeling:\n\nFewer predictors means decreased computational time and complexity.\nIf two predictors are highly-correlated, they are measuring the same underlying information. So, removing one should not compromise the performance of the model.\n\n\nHere, we will see a popular technique to remove predictors."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#between-predictor-correlation",
    "href": "PreProcessing/PreProcessing.slides.html#between-predictor-correlation",
    "title": "Data preprocessing",
    "section": "Between-predictor correlation",
    "text": "Between-predictor correlation\n\nCollinearity is the technical term for the situation where two predictors have a substantial correlation with each other.\n\nIf two or more predictors are highly correlated (either negatively or positively), then methods such as the linear regression model will not work!\n\nTo visualize the severity of collinearity between predictors, we calculate and visualize the correlation matrix."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#example-2-cont.-1",
    "href": "PreProcessing/PreProcessing.slides.html#example-2-cont.-1",
    "title": "Data preprocessing",
    "section": "Example 2 (cont.)",
    "text": "Example 2 (cont.)\n\nWe concentrate on the five numerical predictors in the complete_Auto dataset.\n\n# Select specific variables.\ncomplete_sbAuto = complete_Auto[['cylinders', 'displacement', \n                                 'horsepower', 'weight', \n                                 'acceleration']]"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#correlation-matrix",
    "href": "PreProcessing/PreProcessing.slides.html#correlation-matrix",
    "title": "Data preprocessing",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\nIn Python, we calculate the correlation matrix using the command below.\n\ncorrelation_matrix = complete_sbAuto.corr()\nprint(correlation_matrix)\n\n              cylinders  displacement  horsepower    weight  acceleration\ncylinders      1.000000      0.950823    0.842983  0.897527     -0.504683\ndisplacement   0.950823      1.000000    0.897257  0.932994     -0.543800\nhorsepower     0.842983      0.897257    1.000000  0.864538     -0.689196\nweight         0.897527      0.932994    0.864538  1.000000     -0.416839\nacceleration  -0.504683     -0.543800   -0.689196 -0.416839      1.000000"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-20",
    "href": "PreProcessing/PreProcessing.slides.html#section-20",
    "title": "Data preprocessing",
    "section": "",
    "text": "Next, we plot the correlation matrix using the function heatmap() from seaborn. The argument annot shows the actual value of the pair-wise correlations, and cmap shows a nice color theme.\n\nplt.figure(figsize=(3,3))\nsns.heatmap(correlation_matrix, cmap='coolwarm', annot = True)"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-21",
    "href": "PreProcessing/PreProcessing.slides.html#section-21",
    "title": "Data preprocessing",
    "section": "",
    "text": "The predictors cylinders and displacement are highly correlated. In fact, their correlation is 0.95."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#in-practice",
    "href": "PreProcessing/PreProcessing.slides.html#in-practice",
    "title": "Data preprocessing",
    "section": "In practice",
    "text": "In practice\n\nWe deal with collinearity by removing the minimum number of predictors to ensure that all pairwise correlations are below a certain threshold, say, 0.75.\n\nWe can identify the variables that are highly correlated using quite complex code. However, here we will do it manually using the correlation map.\n\n# Dataset without highly correlated predictors.\nreduced_auto = complete_sbAuto[ ['weight', 'acceleration']]"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#predictors-with-different-units",
    "href": "PreProcessing/PreProcessing.slides.html#predictors-with-different-units",
    "title": "Data preprocessing",
    "section": "Predictors with different units",
    "text": "Predictors with different units\nMany good predictive models have issues with numeric predictors with different units:\n\nMethods such as K-nearest neighbors are based on the distance between observations. If the predictors are on different units or scales, then some predictors will have a larger weight for computing the distance.\nOther methods such as LASSO use the variances of the predictors in their calculations. Predictors with different scales will have different variances and so, those with a higher variance will play a bigger role in the calculations."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-22",
    "href": "PreProcessing/PreProcessing.slides.html#section-22",
    "title": "Data preprocessing",
    "section": "",
    "text": "In a nutshell, some predictors will have a higher impact in the model due to its unit and not its information provided to it."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#standarization",
    "href": "PreProcessing/PreProcessing.slides.html#standarization",
    "title": "Data preprocessing",
    "section": "Standarization",
    "text": "Standarization\n\nStandardization refers to centering and scaling each numeric predictor individually. It puts every predictor on the same scale.\nTo center a predictor variable, the average predictor value is subtracted from all the values.\nTherefore, the centered predictor has a zero mean (that is, its average value is zero)."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-23",
    "href": "PreProcessing/PreProcessing.slides.html#section-23",
    "title": "Data preprocessing",
    "section": "",
    "text": "To scale a predictor, each of its value is divided by its standard deviation.\nScaling the data coerce the values to have a common standard deviation of one.\nIn mathematical terms, we standardize a predictor as:\n\\[{\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2}},\\]\nwith \\(\\bar{X} = \\sum_{i=1}^n \\frac{x_i}{n}\\)."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#example-2-cont.-2",
    "href": "PreProcessing/PreProcessing.slides.html#example-2-cont.-2",
    "title": "Data preprocessing",
    "section": "Example 2 (cont.)",
    "text": "Example 2 (cont.)\nWe use on the five numerical predictors in the complete_sbAuto dataset.\n\ncomplete_sbAuto.head()\n\n\n\n\n\n\n\n\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\n\n\n\n\n0\n8\n307.0\n130\n3504\n12.0\n\n\n1\n8\n350.0\n165\n3693\n11.5\n\n\n2\n8\n318.0\n150\n3436\n11.0\n\n\n3\n8\n304.0\n150\n3433\n12.0\n\n\n4\n8\n302.0\n140\n3449\n10.5"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#two-predictors-in-original-units",
    "href": "PreProcessing/PreProcessing.slides.html#two-predictors-in-original-units",
    "title": "Data preprocessing",
    "section": "Two predictors in original units",
    "text": "Two predictors in original units\nConsider the complete_sbAuto dataset created previously. Consider two points in the plot: \\((175, 5140)\\) and \\((69, 1613)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distance between these points is \\(\\sqrt{(69 - 175)^2 + (1613-5140)^2}\\) \\(= \\sqrt{11236 + 12439729}\\) \\(= 3528.592\\)."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#standarization-in-python",
    "href": "PreProcessing/PreProcessing.slides.html#standarization-in-python",
    "title": "Data preprocessing",
    "section": "Standarization in Python",
    "text": "Standarization in Python\n\nTo standardize numerical predictors, we use the function StandardScaler(). Moreover, we apply the function to the variables using the function fit_transform().\n\n\nscaler = StandardScaler()\nXs = scaler.fit_transform(complete_sbAuto)"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#section-24",
    "href": "PreProcessing/PreProcessing.slides.html#section-24",
    "title": "Data preprocessing",
    "section": "",
    "text": "Unfortunately, the resulting object is not a pandas data frame. We then convert this object to this format. \n\nscaled_df = pd.DataFrame(Xs, columns = complete_sbAuto.columns)\nscaled_df.head()\n\n\n\n\n\n\n\n\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\n\n\n\n\n0\n1.483947\n1.077290\n0.664133\n0.620540\n-1.285258\n\n\n1\n1.483947\n1.488732\n1.574594\n0.843334\n-1.466724\n\n\n2\n1.483947\n1.182542\n1.184397\n0.540382\n-1.648189\n\n\n3\n1.483947\n1.048584\n1.184397\n0.536845\n-1.285258\n\n\n4\n1.483947\n1.029447\n0.924265\n0.555706\n-1.829655"
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#two-predictors-in-standardized-units",
    "href": "PreProcessing/PreProcessing.slides.html#two-predictors-in-standardized-units",
    "title": "Data preprocessing",
    "section": "Two predictors in standardized units",
    "text": "Two predictors in standardized units\nIn the new scale, the two points are now: \\((1.82, 2.53)\\) and \\((-0.91, -1.60)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distance between these points is \\(\\sqrt{(-0.91 - 1.82)^2 + (-1.60-2.53)^2}\\) \\(= \\sqrt{7.45 + 17.05} = 4.95\\)."
  },
  {
    "objectID": "PreProcessing/PreProcessing.slides.html#discussion",
    "href": "PreProcessing/PreProcessing.slides.html#discussion",
    "title": "Data preprocessing",
    "section": "Discussion",
    "text": "Discussion\n\nStandardized predictors are generally used to improve the numerical stability of some calculations.\nIt is generally recommended to always standardize numeric predictors. Perhaps the only exception would be if we consider a linear regression model.\nA drawback of these transformations is the loss of interpretability since the data are no longer in the original units.\nStandardizing the predictors does not affect their correlation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IN5148: Statistics and Data Science with Applications in Engineering",
    "section": "",
    "text": "Course topics\n\nA. Introduction to Data Science and Python\n\nIntroduction to Data Science (slides)\nIntroduction to Python and Pandas (slides) (colab)\nData Types and Visualization (slides) (colab)\nPreprocessing (slides) (colab)\n\n\n\nB. Unsupervised Learning\n\nClustering Methods (slides) (colab)\nPrincipal Component Analysis (slides) (colab)\n\n\n\nC. Supervised Learning for Regression\n\nSupervised Learning and Linear Regression (slides) (colab)\nModel Inference and Evaluation (slides) (colab)\nAdditional Topics (slides) (colab)\n\n\n\nD. Supervised Learning for Classification\n\nDecision trees for classification (slides) (colab)\nK-nearest neighbors (slides) (colab)\nEnsemble methods (slides) (colab)\nNeural Networks\n\n\n\n\nAbout the author\nAlan R. Vazquez (website) is an assistant professor in the Department of Industrial Engineering at Tecnologico de Monterrey, Monterrey campus.\n\nLicense\nIN5148: Statistics and Data Science with Applications in Engineering © 2026 by Alan R. Vazquez is licensed under CC BY-NC-SA 4.0",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.slides.html#course-topics",
    "href": "index.slides.html#course-topics",
    "title": "IN5148: Statistics and Data Science with Applications in Engineering",
    "section": "Course topics",
    "text": "Course topics\nA. Introduction to Data Science and Python\n\nIntroduction to Data Science (slides)\nIntroduction to Python and Pandas (slides) (colab)\nData Types and Visualization (slides) (colab)\nPreprocessing (slides) (colab)\n\nB. Unsupervised Learning\n\nClustering Methods (slides) (colab)\nPrincipal Component Analysis (slides) (colab)\n\nC. Supervised Learning for Regression\n\nSupervised Learning and Linear Regression (slides) (colab)\nModel Inference and Evaluation (slides) (colab)\nAdditional Topics (slides) (colab)\n\nD. Supervised Learning for Classification\n\nDecision trees for classification (slides) (colab)\nK-nearest neighbors (slides) (colab)\nEnsemble methods (slides) (colab)\nNeural Networks"
  },
  {
    "objectID": "index.slides.html#about-the-author",
    "href": "index.slides.html#about-the-author",
    "title": "IN5148: Statistics and Data Science with Applications in Engineering",
    "section": "About the author",
    "text": "About the author\nAlan R. Vazquez (website) is an assistant professor in the Department of Industrial Engineering at Tecnologico de Monterrey, Monterrey campus.\nLicense\nIN5148: Statistics and Data Science with Applications in Engineering © 2026 by Alan R. Vazquez is licensed under CC BY-NC-SA 4.0"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#agenda",
    "href": "UnsupervisedLearning/Clustering.slides.html#agenda",
    "title": "Clustering Methods",
    "section": "Agenda",
    "text": "Agenda\n\n\nUnsupervised Learning\nClustering Methods\nK-Means Method\nHierarchical Clustering"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#types-of-learning",
    "href": "UnsupervisedLearning/Clustering.slides.html#types-of-learning",
    "title": "Clustering Methods",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#types-of-learning-1",
    "href": "UnsupervisedLearning/Clustering.slides.html#types-of-learning-1",
    "title": "Clustering Methods",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#unsupervised-learning-1",
    "href": "UnsupervisedLearning/Clustering.slides.html#unsupervised-learning-1",
    "title": "Clustering Methods",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nGoal: organize or group data to gain insights. It answers questions like these\n\nIs there an informative way to visualize the data?\nCan we discover subgroups among variables or observations?\n\n\nUnsupervised learning is more challenging than supervised learning because it is subjective and there is no simple objective for the analysis, such as predicting a response.\n\n\nIt is also known as exploratory data analysis."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#examples-of-unsupervised-learning",
    "href": "UnsupervisedLearning/Clustering.slides.html#examples-of-unsupervised-learning",
    "title": "Clustering Methods",
    "section": "Examples of Unsupervised Learning",
    "text": "Examples of Unsupervised Learning\n\n\nMarketing. Identify a segment of customers with a high tendency to purchase a specific product.\nRetail. Group customers based on their preferences, style, clothing choices, and store preferences.\nMedical Science. Facilitate the efficient diagnosis and treatment of patients, as well as the discovery of new drugs.\nSociology. Classify people based on their demographics, lifestyle, socioeconomic status, etc."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#unsupervised-learning-methods",
    "href": "UnsupervisedLearning/Clustering.slides.html#unsupervised-learning-methods",
    "title": "Clustering Methods",
    "section": "Unsupervised learning methods",
    "text": "Unsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#unsupervised-learning-methods-1",
    "href": "UnsupervisedLearning/Clustering.slides.html#unsupervised-learning-methods-1",
    "title": "Clustering Methods",
    "section": "Unsupervised learning methods",
    "text": "Unsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#clustering-methods-1",
    "href": "UnsupervisedLearning/Clustering.slides.html#clustering-methods-1",
    "title": "Clustering Methods",
    "section": "Clustering methods",
    "text": "Clustering methods\n\nThey group data in different ways to discover groups with common traits.\n\nTwo classic clustering methods are:\n\nK-Means Method. We seek to divide the observations into K groups.\nHierarchical Clustering. We divide the n observations into 1 group, 2 groups, 3 groups, …, up to n groups. We visualize the divisions using a graph called a dendrogram."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#before-we-move-on",
    "href": "UnsupervisedLearning/Clustering.slides.html#before-we-move-on",
    "title": "Clustering Methods",
    "section": "Before we move on…",
    "text": "Before we move on…\n\nlet’s import the data science libraries into Python.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, sklearn, and scipy libraries in Python."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#example-1",
    "href": "UnsupervisedLearning/Clustering.slides.html#example-1",
    "title": "Clustering Methods",
    "section": "Example 1",
    "text": "Example 1\n\nThe “penguins.xlsx” database contains data on 342 penguins in Antarctica. The data includes:\n\n\n\n\nBill length in millimeters.\nBill depth in millimeters.\nFlipper length in millimeters.\nBody mass in grams."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#data",
    "href": "UnsupervisedLearning/Clustering.slides.html#data",
    "title": "Clustering Methods",
    "section": "Data",
    "text": "Data\n\n\npenguins_data = pd.read_excel(\"penguins.xlsx\")\npenguins_data.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#data-visualization",
    "href": "UnsupervisedLearning/Clustering.slides.html#data-visualization",
    "title": "Clustering Methods",
    "section": "Data visualization",
    "text": "Data visualization\nCan we group penguins based on their characteristics?\n\n\nCode\nplt.figure(figsize=(8, 5)) # Set figure size.\nsns.scatterplot(data=penguins_data, x=\"bill_depth_mm\", y=\"bill_length_mm\") # Define type of plot.\nplt.show() # Display the plot."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#the-k-means-method",
    "href": "UnsupervisedLearning/Clustering.slides.html#the-k-means-method",
    "title": "Clustering Methods",
    "section": "The K-Means method",
    "text": "The K-Means method\n\nGoal: Find K groups of observations such that each observation is in a different group."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-1",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-1",
    "title": "Clustering Methods",
    "section": "",
    "text": "For this, the method requires two elements:\n\nA measure of “closeness” between observations.\nAn algorithm that groups observations that are close to each other.\n\n\nGood clustering is one in which observations within a group are close together and observations in different groups are far apart."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#how-do-we-measure-the-distance-between-observations",
    "href": "UnsupervisedLearning/Clustering.slides.html#how-do-we-measure-the-distance-between-observations",
    "title": "Clustering Methods",
    "section": "How do we measure the distance between observations?",
    "text": "How do we measure the distance between observations?\n\nFor quantitative predictors, we use the Euclidean distance.\nFor example, if we have two predictors \\(X_1\\) and \\(X_2\\) with observations given in the table:\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\n\n\n\n1\n\\(X_{1,1}\\)\n\\(X_{1,2}\\)\n\n\n2\n\\(X_{2,1}\\)\n\\(X_{2,2}\\)"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#euclidean-distance",
    "href": "UnsupervisedLearning/Clustering.slides.html#euclidean-distance",
    "title": "Clustering Methods",
    "section": "Euclidean distance",
    "text": "Euclidean distance\n\n\n\n\\[d = \\sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 }\\]"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-2",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-2",
    "title": "Clustering Methods",
    "section": "",
    "text": "We can extend the Euclidean distance to measure the distance between observations when we have more predictors. For example, with 3 predictors we have\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\n\n\n\n1\n\\(X_{1,1}\\)\n\\(X_{1,2}\\)\n\\(X_{1,3}\\)\n\n\n2\n\\(X_{2,1}\\)\n\\(X_{2,2}\\)\n\\(X_{2,3}\\)\n\n\n\n\nWhere the Euclidean distance is\n\\[d = \\sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 + (X_{1,3} - X_{2,3})^2 }\\]"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#problem-with-euclidean-distance",
    "href": "UnsupervisedLearning/Clustering.slides.html#problem-with-euclidean-distance",
    "title": "Clustering Methods",
    "section": "Problem with Euclidean distance",
    "text": "Problem with Euclidean distance\n\n\nThe Euclidean distance depends on the units of measurement of the predictors!\nPredictors with certain units have greater importance in calculating the distance.\nThis is not good since we want all predictors to have equal importance when calculating the Euclidean distance between two observations.\nThe solution is to standardize the units of the predictors."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#k-means-algorithm",
    "href": "UnsupervisedLearning/Clustering.slides.html#k-means-algorithm",
    "title": "Clustering Methods",
    "section": "K-Means Algorithm",
    "text": "K-Means Algorithm\n\n\n\n\nChoose a value for K, the number of groups.\n\nRandomly assign observations to one of the K groups.\nFind the centroids (average points) of each group.\nReassign observations to the group with the closest centroid.\nRepeat steps 3 and 4 until there are no more changes."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#example-1-cont.",
    "href": "UnsupervisedLearning/Clustering.slides.html#example-1-cont.",
    "title": "Clustering Methods",
    "section": "Example 1 (cont.)",
    "text": "Example 1 (cont.)\n\nLet’s apply the algorithm to the predictors bill_depth_mm and bill_length_mm of the penguins dataset.\n\n\n\n\n\n\n\n\n\nbill_depth_mm\nbill_length_mm\n\n\n\n\n0\n18.7\n39.1\n\n\n1\n17.4\n39.5\n\n\n2\n18.0\n40.3\n\n\n3\n19.3\n36.7\n\n\n4\n20.6\n39.3"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#standarization",
    "href": "UnsupervisedLearning/Clustering.slides.html#standarization",
    "title": "Clustering Methods",
    "section": "Standarization",
    "text": "Standarization\n\nSince the K-means algorithm works with Euclidean distance, we must standardize the predictors before we start.\nIn this way, all of them will be equally informative in the process.\n\n## Standardize\nscaler = StandardScaler()\nXs_penguins = scaler.fit_transform(X_penguins)"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-3",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-3",
    "title": "Clustering Methods",
    "section": "",
    "text": "In Python, we use the KMeans() function of sklearn to apply K-means clustering.\n\nKMeans() tells Python we want to train a K-means clustering algorithm and .fit_predict() actually trains it using the data.\n\n\n# Fit KMeans with 3 clusters\nkmeans = KMeans(n_clusters = 3, random_state = 301655)\nclusters = kmeans.fit_predict(Xs_penguins)\n\nThe argument n_clusters sets the desired number of clusters and random_state allows us to reproduce the analysis."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-4",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-4",
    "title": "Clustering Methods",
    "section": "",
    "text": "The clusters created are contained in the clusters object.\n\n\nCode\nclusters\n\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0,\n       2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2,\n       2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2,\n       0, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2], dtype=int32)"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-5",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-5",
    "title": "Clustering Methods",
    "section": "",
    "text": "To visualize the clusters, we augment the original dataset X_penguins (without standarization) with the clusters object. usign the code below.\n\nclustered_penguins = (X_penguins\n              .assign(Cluster = clusters)\n              )\nclustered_penguins.head()\n\n\n\n\n\n\n\n\nbill_depth_mm\nbill_length_mm\nCluster\n\n\n\n\n0\n18.7\n39.1\n1\n\n\n1\n17.4\n39.5\n1\n\n\n2\n18.0\n40.3\n1\n\n\n3\n19.3\n36.7\n1\n\n\n4\n20.6\n39.3\n1"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-6",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-6",
    "title": "Clustering Methods",
    "section": "",
    "text": "Code\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data = clustered_penguins, x = 'bill_length_mm', y = 'bill_depth_mm', \n                hue = 'Cluster', palette = 'Set1')\nplt.title('K-means Clustering of Penguins')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#the-truth-3-groups-of-penguins",
    "href": "UnsupervisedLearning/Clustering.slides.html#the-truth-3-groups-of-penguins",
    "title": "Clustering Methods",
    "section": "The truth: 3 groups of penguins",
    "text": "The truth: 3 groups of penguins\n\n\nCode\nplt.figure(figsize=(8.5, 5.5))\nsns.scatterplot(data=penguins_data, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n                hue=\"species\", palette = 'Set1') # Define type of plot.\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.tight_layout()\nplt.grid(True)\nplt.show() # Display the plot."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#lets-try-using-more-predictors",
    "href": "UnsupervisedLearning/Clustering.slides.html#lets-try-using-more-predictors",
    "title": "Clustering Methods",
    "section": "Let’s try using more predictors",
    "text": "Let’s try using more predictors\n\n\nX_penguins = penguins_data.filter(['bill_depth_mm', 'bill_length_mm', \n                          'flipper_length_mm', 'body_mass_g'])\n\n## Standardize\nscaler = StandardScaler()\nXs_penguins = scaler.fit_transform(X_penguins)\n\n# Fit KMeans with 3 clusters\nkmeans = KMeans(n_clusters = 3, random_state = 301655)\nclusters = kmeans.fit_predict(Xs_penguins)\n\n# Save new clusters into the original data\nclustered_X = (X_penguins\n              .assign(Cluster = clusters)\n              )"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#these-are-the-three-species",
    "href": "UnsupervisedLearning/Clustering.slides.html#these-are-the-three-species",
    "title": "Clustering Methods",
    "section": "These are the three species",
    "text": "These are the three species\n\n\n\nAdelie\n\n\nGentoo\n\n\nChinstrap"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#determining-the-number-of-clusters",
    "href": "UnsupervisedLearning/Clustering.slides.html#determining-the-number-of-clusters",
    "title": "Clustering Methods",
    "section": "Determining the number of clusters",
    "text": "Determining the number of clusters\n\n\nA simple way to determine the number of clusters is recording the quality of clustering for different numbers of clusters.\nIn sklearn, we can record the inertia of a partition into clusters. Technically, the inertia is the sum of squared distances of observations to their closest cluster center.\nThe lower the intertia the better because this means that all observations are close to their cluster centers overall.\n\n\nAlso known as the Within-Cluster Sum of Squares (WCSS), it calculates how tightly grouped the data points are around their center."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-9",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-9",
    "title": "Clustering Methods",
    "section": "",
    "text": "To record the intertias for different numbers of clusters, we use the code below.\n\ninertias = [] # List to save interia values.\nk_max = 11 # Maximum number of clusters to try.\n\nfor i in range(1,k_max):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(Xs_penguins)\n    inertias.append(kmeans.inertia_)"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-10",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-10",
    "title": "Clustering Methods",
    "section": "",
    "text": "Next, we plot the intertias and look for the elbow in the plot.\nThe elbow represents a number of clusters for which there is no significant improvement in the quality of the clustering.\nIn this case, the number of clusters recommended by this elbow method is 3.\n\n\n\nCode\nplt.figure(figsize=(6, 6))\nplt.plot(range(1,11), inertias, marker='o')\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#comments",
    "href": "UnsupervisedLearning/Clustering.slides.html#comments",
    "title": "Clustering Methods",
    "section": "Comments",
    "text": "Comments\n\nSelecting the number of clusters K is more of an art than a science. You’d better get K right, or you’ll be detecting patterns where none really exist.\nWe need to standardize all predictors.\nThe performance of K-means clustering is affected by the presence of outliers.\nThe algorithm’s solution is sensitive to the starting point. Because of this, it is typically run multiple times, and the best clustering among all runs is reported."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#hierarchical-clustering-1",
    "href": "UnsupervisedLearning/Clustering.slides.html#hierarchical-clustering-1",
    "title": "Clustering Methods",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\n\n\n\n\n\n\nStart with each observation standing alone in its own group.\nThen, gradually merge the groups that are close together.\nContinue this process until all the observations are in one large group.\nFinally, step back and see which grouping works best."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#essential-elements",
    "href": "UnsupervisedLearning/Clustering.slides.html#essential-elements",
    "title": "Clustering Methods",
    "section": "Essential elements",
    "text": "Essential elements\n\n\nDistance between two observations.\n\nWe use Euclidean distance.\nWe must standardize the predictors!\n\nDistance between two groups."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#distance-between-two-groups",
    "href": "UnsupervisedLearning/Clustering.slides.html#distance-between-two-groups",
    "title": "Clustering Methods",
    "section": "Distance between two groups",
    "text": "Distance between two groups\n\n\n\n\nThe distance between two groups of observations is called linkage.\nThere are several types of linking. The most commonly used are:\n\nComplete linkage\nAverage linkage"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#complete-linkage",
    "href": "UnsupervisedLearning/Clustering.slides.html#complete-linkage",
    "title": "Clustering Methods",
    "section": "Complete linkage",
    "text": "Complete linkage\nThe distance between groups is measured using the largest distance between observations."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#average-linkage",
    "href": "UnsupervisedLearning/Clustering.slides.html#average-linkage",
    "title": "Clustering Methods",
    "section": "Average linkage",
    "text": "Average linkage\nThe distance between groups is the average of all the distances between observations."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#hierarchical-clustering-algorithm",
    "href": "UnsupervisedLearning/Clustering.slides.html#hierarchical-clustering-algorithm",
    "title": "Clustering Methods",
    "section": "Hierarchical clustering algorithm",
    "text": "Hierarchical clustering algorithm\n\nThe steps of the algorithm are as follows:\n\nAssign each observation to a cluster.\nMeasure the linkage between all clusters.\nMerge the two most similar clusters.\nThen, merge the next two most similar clusters.\nContinue until all clusters have been merged."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#example-2",
    "href": "UnsupervisedLearning/Clustering.slides.html#example-2",
    "title": "Clustering Methods",
    "section": "Example 2",
    "text": "Example 2\n\nLet’s consider a dataset called “Cereals.xlsx.” The data includes nutritional information for 77 cereals, among other data.\n\ncereal_data = pd.read_excel(\"cereals.xlsx\")"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-11",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-11",
    "title": "Clustering Methods",
    "section": "",
    "text": "Here, we will restrict to 7 numeric predictors.\n\nX_cereal = cereal_data.filter(['calories', 'protein', 'fat', 'sodium', 'fiber',\n                              'carbo', 'sugars', 'potass', 'vitamins'])\nX_cereal.head()\n\n\n\n\n\n\n\n\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\n\n\n\n\n0\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n\n\n1\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n\n\n2\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n\n\n3\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n\n\n4\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#do-not-forget-to-standardize",
    "href": "UnsupervisedLearning/Clustering.slides.html#do-not-forget-to-standardize",
    "title": "Clustering Methods",
    "section": "Do not forget to standardize",
    "text": "Do not forget to standardize\n\nSince the hierarchical clustering algorithm also works with distances, we must standardize the predictors to have an accurate analysis.\n\nscaler = StandardScaler()\nXs_cereal = scaler.fit_transform(X_cereal)"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-12",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-12",
    "title": "Clustering Methods",
    "section": "",
    "text": "Unfortunately, the Agglomerative() function in sklearn is not as user friendly compared to other available functions in Python. In particular, the scipy library has a function called linkage() for hierarchical clustering that works as follows.\n\nClust_Cereal = linkage(Xs_cereal, method = 'complete')\n\nThe argument method sets the type of linkage to be used."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#results-dendrogram",
    "href": "UnsupervisedLearning/Clustering.slides.html#results-dendrogram",
    "title": "Clustering Methods",
    "section": "Results: Dendrogram",
    "text": "Results: Dendrogram\n\n\n\n\n\n\nA dendrogram is a tree diagram that summarizes and visualizes the clustering process.\nObservations are on the horizontal axis and at the bottom of the diagram.\nThe vertical axis shows the distance between groups.\nIt is read from top to bottom."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#what-to-do-with-a-dendrogram",
    "href": "UnsupervisedLearning/Clustering.slides.html#what-to-do-with-a-dendrogram",
    "title": "Clustering Methods",
    "section": "What to do with a dendrogram?",
    "text": "What to do with a dendrogram?\n\n\n\n\nWe draw a horizontal line at a specific height to define the groups.\nThis line defines three groups."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-13",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-13",
    "title": "Clustering Methods",
    "section": "",
    "text": "This line defines 5 groups."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#dendrogram-in-python",
    "href": "UnsupervisedLearning/Clustering.slides.html#dendrogram-in-python",
    "title": "Clustering Methods",
    "section": "Dendrogram in Python",
    "text": "Dendrogram in Python\nTo produce a nice dendrogram in Python, we use the function dendrogram from scipy.\n\n\nCode\nplt.figure(figsize=(8, 4))\ndendrogram(Clust_Cereal, color_threshold=None)\nplt.title('Hierarchical Clustering Dendrogram (Complete Linkage)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#comments-1",
    "href": "UnsupervisedLearning/Clustering.slides.html#comments-1",
    "title": "Clustering Methods",
    "section": "Comments",
    "text": "Comments\n\n\nRemember to standardize the predictors!\nIt’s not easy to choose the correct number of clusters using the dendrogram.\nThe results depend on the linkage measure used.\n\nComplete linkage results in narrower clusters.\nAverage linkage strikes a balance between narrow and thinner clusters.\n\nHierarchical clustering is useful for detecting outliers."
  },
  {
    "objectID": "UnsupervisedLearning/Clustering.slides.html#section-14",
    "href": "UnsupervisedLearning/Clustering.slides.html#section-14",
    "title": "Clustering Methods",
    "section": "",
    "text": "With these methods, there is no single correct answer; any solution that exposes some interesting aspect of the data should be considered.\n\nJames et al. (2017)"
  },
  {
    "objectID": "Tools/Tools1.slides.html#agenda",
    "href": "Tools/Tools1.slides.html#agenda",
    "title": "Introduction to Python and Pandas",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction to Python\nReading data with Python\nData manipulation with pandas"
  },
  {
    "objectID": "Tools/Tools1.slides.html#python",
    "href": "Tools/Tools1.slides.html#python",
    "title": "Introduction to Python and Pandas",
    "section": "Python",
    "text": "Python\n\n\n\n\nA versatile programming language.\nIt is free!\nIt is widely used for data cleaning, data visualization, and data modelling.\nIt can be extended with packages (libraries) developed by other users."
  },
  {
    "objectID": "Tools/Tools1.slides.html#google-colab",
    "href": "Tools/Tools1.slides.html#google-colab",
    "title": "Introduction to Python and Pandas",
    "section": "Google Colab",
    "text": "Google Colab\nGoogle’s free cloud collaboration platform for creating Python documents.\n\nRun Python and collaborate on Jupyter notebooks for free.\nHarness the power of GPUs for free to accelerate your data science projects.\nEasily save and upload your notebooks to Google Drive."
  },
  {
    "objectID": "Tools/Tools1.slides.html#lets-try-a-command-in-python",
    "href": "Tools/Tools1.slides.html#lets-try-a-command-in-python",
    "title": "Introduction to Python and Pandas",
    "section": "Let’s try a command in Python",
    "text": "Let’s try a command in Python\n\nWhat do you think will happen if we run this command?\n\nprint(\"Hello world!\")\n\n\n\n\nHello world!"
  },
  {
    "objectID": "Tools/Tools1.slides.html#lets-try-another-command",
    "href": "Tools/Tools1.slides.html#lets-try-another-command",
    "title": "Introduction to Python and Pandas",
    "section": "Let’s try another command",
    "text": "Let’s try another command\n\nWhat do you think will happen if we run this command?\n\nsum([1, 5, 10])\n\n\n\n\n16"
  },
  {
    "objectID": "Tools/Tools1.slides.html#use-python-as-a-basic-calculator",
    "href": "Tools/Tools1.slides.html#use-python-as-a-basic-calculator",
    "title": "Introduction to Python and Pandas",
    "section": "Use Python as a basic calculator",
    "text": "Use Python as a basic calculator\n\n\n5 + 1\n\n6\n\n\n\n10 - 3\n\n7\n\n\n\n2 * 4\n\n8\n\n\n\n9 / 3\n\n3.0"
  },
  {
    "objectID": "Tools/Tools1.slides.html#comments",
    "href": "Tools/Tools1.slides.html#comments",
    "title": "Introduction to Python and Pandas",
    "section": "Comments",
    "text": "Comments\n\nSometimes we write things in the coding window that we want Python to ignore. These are called comments and start with #.\n\nPython will ignore the comments and just execute the code.\n\n# you can put whatever after #\n# for example... blah blah blah\n\n\nSi desea escribir un comentario que ocupe más de una línea, es una buena idea poner un # al principio de cada línea."
  },
  {
    "objectID": "Tools/Tools1.slides.html#introduction-to-functions-in-python",
    "href": "Tools/Tools1.slides.html#introduction-to-functions-in-python",
    "title": "Introduction to Python and Pandas",
    "section": "Introduction to functions in Python",
    "text": "Introduction to functions in Python\n\nOne of the best things about Python is that there are many built-in commands you can use. These are called functions.\n\nFunctions have two basic parts:\n\nThe first part is the name of the function (for example, sum).\nThe second part is the input to the function, which goes inside the parentheses (sum([1, 5, 15]))."
  },
  {
    "objectID": "Tools/Tools1.slides.html#python-is-strict",
    "href": "Tools/Tools1.slides.html#python-is-strict",
    "title": "Introduction to Python and Pandas",
    "section": "Python is strict",
    "text": "Python is strict\nPython, like all programming languages, is very strict. For example, if you write\n\nsum([1, 100])\n\n101\n\n\nit will tell you the answer, 101.\n\nBut if you write\n\nSum([1, 100])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 Sum([1, 100])\n\nNameError: name 'Sum' is not defined\n\n\n\nwith the “s” capitalized, he will act like he has no idea what we are talking about!\n\nlo mismo si olvidas incluir un parentesis"
  },
  {
    "objectID": "Tools/Tools1.slides.html#save-your-work-in-python-objects",
    "href": "Tools/Tools1.slides.html#save-your-work-in-python-objects",
    "title": "Introduction to Python and Pandas",
    "section": "Save your work in Python objects",
    "text": "Save your work in Python objects\n\nVirtually anything, including the results of any Python function, can be saved in an object.\nThis is accomplished by using an assignment operator, which can be an equals symbol (=).\n\nYou can make up any name you want for a Python object. However, there are two basic rules for this:\n\nIt must be different from a function name in Python.\nIt should be as specific as possible."
  },
  {
    "objectID": "Tools/Tools1.slides.html#for-example",
    "href": "Tools/Tools1.slides.html#for-example",
    "title": "Introduction to Python and Pandas",
    "section": "For example",
    "text": "For example\n\n\n# This code will assign the number 18\n# to the object called my_favorite_number\n\nmy_favorite_number = 18\n\nAfter running this code, nothing happens. But if we run the object on its own, we can see what’s inside it.\n\nmy_favorite_number\n\n18\n\n\nYou can also use print(my_favorite_number)."
  },
  {
    "objectID": "Tools/Tools1.slides.html#lists",
    "href": "Tools/Tools1.slides.html#lists",
    "title": "Introduction to Python and Pandas",
    "section": "Lists",
    "text": "Lists\n\nSo far we have used Python objects to store a single number. But in statistics we are dealing with variation, which by definition needs more than one number.\n\nA Python object can also store a complete set of numbers, called a list.\nYou can think of a list as a vector of numbers (or values).\n\n\nThe [] command can be used to combine several individual values into a list.\n\npuedes pensar que el c es por combinar"
  },
  {
    "objectID": "Tools/Tools1.slides.html#for-example-1",
    "href": "Tools/Tools1.slides.html#for-example-1",
    "title": "Introduction to Python and Pandas",
    "section": "For example",
    "text": "For example\n\nThis code creates two vectors\n\nmy_list = [1, 2, 3, 4, 5]\nmy_list_2 = [10, 10, 10, 10, 10]\n\nLet’s see its content\n\nmy_list\n\n[1, 2, 3, 4, 5]\n\n\n\nmy_list_2\n\n[10, 10, 10, 10, 10]"
  },
  {
    "objectID": "Tools/Tools1.slides.html#operations",
    "href": "Tools/Tools1.slides.html#operations",
    "title": "Introduction to Python and Pandas",
    "section": "Operations",
    "text": "Operations\n\nWe can do simple operations with vectors. For example, we can sum all the elements of a list.\n\nmy_list = [1, 2, 3, 4, 5]\nsum(my_list)\n\n15"
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing",
    "href": "Tools/Tools1.slides.html#indexing",
    "title": "Introduction to Python and Pandas",
    "section": "Indexing",
    "text": "Indexing\n\nWe can index a position in the vector using square brackets with a number like this: [1].\nSo, if we wanted to print the contents of the first position in my_list, we could write\n\nmy_list[1]\n\n2\n\n\nAn feature of Python is that the first element of a list or vector is indexed using the number 0.\n\nmy_list[0]\n\n1"
  },
  {
    "objectID": "Tools/Tools1.slides.html#a-little-more-about-objects-in-python",
    "href": "Tools/Tools1.slides.html#a-little-more-about-objects-in-python",
    "title": "Introduction to Python and Pandas",
    "section": "A little more about objects in Python",
    "text": "A little more about objects in Python\n\nYou can think of Python objects as containers that hold values.\nA Python object can hold a single value, or it can hold a group of values (as in a vector).\nSo far, we’ve only put numbers into Python objects.\n\n\nPython objects can actually contain three types of values: numbers, characters, and booleans."
  },
  {
    "objectID": "Tools/Tools1.slides.html#character-values",
    "href": "Tools/Tools1.slides.html#character-values",
    "title": "Introduction to Python and Pandas",
    "section": "Character values",
    "text": "Character values\n\nCharacters are made up of text, such as words or sentences. An example of a list with characters as elements is:\n\n\nmany_greetings = [\"hi\", \"hello\", \"hola\", \"bonjour\", \"ni hao\", \"merhaba\"]\nmany_greetings\n\n['hi', 'hello', 'hola', 'bonjour', 'ni hao', 'merhaba']\n\n\n\n\nIt is important to know that numbers can also be treated as characters, depending on the context.\nFor example, when 20 is enclosed in quotes (\"20\") it will be treated as a character value, even though it encloses a number in quotes."
  },
  {
    "objectID": "Tools/Tools1.slides.html#boolean-values",
    "href": "Tools/Tools1.slides.html#boolean-values",
    "title": "Introduction to Python and Pandas",
    "section": "Boolean values",
    "text": "Boolean values\n\nBoolean values are True or False.\nWe may have a question like:\n\nIs the first element of the vector many_greetings \"hola\"?\n\n\nWe can ask Python to find out and return the answer True or False.\n\nmany_greetings[1] == \"hola\"\n\nFalse"
  },
  {
    "objectID": "Tools/Tools1.slides.html#logical-operators",
    "href": "Tools/Tools1.slides.html#logical-operators",
    "title": "Introduction to Python and Pandas",
    "section": "Logical operators",
    "text": "Logical operators\n\nMost of the questions we ask Python to answer with True or False involve comparison operators like &gt;, &lt;, &gt;=, &lt;=, and ==.\nThe double == sign checks whether two values are equal. There is even a comparison operator to check whether values are not equal: !=.\nFor example, 5 != 3 is a True statement."
  },
  {
    "objectID": "Tools/Tools1.slides.html#common-logical-operators",
    "href": "Tools/Tools1.slides.html#common-logical-operators",
    "title": "Introduction to Python and Pandas",
    "section": "Common logical operators",
    "text": "Common logical operators\n\n\n&gt; (larger than)\n&gt;= (larger than or equal to)\n&lt; (smaller than)\n&lt;= (smaller than or equal to)\n== (equal to)\n!= (not equal to)"
  },
  {
    "objectID": "Tools/Tools1.slides.html#question",
    "href": "Tools/Tools1.slides.html#question",
    "title": "Introduction to Python and Pandas",
    "section": "Question",
    "text": "Question\n\nRead this code and predict its response. Then, run the code in Google Colab and validate if you were correct.\n\nA = 1\nB = 5\ncompare = A &gt; B\ncompare"
  },
  {
    "objectID": "Tools/Tools1.slides.html#programming-culture-trial-and-error",
    "href": "Tools/Tools1.slides.html#programming-culture-trial-and-error",
    "title": "Introduction to Python and Pandas",
    "section": "Programming culture: Trial and error",
    "text": "Programming culture: Trial and error\n\nThe best way to learn programming is to try things out and see what happens. Write some code, run it, and think about why it didn’t work.\nThere are many ways to make small mistakes in programming (for example, typing a capital letter when a lowercase letter is needed).\nWe often have to find these mistakes through trial and error."
  },
  {
    "objectID": "Tools/Tools1.slides.html#python-libraries",
    "href": "Tools/Tools1.slides.html#python-libraries",
    "title": "Introduction to Python and Pandas",
    "section": "Python libraries",
    "text": "Python libraries\nLibraries are the fundamental units of reproducible Python code. They include reusable Python functions, documentation describing how to use them, and sample data.\n\nIn this course, we will be working mostly with the following libraries:\n\npandas for data manipulation\nmatplotlib and seaborn for data visualization\nstatsmodels and scikit-learn for data modelling"
  },
  {
    "objectID": "Tools/Tools1.slides.html#data-organization",
    "href": "Tools/Tools1.slides.html#data-organization",
    "title": "Introduction to Python and Pandas",
    "section": "Data organization",
    "text": "Data organization\nIn data science, we organize data into rows and columns.\n\n    Condition  Age   Wt    Wt2\n1  Uninformed   35  136  135.8\n2  Uninformed   45  162  161.8\n3    Informed   52  117  116.8\n4    Informed   29  184  182.8\n5  Uninformed   38  134  136.6\n6    Informed   39  189  183.2\n\n\nThe rows are the sampled cases. In this example, the rows are housekeepers from different hotels. There are six rows, so there are six housekeepers in this data set.\n\n\nDepending on the study, the rows could be people, states, couples, mice—any case you’re taking a sample from to study."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section",
    "href": "Tools/Tools1.slides.html#section",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "The columns represent variables or attributes of each case that were measured.\n\n    Condition  Age   Wt    Wt2\n1  Uninformed   35  136  135.8\n2  Uninformed   45  162  161.8\n3    Informed   52  117  116.8\n4    Informed   29  184  182.8\n5  Uninformed   38  134  136.6\n6    Informed   39  189  183.2\n\n\n\nIn this study, housekeepers were either informed or not that their daily work of cleaning hotel rooms was equivalent to getting adequate exercise for good health.\n\n\n\nSo one of the variables, Condition, indicates whether they were informed of this fact or not."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-1",
    "href": "Tools/Tools1.slides.html#section-1",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "Other variables include the age of the housekeeper (Age), her weight before starting the study (Wt), and her weight at the end of the study (Wt2), measured four weeks later.\n\nTherefore, the values in each row represent the values of that particular case in each of the variables measured.\n    Condition  Age   Wt    Wt2\n1  Uninformed   35  136  135.8\n2  Uninformed   45  162  161.8\n3    Informed   52  117  116.8\n4    Informed   29  184  182.8\n5  Uninformed   38  134  136.6\n6    Informed   39  189  183.2\n\n¿Cuántas variables hay en este conjunto de datos?"
  },
  {
    "objectID": "Tools/Tools1.slides.html#loading-data-in-python",
    "href": "Tools/Tools1.slides.html#loading-data-in-python",
    "title": "Introduction to Python and Pandas",
    "section": "Loading data in Python",
    "text": "Loading data in Python\nIn this course, we will assume that data is stored in an Excel file with the above organization. As an example, let’s use the file penguins.xlsx.\n\n\n\n\n\n\n\nThe file must be previously uploaded to Google Colab."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-2",
    "href": "Tools/Tools1.slides.html#section-2",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "The dataset penguins.xlsx contains data from penguins living in three islands."
  },
  {
    "objectID": "Tools/Tools1.slides.html#pandas-library",
    "href": "Tools/Tools1.slides.html#pandas-library",
    "title": "Introduction to Python and Pandas",
    "section": "pandas library",
    "text": "pandas library\n\n\n\n\n\n\n\n\n\n\npandas is an open-source Python library for data manipulation and analysis.\nIt is built on top of numpy for high-performance data operations..\nIt allows the user to import, clean, transform, and analyze data efficiently\nhttps://pandas.pydata.org/"
  },
  {
    "objectID": "Tools/Tools1.slides.html#importing-pandas",
    "href": "Tools/Tools1.slides.html#importing-pandas",
    "title": "Introduction to Python and Pandas",
    "section": "Importing pandas",
    "text": "Importing pandas\nFortunately, the pandas library is already pre-installed in Google Colab.\n\nHowever, we need to inform Google Colab that we want to use pandas and its functions using the following command:\n\nimport pandas as pd\n\n\nThe command as pd allows us to have a short name for pandas. To use a function of pandas, we use the command pd.function()."
  },
  {
    "objectID": "Tools/Tools1.slides.html#loading-data-using-pandas",
    "href": "Tools/Tools1.slides.html#loading-data-using-pandas",
    "title": "Introduction to Python and Pandas",
    "section": "Loading data using pandas",
    "text": "Loading data using pandas\n\nThe following code shows how to read the data in the file “penguins.xlsx” into Python.\n\n# Load the Excel file into a pandas DataFrame.\npenguins_data = pd.read_excel(\"penguins.xlsx\")"
  },
  {
    "objectID": "Tools/Tools1.slides.html#the-function-head",
    "href": "Tools/Tools1.slides.html#the-function-head",
    "title": "Introduction to Python and Pandas",
    "section": "The function head()",
    "text": "The function head()\nThe function head() allows you to print the first rows of a pandas data frame.\n\n# Print the first 4 rows of the dataset.\npenguins_data.head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing-variables-a-dataset",
    "href": "Tools/Tools1.slides.html#indexing-variables-a-dataset",
    "title": "Introduction to Python and Pandas",
    "section": "Indexing variables a dataset",
    "text": "Indexing variables a dataset\nWe can select a specific variables of a data frame using the syntaxis below.\n\npenguins_data['bill_length_mm']\n\n0      39.1\n1      39.5\n2      40.3\n3       NaN\n4      36.7\n       ... \n339    55.8\n340    43.5\n341    49.6\n342    50.8\n343    50.2\nName: bill_length_mm, Length: 344, dtype: float64\n\n\nHere, we selected the variable bill_length_mm in the penguins_data dataset."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-3",
    "href": "Tools/Tools1.slides.html#section-3",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "To index multiple variables of a data frame, we put the names of the variables in a list object. For example, we select bill_length_mm, species, and island as follows:\n\nsub_penguins_data = penguins_data[ ['bill_length_mm',  'species', 'island'] ]\nsub_penguins_data.head()\n\n\n\n\n\n\n\n\nbill_length_mm\nspecies\nisland\n\n\n\n\n0\n39.1\nAdelie\nTorgersen\n\n\n1\n39.5\nAdelie\nTorgersen\n\n\n2\n40.3\nAdelie\nTorgersen\n\n\n3\nNaN\nAdelie\nTorgersen\n\n\n4\n36.7\nAdelie\nTorgersen"
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing-rows",
    "href": "Tools/Tools1.slides.html#indexing-rows",
    "title": "Introduction to Python and Pandas",
    "section": "Indexing rows",
    "text": "Indexing rows\nTo index rows in a dataset, we use the argument loc from pandas. For example, we select the rows 3 to 6 of the penguins_dataset dataset:\n\nrows_penguins_data = penguins_data.loc[2:5]\nrows_penguins_data\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-4",
    "href": "Tools/Tools1.slides.html#section-4",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "rows_penguins_data\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n\n\n\n\n\nNote that the index 2 and 5 refer to observations 3 and 7, respectively, in the dataset. This is because the first index in Python is 0."
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing-rows-and-columns",
    "href": "Tools/Tools1.slides.html#indexing-rows-and-columns",
    "title": "Introduction to Python and Pandas",
    "section": "Indexing rows and columns",
    "text": "Indexing rows and columns\nUsing loc, we can also retrieve a subset from the dataset by selecting specific columns and rows.\n\nsub_rows_pdata = penguins_data.loc[2:5, ['bill_length_mm',  'species', 'island'] ]\nsub_rows_pdata\n\n\n\n\n\n\n\n\nbill_length_mm\nspecies\nisland\n\n\n\n\n2\n40.3\nAdelie\nTorgersen\n\n\n3\nNaN\nAdelie\nTorgersen\n\n\n4\n36.7\nAdelie\nTorgersen\n\n\n5\n39.3\nAdelie\nTorgersen"
  },
  {
    "objectID": "Tools/Tools1.slides.html#chaining-operations-with-pandas",
    "href": "Tools/Tools1.slides.html#chaining-operations-with-pandas",
    "title": "Introduction to Python and Pandas",
    "section": "Chaining operations with pandas",
    "text": "Chaining operations with pandas\nOne of the most important techniques in pandas is chaining, which allows for cleaner and more readable data manipulation.\nThe general structure of chaining looks like this:"
  },
  {
    "objectID": "Tools/Tools1.slides.html#key-pandas-methods",
    "href": "Tools/Tools1.slides.html#key-pandas-methods",
    "title": "Introduction to Python and Pandas",
    "section": "Key pandas methods",
    "text": "Key pandas methods\npandas provides methods or functions to solve common data manipulation tasks:\n\n.filter() selects specific columns or rows.\n.query() filters observations based on conditions.\n.assign() adds new variables that are functions of existing variables.\n.sort_values() changes the order of rows.\n.agg() reduces multiple values to a single numerical summary."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-5",
    "href": "Tools/Tools1.slides.html#section-5",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "To practice, we will use the dataset penguins_data."
  },
  {
    "objectID": "Tools/Tools1.slides.html#example",
    "href": "Tools/Tools1.slides.html#example",
    "title": "Introduction to Python and Pandas",
    "section": "Example",
    "text": "Example\nLet’s load the dataset and the pandas library.\n\nimport pandas as pd\n\n# Load the Excel file into a pandas DataFrame.\npenguins_data = pd.read_excel(\"penguins.xlsx\")\n\n# Preview the dataset.\npenguins_data.head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#selecting-columns-with-.filter",
    "href": "Tools/Tools1.slides.html#selecting-columns-with-.filter",
    "title": "Introduction to Python and Pandas",
    "section": "Selecting columns with .filter()",
    "text": "Selecting columns with .filter()\nSelect the columns species, body_mass_g and sex.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n).head()\n\n\n\n\n\n\n\n\nspecies\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\n3750.0\nmale\n\n\n1\nAdelie\n3800.0\nfemale\n\n\n2\nAdelie\n3250.0\nfemale\n\n\n3\nAdelie\nNaN\nNaN\n\n\n4\nAdelie\n3450.0\nfemale"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-6",
    "href": "Tools/Tools1.slides.html#section-6",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "The axis argument tells .filter() whether to select rows (0) or columns (1) from the dataframe.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n).head()\n\n\n\nThe .head() command allows us to print the first six rows of the newly produced dataframe. We must remove it to have the entire new dataframe."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-7",
    "href": "Tools/Tools1.slides.html#section-7",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "We can also use .filter() to select rows too. To this end, we set axis = 1. We can select specific rows, such as 0 and 10.\n\n(penguins_data\n  .filter([0, 10], axis = 0)\n)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n10\nAdelie\nTorgersen\n37.8\n17.1\n186.0\n3300.0\nNaN\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-8",
    "href": "Tools/Tools1.slides.html#section-8",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "Or, we can select a set of rows using the function range(). For example, let’s select the first 5 rows.\n\n(penguins_data\n  .filter(range(5), axis = 0)\n)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#filtering-rows-with-.query",
    "href": "Tools/Tools1.slides.html#filtering-rows-with-.query",
    "title": "Introduction to Python and Pandas",
    "section": "Filtering rows with .query()",
    "text": "Filtering rows with .query()\n\nAn alternative way of selecting rows is .query(). Compared to .filter(), .query() allows us to filter the data using statements or queries involving the variables.\n\nFor example, let’s filter the data for the species “Gentoo.”\n\n(penguins_data\n  .query(\"species == 'Gentoo'\")\n)"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-9",
    "href": "Tools/Tools1.slides.html#section-9",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "(penguins_data\n  .query(\"species == 'Gentoo'\")\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n152\nGentoo\nBiscoe\n46.1\n13.2\n211.0\n4500.0\nfemale\n2007\n\n\n153\nGentoo\nBiscoe\n50.0\n16.3\n230.0\n5700.0\nmale\n2007\n\n\n154\nGentoo\nBiscoe\n48.7\n14.1\n210.0\n4450.0\nfemale\n2007\n\n\n155\nGentoo\nBiscoe\n50.0\n15.2\n218.0\n5700.0\nmale\n2007\n\n\n156\nGentoo\nBiscoe\n47.6\n14.5\n215.0\n5400.0\nmale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-10",
    "href": "Tools/Tools1.slides.html#section-10",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "We can also filter the data to get penguins with a body mass greater than 5000g.\n\n(penguins_data\n  .query(\"body_mass_g &gt; 5000\")\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n153\nGentoo\nBiscoe\n50.0\n16.3\n230.0\n5700.0\nmale\n2007\n\n\n155\nGentoo\nBiscoe\n50.0\n15.2\n218.0\n5700.0\nmale\n2007\n\n\n156\nGentoo\nBiscoe\n47.6\n14.5\n215.0\n5400.0\nmale\n2007\n\n\n159\nGentoo\nBiscoe\n46.7\n15.3\n219.0\n5200.0\nmale\n2007\n\n\n161\nGentoo\nBiscoe\n46.8\n15.4\n215.0\n5150.0\nmale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-11",
    "href": "Tools/Tools1.slides.html#section-11",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "We can even combine .filter() and .query(). For example, let’s select the columns species, body_mass_g and sex, then filter the data for the “Gentoo” species.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n  .query(\"species == 'Gentoo'\")\n).head(4)\n\n\n\n\n\n\n\n\nspecies\nbody_mass_g\nsex\n\n\n\n\n152\nGentoo\n4500.0\nfemale\n\n\n153\nGentoo\n5700.0\nmale\n\n\n154\nGentoo\n4450.0\nfemale\n\n\n155\nGentoo\n5700.0\nmale"
  },
  {
    "objectID": "Tools/Tools1.slides.html#create-new-columns-with-.assign",
    "href": "Tools/Tools1.slides.html#create-new-columns-with-.assign",
    "title": "Introduction to Python and Pandas",
    "section": "Create new columns with .assign()",
    "text": "Create new columns with .assign()\n\nWith .assign(), we can create new columns (variables) that are functions of existing ones. This function uses a special Python keyword called lambda. Technically, this keyword defines an anonymous function.\n\nFor example, we create a new variable LDRatio equaling the ratio of bill_length_mm and bill_depth_mm.\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n)"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-12",
    "href": "Tools/Tools1.slides.html#section-12",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "In this code, the df after lambda indicates that the dataframe (penguins_data) will be referred to as df inside the function. The colon : sets the start of the function.\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n)\n\nThe code appends the new variable to the end of the resulting dataframe."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-13",
    "href": "Tools/Tools1.slides.html#section-13",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "We can see the new variable using .filter().\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"LDRatio\"], axis = 1)\n).head()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nLDRatio\n\n\n\n\n0\n39.1\n18.7\n2.090909\n\n\n1\n39.5\n17.4\n2.270115\n\n\n2\n40.3\n18.0\n2.238889\n\n\n3\nNaN\nNaN\nNaN\n\n\n4\n36.7\n19.3\n1.901554"
  },
  {
    "objectID": "Tools/Tools1.slides.html#sorting-with-.sort_values",
    "href": "Tools/Tools1.slides.html#sorting-with-.sort_values",
    "title": "Introduction to Python and Pandas",
    "section": "Sorting with .sort_values()",
    "text": "Sorting with .sort_values()\nWe can sort the data based on a column like bill_length_mm.\n\n(penguins_data\n  .sort_values(\"bill_length_mm\")\n).head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n142\nAdelie\nDream\n32.1\n15.5\n188.0\n3050.0\nfemale\n2009\n\n\n98\nAdelie\nDream\n33.1\n16.1\n178.0\n2900.0\nfemale\n2008\n\n\n70\nAdelie\nTorgersen\n33.5\n19.0\n190.0\n3600.0\nfemale\n2008\n\n\n92\nAdelie\nDream\n34.0\n17.1\n185.0\n3400.0\nfemale\n2008"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-14",
    "href": "Tools/Tools1.slides.html#section-14",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "To sort in descending order, use ascending=False inside sort_values().\n\n(penguins_data\n  .sort_values(\"bill_length_mm\", ascending=False)\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n185\nGentoo\nBiscoe\n59.6\n17.0\n230.0\n6050.0\nmale\n2007\n\n\n293\nChinstrap\nDream\n58.0\n17.8\n181.0\n3700.0\nfemale\n2007\n\n\n253\nGentoo\nBiscoe\n55.9\n17.0\n228.0\n5600.0\nmale\n2009\n\n\n339\nChinstrap\nDream\n55.8\n19.8\n207.0\n4000.0\nmale\n2009\n\n\n267\nGentoo\nBiscoe\n55.1\n16.0\n230.0\n5850.0\nmale\n2009"
  },
  {
    "objectID": "Tools/Tools1.slides.html#summarizing-with-.agg",
    "href": "Tools/Tools1.slides.html#summarizing-with-.agg",
    "title": "Introduction to Python and Pandas",
    "section": "Summarizing with .agg()",
    "text": "Summarizing with .agg()\nWe can calculate summary statistics of the columns bill_length_mm, bill_depth_mm, and body_mass_g.\n\n(penguins_data\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"], axis = 1)\n  .agg([\"mean\"])\n)\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nbody_mass_g\n\n\n\n\nmean\n43.92193\n17.15117\n4201.754386\n\n\n\n\n\n\n\n\n\nBy default, agg() ignores missing values."
  },
  {
    "objectID": "Tools/Tools1.slides.html#saving-results-in-new-objects",
    "href": "Tools/Tools1.slides.html#saving-results-in-new-objects",
    "title": "Introduction to Python and Pandas",
    "section": "Saving results in new objects",
    "text": "Saving results in new objects\n\nAfter performing operations on our data, we can save the modified dataset as a new object.\n\nmean_penguins_data = (penguins_data\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"], axis = 1)\n  .agg([\"mean\"])\n)\n\nmean_penguins_data\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nbody_mass_g\n\n\n\n\nmean\n43.92193\n17.15117\n4201.754386"
  },
  {
    "objectID": "Tools/Tools1.slides.html#more-on-pandas",
    "href": "Tools/Tools1.slides.html#more-on-pandas",
    "title": "Introduction to Python and Pandas",
    "section": "More on pandas",
    "text": "More on pandas\n\n\nhttps://wesmckinney.com/book/"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#agenda",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#agenda",
    "title": "Supervised Learning and Linear Regression",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction to Supervised Learning\nLinear Regression Model\nK-fold Cross Validation"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#supervised-learning",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#supervised-learning",
    "title": "Supervised Learning and Linear Regression",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nIncludes algorithms that learn by example. The user provides the supervised algorithm with a known data set that includes the corresponding known inputs and outputs. The algorithm must find a method to determine how to reach those inputs and outputs.\nWhile the user knows the correct answers to the problem, the algorithm identifies patterns in the data, learns from observations, and makes predictions.\nThe algorithm makes predictions that can be corrected by the user, and this process continues until the algorithm reaches a high level of accuracy and performance."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#supervised-learning-problems",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#supervised-learning-problems",
    "title": "Supervised Learning and Linear Regression",
    "section": "Supervised learning problems",
    "text": "Supervised learning problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#supervised-learning-problems-1",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#supervised-learning-problems-1",
    "title": "Supervised Learning and Linear Regression",
    "section": "Supervised learning problems",
    "text": "Supervised learning problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#regression-problem",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#regression-problem",
    "title": "Supervised Learning and Linear Regression",
    "section": "Regression problem",
    "text": "Regression problem\n\nGoal: Find the best function \\(f(\\boldsymbol{X})\\) of the predictors \\(\\boldsymbol{X} = (X_1, \\ldots, X_p)\\) that describes the response \\(Y\\).\nIn mathematical terms, we want to establish the following relationship:\n\\[Y = f(\\boldsymbol{X}) + \\epsilon\\]\n\nWhere \\(\\epsilon\\) is a natural (random) error."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#how-to-find-the-shape-of-fx",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#how-to-find-the-shape-of-fx",
    "title": "Supervised Learning and Linear Regression",
    "section": "How to find the shape of \\(f(X)\\)?",
    "text": "How to find the shape of \\(f(X)\\)?\n\nUsing training data."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#how-to-find-the-shape-of-fx-1",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#how-to-find-the-shape-of-fx-1",
    "title": "Supervised Learning and Linear Regression",
    "section": "How to find the shape of \\(f(X)\\)?",
    "text": "How to find the shape of \\(f(X)\\)?\n\nUsing training data."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx",
    "title": "Supervised Learning and Linear Regression",
    "section": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?",
    "text": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?\n\n\n\nUsing validation data."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx-1",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx-1",
    "title": "Supervised Learning and Linear Regression",
    "section": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?",
    "text": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?\n\n\n\nUsing validation data."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#moreover",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#moreover",
    "title": "Supervised Learning and Linear Regression",
    "section": "Moreover…",
    "text": "Moreover…\n\n\n\n\nWe can use test data for a final evaluation of the model.\nTest data is data obtained from the process that generated the training data.\nTest data is independent of the training data."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#linear-regression-model-1",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#linear-regression-model-1",
    "title": "Supervised Learning and Linear Regression",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\nA common candidate function for predicting a response is the linear regression model. It has the mathematical form:\n\\[\\hat{Y}_i = \\hat{f}(\\boldsymbol{X}_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\cdots + \\hat{\\beta}_p X_{ip}.\\]\n\nWhere \\(i = 1, \\ldots, n_t\\) is the index of the \\(n_t\\) training data.\n\\(\\hat{Y}_i\\) is the prediction of the actual value of the response \\(Y_i\\) associated with values of \\(p\\) predictors denoted by \\(\\boldsymbol{X}_i = (X_{i1}, \\ldots, X_{ip})\\).\nThe values \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), …, \\(\\hat{\\beta}_p\\) are the coefficients of the model."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#interpretation-of-coefficients",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#interpretation-of-coefficients",
    "title": "Supervised Learning and Linear Regression",
    "section": "Interpretation of coefficients",
    "text": "Interpretation of coefficients\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\cdots + \\hat{\\beta}_p X_{ip},\\]\nwhere the unknown parameter \\(\\hat{\\beta}_0\\) is called the “intercept,” and \\(\\hat{\\beta}_j\\) is the “coefficient” of the j-th predictor.\n\nFor the j-th predictor, we have that:\n\n\\(\\hat{\\beta}_j = 0\\) implies no dependence.\n\\(\\hat{\\beta}_j &gt; 0\\) implies positive dependence.\n\\(\\hat{\\beta}_j &lt; 0\\) implies negative dependence."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#section-1",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#section-1",
    "title": "Supervised Learning and Linear Regression",
    "section": "",
    "text": "\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\cdots + \\hat{\\beta}_p X_{ip},\\]\nInterpretation:\n\n\\(\\hat{\\beta}_0\\) is the average response when all predictors \\(X_j\\) equal 0.\n\\(\\hat{\\beta}_j\\) is the amount of increase in the average response by a 1 unit increase in the predictor \\(X_j\\), when all other predictors are fixed to an arbitrary value."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#estimation-of-coefficients",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#estimation-of-coefficients",
    "title": "Supervised Learning and Linear Regression",
    "section": "Estimation of Coefficients",
    "text": "Estimation of Coefficients\nThe values of \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), …, \\(\\hat{\\beta}_p\\) are obtained from the training data using method of least squares.\nThis method finds the coefficient values that minimize the error made by the model \\(\\hat{f}(X_i)\\) when trying to predict the responses in the training set:\n\n\\[RSS = \\sum_{i=1}^{n_t} (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\cdots + \\hat{\\beta}_p X_{ip} ))^2  \\]\n\nwhere \\(RSS\\) means residual sum of squares."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#the-idea-in-two-dimensions",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#the-idea-in-two-dimensions",
    "title": "Supervised Learning and Linear Regression",
    "section": "The idea in two dimensions",
    "text": "The idea in two dimensions"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#load-the-libraries",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#load-the-libraries",
    "title": "Supervised Learning and Linear Regression",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nBefore we see an example, let’s import the data science libraries into Python.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, root_mean_squared_error\nfrom sklearn.metrics import r2_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#example-1",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#example-1",
    "title": "Supervised Learning and Linear Regression",
    "section": "Example 1",
    "text": "Example 1\nWe used the dataset called “Advertising.xlsx” in Canvas.\n\nTV: Money spent on TV ads for a product (in hundreds of $).\nRadio: Money spent on Radio ads for a product (in hundreds of $).\nNewspaper: Money spent on Newspaper ads for a product (in hundreds of $).\nSales: Sales generated from the product ( in thousands of $).\n200 markets\n\n\n# Load the data into Python\nAds_data = pd.read_excel('Advertising.xlsx')"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#notation",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#notation",
    "title": "Supervised Learning and Linear Regression",
    "section": "Notation",
    "text": "Notation\n\nFor our linear regression model:\n\n\\(Y_i\\) is the \\(i\\)-th observation for Sales.\n\\(X_{i1}\\) is the \\(i\\)-th observation for TV.\n\\(X_{i2}\\) is the \\(i\\)-th observation for Radio.\n\\(X_{i3}\\) is the \\(i\\)-th observation for Newspaper."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#section-2",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#section-2",
    "title": "Supervised Learning and Linear Regression",
    "section": "",
    "text": "Ads_data.head()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n9.3\n\n\n3\n151.5\n41.3\n58.5\n18.5\n\n\n4\n180.8\n10.8\n58.4\n12.9"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#section-3",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#section-3",
    "title": "Supervised Learning and Linear Regression",
    "section": "",
    "text": "Now, let’s set the predictors and response. In the definition of X_full, the double bracket in [] is important because it allows us to have a pandas DataFrame as output. This makes it easier to fit the linear regression model with scikit-learn.\n\n# Chose the predictor.\nX_full = Ads_data.filter(['TV', 'Radio', 'Newspaper'])\n\n# Set the response.\nY_full = Ads_data.filter(['Sales'])"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#create-training-and-validation-data",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#create-training-and-validation-data",
    "title": "Supervised Learning and Linear Regression",
    "section": "Create training and validation data",
    "text": "Create training and validation data\n\nTo evaluate a model’s performance on unobserved data, we split the current dataset into a training dataset and a validation dataset. To do this, we use the scikit-learn train_test_split() function.\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n                                                      test_size = 0.25,\n                                                      random_state = 301655)\n\nWe use 75% of the data for training and the rest for validation."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#fit-a-linear-regression-model-in-python",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#fit-a-linear-regression-model-in-python",
    "title": "Supervised Learning and Linear Regression",
    "section": "Fit a linear regression model in Python",
    "text": "Fit a linear regression model in Python\n\nIn Python, we use the LinearRegression() and fit() functions from the scikit-learn to fit a linear regression model.\n\n# 1. Create the linear regression model\nLRmodel = LinearRegression()\n\n# 2. Fit the model.\nLRmodel.fit(X_train, Y_train)"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#section-4",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#section-4",
    "title": "Supervised Learning and Linear Regression",
    "section": "",
    "text": "The following commands allow you to show the estimated coefficients of the model.\n\nprint(\"Coefficients:\", LRmodel.coef_)\n\nCoefficients: [[ 0.04620676  0.18682436 -0.0059342 ]]\n\n\nWe can also show the estimated intercept.\n\nprint(\"Intercept:\", LRmodel.intercept_)\n\nIntercept: [3.25814952]\n\n\n\nThe estimated model thus is\n\\[\\hat{Y}_i = 3.258 + 0.046 X_{i1} + 0.186 X_{i2} - 0.005X_{i3}.\\]"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#interpretation",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#interpretation",
    "title": "Supervised Learning and Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\\hat{Y}_i = 3.258 + 0.046 X_{i1} + 0.186 X_{i2} - 0.005X_{i3}.\\]\n\n\nThe average sales are 3.258 thousands of dollars when the advertising budgets for TV, Radio, and Newspaper are 0 dollars.\nIncreasing the advertising TV by 1 hundreds of dollars increases the average sales by 0.046 thousands of dollars.\nIncreasing the advertising Radio by 1 hundreds of dollars increases the average sales by 0.186 thousands of dollars.\nIncreasing the advertising Newspaper by 1 hundreds of dollars reduces the average sales by 0.005 thousands of dollars."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#prediction-error",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#prediction-error",
    "title": "Supervised Learning and Linear Regression",
    "section": "Prediction error",
    "text": "Prediction error\nAfter estimating and validating the linear regression model, we can check the quality of its predictions on unobserved data. That is, on the data in the validation set.\nOne metric for this is the mean prediction error (MSE\\(_v\\)):\n\n\\[\\text{MSE}_v = \\frac{\\sum_{i=1}^{n_v} (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\cdots + \\hat{\\beta}_p X_{ip}))^2}{n_v}\\]\n\n\nFor \\(n_v\\), the validation data!\n\nThe smaller \\(\\text{MSE}_v\\), the better the predictions."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#section-5",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#section-5",
    "title": "Supervised Learning and Linear Regression",
    "section": "",
    "text": "In practice, the square root of the mean prediction error is used:\n\\[\\text{RMSE}_v = \\sqrt{\\text{MSE}_v}.\\]\nThe advantage of \\(\\text{RMSE}_v\\) is that it can be interpreted as:\n\nThe average variability of a model prediction.\n\nFor example, if \\(\\text{RMSE}_v = 1\\), then a prediction of \\(\\hat{Y} = 5\\) will have an (average) error rate of \\(\\pm 1\\)."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#in-python",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#in-python",
    "title": "Supervised Learning and Linear Regression",
    "section": "In Python",
    "text": "In Python\n\nTo evaluate the model’s performance, we use the validation dataset. Specifically, we use the predictor matrix stored in X_valid.\n\nIn Python, we make the prediction using the pre-trained LRmodel.\n\nY_pred = LRmodel.predict(X_valid)"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#section-6",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#section-6",
    "title": "Supervised Learning and Linear Regression",
    "section": "",
    "text": "To evaluate the model, we use the function mean_squared_error() from scikit-learn. Recall that the responses from the validation dataset are in Y_valid, and the model predictions are in Y_pred.\n\nmse = mean_squared_error(Y_valid, Y_pred)  # Mean Squared Error (MSE)\nprint(round(mse, 2))\n\n5.09\n\n\n\nTo obtain the root mean squared error (RMSE), we use root_mean_squared_error() instead.\n\nrmse = root_mean_squared_error(Y_valid, Y_pred)\nprint(round(rmse, 2))\n\n2.26"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#another-metric-r2",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#another-metric-r2",
    "title": "Supervised Learning and Linear Regression",
    "section": "Another Metric: \\(R^2\\)",
    "text": "Another Metric: \\(R^2\\)\n\nIn the context of Data Science, \\(R^2\\) can be interpreted as the squared correlation between the actual responses and those predicted by the model.\nThe higher the correlation, the better the agreement between the predicted and actual responses.\n\n\nWe compute \\(R^2\\) in Python as follows:\n\nrtwo_sc = r2_score(Y_valid, Y_pred)  # Rsquared\nprint(round(rtwo_sc, 2))\n\n0.78"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#limitations-of-a-validation-set",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#limitations-of-a-validation-set",
    "title": "Supervised Learning and Linear Regression",
    "section": "Limitations of a validation set",
    "text": "Limitations of a validation set\n\n\nBy using a training data set that is much smaller than our actual data, the estimated model \\(\\hat{f}(\\boldsymbol{X})\\) will be less good than if we used the full training data. That is, more likely for predictions to be far from actual values.\nThus, the validation MSE is likely to be bigger than had we (a) used the full data set and (b) fit the correct model.\nIn other words, using less than all data results in a \\(\\text{MSE}_v\\) that is not a good representation of the predictive performance of \\(\\hat{f}(\\boldsymbol{X})\\)."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cross-validation-1",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cross-validation-1",
    "title": "Supervised Learning and Linear Regression",
    "section": "\\(K\\)-fold Cross-Validation",
    "text": "\\(K\\)-fold Cross-Validation\n\nBasic Idea: Divide the training data into \\(K\\) equally-sized divisions or folds."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cross-validation-2",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cross-validation-2",
    "title": "Supervised Learning and Linear Regression",
    "section": "\\(K\\)-fold Cross-Validation",
    "text": "\\(K\\)-fold Cross-Validation\n\nBasic Idea: Divide the training data into \\(K\\) equally-sized divisions or folds (\\(K = 5\\) here)."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cross-validation-3",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cross-validation-3",
    "title": "Supervised Learning and Linear Regression",
    "section": "\\(K\\)-fold Cross-Validation",
    "text": "\\(K\\)-fold Cross-Validation\n\nBasic Idea: Divide the training data into \\(K\\) equally-sized divisions or folds (\\(K = 5\\) here).\n\n\n\n\n\n\n\n\n\n\nUsing training, construct \\(\\hat{f}^{(-1)}\\).\nUsing validation, calculate \\(CV_1(\\hat{f}^{(-1)}) = \\frac{1}{n_1} \\sum_{i \\in F_1} (Y_i - \\hat{f}^{(-1)}(\\boldsymbol{X}_i))^2\\)\nCall it the Fold-Based Error Estimate"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cross-validation-4",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cross-validation-4",
    "title": "Supervised Learning and Linear Regression",
    "section": "\\(K\\)-fold Cross-Validation",
    "text": "\\(K\\)-fold Cross-Validation\n\nBasic Idea: Divide the training data into \\(K\\) equally-sized divisions or folds (\\(K = 5\\) here).\n\n\n\n\n\n\n\n\n\n\nUsing training, construct \\(\\hat{f}^{(-2)}\\).\nUsing validation, calculate \\(CV_1(\\hat{f}^{(-2)}) = \\frac{1}{n_2} \\sum_{i \\in F_2} (Y_i - \\hat{f}^{(-2)}(\\boldsymbol{X}_i))^2\\)"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cross-validation-5",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cross-validation-5",
    "title": "Supervised Learning and Linear Regression",
    "section": "\\(K\\)-fold Cross-Validation",
    "text": "\\(K\\)-fold Cross-Validation\n\nBasic Idea: Divide the training data into \\(K\\) equally-sized divisions or folds (\\(K = 5\\) here).\n\n\n\n\n\n\n\n\n\n\nUsing training, construct \\(\\hat{f}^{(-5)}\\).\nUsing validation, calculate \\(CV_1(\\hat{f}^{(-5)}) = \\frac{1}{n_5} \\sum_{i \\in F_5} (Y_i - \\hat{f}^{(-5)}(\\boldsymbol{X}_i))^2\\)"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#section-7",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#section-7",
    "title": "Supervised Learning and Linear Regression",
    "section": "",
    "text": "We average these fold-based error estimates to yield an evaluation metric:\n\\[CV(\\hat{f}) = \\frac{1}{K} \\sum^{K}_{k=1} CV_k (\\hat{f}^{(-k)}).\\]\n\nCalled the \\(K\\)-fold cross-validation estimate.\nHere, we used \\(K = 5\\) but another popular choice is \\(K = 10\\)."
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cv-in-python",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#k-fold-cv-in-python",
    "title": "Supervised Learning and Linear Regression",
    "section": "\\(K\\)-fold CV in Python",
    "text": "\\(K\\)-fold CV in Python\nIn Python, we apply \\(K\\)-fold cross validation (CV) using the function cross_val_score() from scikit-learn. The argument cv sets the number of folds to use, and scoring sets the evaluation metric to compute on the folds.\n\n# 1. Define a new linear regression model\nLRmodel_cv = LinearRegression()\n\n# 2. Apply 5-fold CV\nneg_cv_MSEs = cross_val_score(LRmodel_cv, X_full, Y_full, cv = 5, \n                        scoring = \"neg_mean_squared_error\")\n\n# 3. Show scores\nprint(neg_cv_MSEs)\n\n[-3.1365399  -2.42566776 -1.58522508 -5.42615506 -2.79114519]"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#section-8",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#section-8",
    "title": "Supervised Learning and Linear Regression",
    "section": "",
    "text": "Unfortunately, cross_val_score() outputs negative scores. We simply turn them into positive by multiplying them by -1 or adding a - symbol.\n\ncv_MSEs = -neg_cv_MSEs\n\n\nAfter that, we average the values using .mean() to obtain a the \\(5\\)-fold CV estimate.\n\nMSE_cv = cv_MSEs.mean()\nprint(round(MSE_cv, 3))\n\n3.073"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#section-9",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#section-9",
    "title": "Supervised Learning and Linear Regression",
    "section": "",
    "text": "Note that we can compute a \\(K\\)-fold CV estimate for any evaluation metric including the \\(R^2\\). To this end, we set scoring = \"r2\".\n\n# 1. Define a new linear regression model\nLRmodel_cv = LinearRegression()\n\n# 2. Apply 5-fold CV\ncv_Rsq = cross_val_score(LRmodel_cv, X_full, Y_full, cv = 5, \n                        scoring = \"r2\")\n\n# 3. Compute CV estimate\nRsq_cv = cv_Rsq.mean()\nprint(round(Rsq_cv, 3))\n\n0.887"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#section-10",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#section-10",
    "title": "Supervised Learning and Linear Regression",
    "section": "",
    "text": "We can also compute a \\(K\\)-fold CV estimate for the root mean squared error (RMSE).\n\nLRmodel_cv = LinearRegression()\nneg_cv_RMSEs = cross_val_score(LRmodel_cv, X_full, Y_full, cv = 5, \n                        scoring = \"neg_root_mean_squared_error\")\ncv_RMSEs = -neg_cv_RMSEs\nRMSE_cv = cv_RMSEs.mean()\nprint(round(RMSE_cv, 3))\n\n1.718"
  },
  {
    "objectID": "LinearRegression/Intro_and_LinearRegression.slides.html#final-remarks",
    "href": "LinearRegression/Intro_and_LinearRegression.slides.html#final-remarks",
    "title": "Supervised Learning and Linear Regression",
    "section": "Final remarks",
    "text": "Final remarks\n\n\\(K\\)-fold CV provides a robust estimate of prediction error by averaging performance across multiple data splits.\n\nIt improves data efficiency by allowing all observations to be used for both training and validation.\n\nIt is widely used for model comparison and hyperparameter tuning.\nIt can be extended to generalized \\(K\\)-fold cross-validation (e.g., stratified, grouped, or time-series folds) to respect data structure."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#agenda",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#agenda",
    "title": "Model Evaluation and Inference",
    "section": "Agenda",
    "text": "Agenda\n\n\nResidual analysis\nInference about individual \\(\\beta\\)’s using t-tests"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#load-the-libraries",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#load-the-libraries",
    "title": "Model Evaluation and Inference",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nLet’s import the required libraries into Python together with other relevant libraries.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\n\nWe will introduce the new library statsmodels.api later."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#linear-regression-model",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#linear-regression-model",
    "title": "Model Evaluation and Inference",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\nRecall that the linear regression model is a common candidate function for predicting a response:\n\\[\\hat{Y}_i = \\hat{f}(\\boldsymbol{X}_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\cdots + \\hat{\\beta}_p X_{ip}.\\]\n\nWhere \\(i = 1, \\ldots, n_t\\) is the index of the \\(n_t\\) training data.\n\\(\\hat{Y}_i\\) is the prediction of the actual value of the response \\(Y_i\\) associated with values of \\(p\\) predictors denoted by \\(\\boldsymbol{X}_i = (X_{i1}, \\ldots, X_{ip})\\).\nThe values \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), …, \\(\\hat{\\beta}_p\\) are the coefficients of the model."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#example-1",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#example-1",
    "title": "Model Evaluation and Inference",
    "section": "Example 1",
    "text": "Example 1\nLet’s consider the “auto.xlsx” dataset.\n\nauto_data = pd.read_excel('auto.xlsx')\n\n\nLet’s assume that we want to train the following model.\n\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1}+ \\hat{\\beta}_2 X_{i2},\n\\]\nwhere \\(\\hat{Y}_i\\) is the predicted mpg, \\(X_{i1}\\) is the weight, and \\(X_{i2}\\) is the acceleration of the \\(i\\)-th car, \\(i = 1, \\ldots, 392\\).\nHere, we will use the entire dataset for training and ignore \\(K\\)-fold cross-validation to illustrate the concepts."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#research-question",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#research-question",
    "title": "Model Evaluation and Inference",
    "section": "Research question",
    "text": "Research question\n\n\nDo the weight and acceleration have a significant association with the mpg of a car?"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#the-two-cultures-of-statistical-models",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#the-two-cultures-of-statistical-models",
    "title": "Model Evaluation and Inference",
    "section": "The two cultures of statistical models",
    "text": "The two cultures of statistical models\n\n\nInference: develop a model that fits the data well. Then make inferences about the data-generating process based on the structure of such model.\nPrediction: Silent about the underlying mechanism generating the data and allow for many predictive algorithms, which only care about accuracy of predictions.\n\n\nThey overlap very often."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "The least squares estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) are subject to uncertainty, since they are calculated based on a random sample of data. That is, an instance of training data.\nTherefore, assessing the amount of the uncertainty in these estimates is important. To this end, we use hypothesis tests on individual coefficients."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#hypothesis-test",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#hypothesis-test",
    "title": "Model Evaluation and Inference",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\nA statistical hypothesis is a statement about the coefficients of a model.\n\n\nIn our case, we are interested in testing:\n\n\\(H_0: \\beta_j = 0\\) v.s. \\(H_1: \\beta_j \\neq 0\\) (Two-tailed Test)\n\n\n\\(\\beta_j\\) is the true (unknown) coefficient of the predictor.\nRejecting \\(H_0\\) (in favor of \\(H_1\\)) implies that the predictor has a significant association with the response.\nNot rejecting \\(H_0\\) implies that the predictor does not have a significant association with the response."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-1",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-1",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "\\(H_0: \\beta_j = 0\\) v.s. \\(H_1: \\beta_j \\neq 0\\) (Two-tailed Test)\n\n\nTesting this hypothesis consists of the following steps:\n\nTake a random sample (your training data).\nCompute the appropriate test statistic.\nReject or fail to reject the null hypothesis based on a computed p-value."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#step-1.-random-sample",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#step-1.-random-sample",
    "title": "Model Evaluation and Inference",
    "section": "Step 1. Random sample",
    "text": "Step 1. Random sample\n\nThe random sample is the training data we use to train or fit the model.\n\n# Dataset with predictors.\nauto_X_train = auto_data[['weight', 'acceleration']]\n\n# Dataset with the response only.\nauto_Y_train = auto_data['mpg']\n\n\nAs mentioned before, we will use the entire dataset for training."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#step-2.-test-statistic",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#step-2.-test-statistic",
    "title": "Model Evaluation and Inference",
    "section": "Step 2. Test statistic",
    "text": "Step 2. Test statistic\n\nThe test statistic is\n\\[t_0 = \\frac{\\hat{\\beta}_{j}}{\\sqrt{ \\hat{v}_{jj}} }\\]\n\n\\(\\hat{\\beta}_j\\) is the least squares estimate of the true coefficient \\(\\beta_j\\).\n\\(\\sqrt{\\hat{v}_{jj}}\\) is the standard error of the estimate \\(\\hat{\\beta}_j\\) calculated using Python."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-2",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-2",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Recall that we obtain the least squares estimates (\\(\\hat{\\beta}_{j}\\)) in Python using:\n\n# 1. Create the linear regression model\nLRmodel = LinearRegression()\n\n# 2. Fit the model.\nLRmodel.fit(auto_X_train, auto_Y_train)\n\n# 3. Show estimated coefficients.\nprint(\"Intercept:\", LRmodel.intercept_)\nprint(\"Coefficients:\", LRmodel.coef_)\n\nIntercept: 41.095328779604216\nCoefficients: [-0.0072931  0.2616504]\n\n\n\nLater, we will see how to obtain the standard error of the estimate \\(\\hat{\\beta}_j\\)."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-3",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-3",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "\\[t_0 = \\frac{\\hat{\\beta}_{j}}{\\sqrt{ \\hat{v}_{jj}} }\\]\n\n\nWe like this statistic because it follows a well-known distribution.\nIf the null hypothesis (\\(H_0: \\beta_j = 0\\)) is true, the statistic \\(t_0\\) follows a \\(t\\) distribution with \\(n-p-1\\) degrees of freedom\nRemember that \\(n\\) is the number of observations and \\(p\\) the number of predictors."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#t-distribution",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#t-distribution",
    "title": "Model Evaluation and Inference",
    "section": "t distribution",
    "text": "t distribution\n\n\n\nThis distribution is also known as the student’s t-distribution.\nIt was invented by William Gosset when he worked at the Guinness Brewery in Ireland.\nIt has one parameter \\(\\nu\\) which generally equals a number of degrees of freedom.\nThe parameter \\(\\nu\\) controls the shape of the distribution.\n\n William Gosset"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-4",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-4",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "The t-distribution resembles a standard normal distribution when \\(\\nu\\) goes to infinity."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#step-3.-calculate-the-p-value",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#step-3.-calculate-the-p-value",
    "title": "Model Evaluation and Inference",
    "section": "Step 3. Calculate the p-value",
    "text": "Step 3. Calculate the p-value\n\nThe p-value is the probability that the \\(t\\) test statistic will get a value that is at least as extreme as the observed \\(t_0\\) value when the null hypothesis (\\(H_0\\)) is true."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-5",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-5",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "For \\(H_0: \\beta_j = 0\\) v.s. \\(H_1: \\beta_j \\neq 0\\), the p-value is calculated using both tails of the \\(t\\) distribution. It is the blue area under the curve below.\n\nWe use the two tails because \\(H_1\\) includes the possibility that the value of \\(\\beta_j\\) is positive or negative."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#the-p-value-is-not-the-probability-that-h_0-is-true",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#the-p-value-is-not-the-probability-that-h_0-is-true",
    "title": "Model Evaluation and Inference",
    "section": "The p-value is not the probability that \\(H_0\\) is true",
    "text": "The p-value is not the probability that \\(H_0\\) is true\nSince the p-value is a probability, and since small p-values indicate that \\(H_0\\) is unlikely to be true, it is tempting to think that the p-value represents the probability that \\(H_0\\) is true.\n\n\n\nThis is not the case!\n\n\n\nRemember that the p-value is the probability that the \\(t\\) test statistic will get a value that is at least as extreme as the observed \\(t_0\\) value when \\(H_0\\) is true."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#how-small-must-the-p-value-be-to-reject-h_0",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#how-small-must-the-p-value-be-to-reject-h_0",
    "title": "Model Evaluation and Inference",
    "section": "How small must the p-value be to reject \\(H_0\\)?",
    "text": "How small must the p-value be to reject \\(H_0\\)?\n\n\n\nAnswer: Very small!\nBut, how small?\nAnswer: Very small!\nBut, tell me, how small?\nAnswer: Okay, it must be less than a value called \\(\\alpha\\) which is usually 0.1, 0.05, or 0.01.\nThank you!\n\n\n Sir Ronald Fisher"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#decision",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#decision",
    "title": "Model Evaluation and Inference",
    "section": "Decision",
    "text": "Decision\n\nFor a significance level of \\(\\alpha = 0.05\\):\n\nIf the p-value is smaller than \\(\\alpha\\), we reject \\(H_0: \\beta_j = 0\\) in favor of \\(H_1: \\beta_j \\neq 0\\).\nIf the p-value is larger than \\(\\alpha\\), we do not reject \\(H_0: \\beta_j = 0\\).\n\nNo scientific basis for this advice. In practice, report the p-value and explore the data using descriptive statistics."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#t-tests-in-python",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#t-tests-in-python",
    "title": "Model Evaluation and Inference",
    "section": "t-tests in Python",
    "text": "t-tests in Python\n\nUnfortunately, there is no native function in scikit-learn to compute all elements of t-tests for a linear regression model.\n\nThe easiest and safest way to do this is to train a model from scratch using the statsmodels library in Python.\n🙁"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#statsmodels-library",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#statsmodels-library",
    "title": "Model Evaluation and Inference",
    "section": "statsmodels library",
    "text": "statsmodels library\n\nstatsmodels is a powerful Python library for statistical modeling, data analysis, and hypothesis testing.\nIt provides classes and functions for estimating statistical models.\nIt is built on top of libraries such as NumPy, SciPy, and pandas\nhttps://www.statsmodels.org/stable/index.html"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-7",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-7",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "An issue with statsmodels is that we must add the intercept to our predictor matrix using the add_constant() function.\n\nauto_X_train_with_intercept = sm.add_constant(auto_X_train)\n\n\nWe fit or train a linear regression model using the .OLS() and .fit() functions.\n\n# Create linear regression object in statsmodel\nregr = sm.OLS(auto_Y_train, auto_X_train_with_intercept)\n\n# Train the model using the training sets\nlinear_model = regr.fit()"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-8",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-8",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "We access the information on the t-tests using the .summary() function.\n\nmodel_summary = linear_model.summary()\nprint(model_summary)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.700\nModel:                            OLS   Adj. R-squared:                  0.698\nMethod:                 Least Squares   F-statistic:                     453.2\nDate:                Thu, 19 Feb 2026   Prob (F-statistic):          2.43e-102\nTime:                        16:18:46   Log-Likelihood:                -1125.4\nNo. Observations:                 392   AIC:                             2257.\nDf Residuals:                     389   BIC:                             2269.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst           41.0953      1.868     21.999      0.000      37.423      44.768\nweight          -0.0073      0.000    -25.966      0.000      -0.008      -0.007\nacceleration     0.2617      0.086      3.026      0.003       0.092       0.432\n==============================================================================\nOmnibus:                       31.397   Durbin-Watson:                   0.818\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               39.962\nSkew:                           0.632   Prob(JB):                     2.10e-09\nKurtosis:                       3.922   Cond. No.                     2.67e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.67e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#table-of-t-tests",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#table-of-t-tests",
    "title": "Model Evaluation and Inference",
    "section": "Table of t-tests",
    "text": "Table of t-tests\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;\\(|t|\\)\n\\([0.025\\)\n\\(0.975 ]\\)\n\n\n\n\nconst\n41.0953\n1.868\n21.999\n0.000\n37.423\n44.768\n\n\nweight\n-0.0073\n0.000\n-25.966\n0.000\n-0.008\n-0.007\n\n\nacceleration\n0.2617\n0.086\n3.026\n0.003\n0.092\n0.432\n\n\n\nThe column std err contains the values of the estimated standard error (\\(\\sqrt{ \\hat{v}_{jj}}\\)) of the estimates \\(\\hat{\\beta}_j\\). They represent the variability in the estimates."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-9",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-9",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "coef\nstd err\nt\nP&gt;\\(|t|\\)\n\\([0.025\\)\n\\(0.975 ]\\)\n\n\n\n\nconst\n41.0953\n1.868\n21.999\n0.000\n37.423\n44.768\n\n\nweight\n-0.0073\n0.000\n-25.966\n0.000\n-0.008\n-0.007\n\n\nacceleration\n0.2617\n0.086\n3.026\n0.003\n0.092\n0.432\n\n\n\nThe column t contains the values of the observed statistic \\(t_0\\) for the hypothesis tests."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-10",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-10",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "coef\nstd err\nt\nP&gt;\\(|t|\\)\n\\([0.025\\)\n\\(0.975 ]\\)\n\n\n\n\nconst\n41.0953\n1.868\n21.999\n0.000\n37.423\n44.768\n\n\nweight\n-0.0073\n0.000\n-25.966\n0.000\n-0.008\n-0.007\n\n\nacceleration\n0.2617\n0.086\n3.026\n0.003\n0.092\n0.432\n\n\n\nThe column P&gt;|t| contains the p-values for the hypothesis tests.\nSince the p-value is smaller than \\(\\alpha = 0.05\\), we reject \\(H_0\\) for acceleration and weight. Therefore, weight and acceleration have a significant association with the mpg of a car."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-11",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-11",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "coef\nstd err\nt\nP&gt;\\(|t|\\)\n\\([0.025\\)\n\\(0.975 ]\\)\n\n\n\n\nconst\n41.0953\n1.868\n21.999\n0.000\n37.423\n44.768\n\n\nweight\n-0.0073\n0.000\n-25.966\n0.000\n-0.008\n-0.007\n\n\nacceleration\n0.2617\n0.086\n3.026\n0.003\n0.092\n0.432\n\n\n\nThe columns \\([0.025\\) and \\(0.975]\\) show the limits of a 95% confidence interval on each true coefficient \\(\\beta_j\\). This interval contains the true parameter value with a confidence of 95%."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#residuals",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#residuals",
    "title": "Model Evaluation and Inference",
    "section": "Residuals",
    "text": "Residuals\n\nThe errors of the estimated model are called residuals \\(\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i\\), \\(i = 1, \\ldots, n.\\)\n\nIn linear regression, we can validate the adequacy of the model by checking whether the residuals \\(\\hat{\\epsilon}_1, \\hat{\\epsilon}_2, \\ldots, \\hat{\\epsilon}_n\\) satisfy three (or four) assumptions."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#assumptions",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#assumptions",
    "title": "Model Evaluation and Inference",
    "section": "Assumptions",
    "text": "Assumptions\nThe residuals \\(\\hat{\\epsilon}_i\\)’s must then satisfy the following assumptions:\n\nOn average, they are close to zero for any value of the predictors \\(X_j\\).\nFor any value of the predictor \\(X_j\\), the dispersion or variance is roughly the same.\nThe \\(\\epsilon_i\\)’s are independent from each other.\nThe \\(\\hat{\\epsilon}_i\\)’s follow normal distribution.\n\n\nTo evaluate the assumption of a linear regression model, we use a Residual analysis."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#residual-analysis-1",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#residual-analysis-1",
    "title": "Model Evaluation and Inference",
    "section": "Residual Analysis",
    "text": "Residual Analysis\n\nTo check the validity of these assumptions, we will follow a graphical approach. Specifically, we will construct three informative plots of the residuals.\n\nResiduals vs Fitted Values Plot. To assess the structure of the model and check for constant variance\nResiduals Vs Time Plot. To check independence.\nNormal Quantile-Quantile Plot. To assess if the residuals follow a normal distribution"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#example-2",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#example-2",
    "title": "Model Evaluation and Inference",
    "section": "Example 2",
    "text": "Example 2\n\n\nThis example is inspired by Foster, Stine and Waterman (1997, pages 191–199).\nThe data are in the form of the time taken (in minutes) for a production run, \\(Y\\), and the number of items produced, \\(X\\), for 20 randomly selected orders as supervised by a manager.\nWe wish to develop an equation to model the relationship between the run time (\\(Y\\)) and the run size (\\(X\\))."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#dataset",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#dataset",
    "title": "Model Evaluation and Inference",
    "section": "Dataset",
    "text": "Dataset\nThe dataset is in the file “production.xlsx”.\n\nproduction_data = pd.read_excel('production.xlsx')\nproduction_data.head()\n\n\n\n\n\n\n\n\nCase\nRunTime\nRunSize\n\n\n\n\n0\n1\n195\n175\n\n\n1\n2\n215\n189\n\n\n2\n3\n243\n344\n\n\n3\n4\n162\n88\n\n\n4\n5\n185\n114"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#linear-regression-model-1",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#linear-regression-model-1",
    "title": "Model Evaluation and Inference",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\n\\]\nwhere \\(X_i\\) is the run size and \\(\\hat{Y}_i\\) is the predicted run time for the \\(i\\)-th observation, \\(i = 1, \\ldots, 20\\).\n\nWe fit the model in scikit-learn using the entire data for training.\n\n# Defining the predictor (X) and the response variable (Y).\nprod_Y_train = production_data['RunTime']\nprod_X_train = production_data[['RunSize']]\n\n# Fitting the simple linear regression model.\nLRmodelProd = LinearRegression()\nLRmodelProd.fit(prod_X_train, prod_Y_train)"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#estimated-model",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#estimated-model",
    "title": "Model Evaluation and Inference",
    "section": "Estimated model",
    "text": "Estimated model\nThe estimated coefficients are\n\nprint(\"Intercept:\", LRmodel.intercept_)\nprint(\"Coefficients:\", LRmodel.coef_)\n\nSo, the fitted model is\n\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i = 149.75 + 0.26 X_i,\n\\]\nwhere \\(\\hat{Y}_i\\) is the predicted or fitted value of run time for the \\(i\\)-th observation, \\(i = 1, \\ldots, 20\\).\nThe residuals of this estimated model are \\(\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i\\)."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#calculation-of-fitted-values-and-residuals",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#calculation-of-fitted-values-and-residuals",
    "title": "Model Evaluation and Inference",
    "section": "Calculation of fitted values and residuals",
    "text": "Calculation of fitted values and residuals\n\nRecall that we can calculate the predicted values and residuals as follows.\n\n# Make predictions using the the model\nprod_Y_pred = LRmodelProd.predict(prod_X_train) + 0*prod_Y_train\n\n# Calculate residuals.\nresiduals = prod_Y_train - prod_Y_pred\n\nThe extra 0*prod_Y_train above allows us to have the correct Python for the predictions from scikit-learn functions."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#intuition-behind",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#intuition-behind",
    "title": "Model Evaluation and Inference",
    "section": "Intuition behind …",
    "text": "Intuition behind …\nthe Residuals versus Fitted Values plot."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#residuals-vs-fitted-values",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#residuals-vs-fitted-values",
    "title": "Model Evaluation and Inference",
    "section": "Residuals vs Fitted Values",
    "text": "Residuals vs Fitted Values\n\n\n\n\nCode\nplt.figure(figsize=(5, 5))\nsns.scatterplot(x='RunSize', y='RunTime', data=production_data, color='blue')\nplt.plot(production_data['RunSize'], prod_Y_pred, color='red', linestyle='--', linewidth=2)\nplt.title(\"Fitted Regression Line\")\nplt.xlabel(\"Run size\", fontsize = 14)\nplt.ylabel(\"Run time\", fontsize = 14)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Residual vs Fitted Values Plot\nplt.figure(figsize=(5, 5))\nsns.scatterplot(x = prod_Y_pred, y = residuals)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title('Residuals vs Fitted Values')\nplt.xlabel('Fitted (predicted) Values', fontsize = 14)\nplt.ylabel('Residuals', fontsize = 14)\nplt.show()"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#residuals-vs-fitted-values-1",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#residuals-vs-fitted-values-1",
    "title": "Model Evaluation and Inference",
    "section": "Residuals vs Fitted Values",
    "text": "Residuals vs Fitted Values\n\n\n\n\nIf there is a trend, the model is misspecified.\nA “funnel” shape indicates that the assumption of constant variance is not met."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-12",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-12",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Examples of plots that do not support the conclusion of constant variance."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-13",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-13",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Another example.\n\nThe phenomenon of non-constant variance is called heteroscedasticity."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#residuals-vs-time-plot-1",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#residuals-vs-time-plot-1",
    "title": "Model Evaluation and Inference",
    "section": "Residuals vs Time Plot",
    "text": "Residuals vs Time Plot\n\nBy “time,” we mean that time the observation was taken or the order in which it was taken. The plot should not show any structure or pattern in the residuals.\nDependence on time is a common source of lack of independence, but other plots might also detect lack of independence.\nIdeally, we plot the residuals versus each variable of interest we could think of, either included or excluded in the model.\nAssessing the assumption of independence is hard in practice."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-14",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-14",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Code\n# Residuals vs Time (Case) Plot\nplt.figure(figsize=(7, 5))\nsns.scatterplot(x = production_data['Case'], y = residuals)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title('Residuals vs Time (Case)')\nplt.xlabel('Case')\nplt.xticks(production_data['Case'])\nplt.ylabel('Residuals')\nplt.show()"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-15",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-15",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Examples of plots that do and do not support the independence assumption."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#checking-for-normality",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#checking-for-normality",
    "title": "Model Evaluation and Inference",
    "section": "Checking for normality",
    "text": "Checking for normality\n\nThis assumption is generally checked by looking at the distribution of the residuals.\nTwo plots:\n\nHistogram.\nNormal Quantile-Quantile Plot (also called normal probability plot)."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#histogram",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#histogram",
    "title": "Model Evaluation and Inference",
    "section": "Histogram",
    "text": "Histogram\nIdeally, the histogram resembles a normal distribution around 0. If the number of observations is small, the histogram may not be an effective visualization.\n\n\nCode\n# Histogram of residuals\nplt.figure(figsize=(5, 3))\nsns.histplot(residuals)\nplt.title('Histogram of Residuals')\nplt.xlabel('Residuals')\nplt.show()"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#normal-quantile-quantile-qq-plot",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#normal-quantile-quantile-qq-plot",
    "title": "Model Evaluation and Inference",
    "section": "Normal Quantile-Quantile (QQ) Plot",
    "text": "Normal Quantile-Quantile (QQ) Plot\n\nA normal QQ plot is helpful for deciding whether a sample was drawn from a distribution that is approximately normal.\n\n\nFirst, let \\(\\hat{\\epsilon}_{[1]}, \\hat{\\epsilon}_{[2]}, \\ldots, \\hat{\\epsilon}_{[n]}\\) be the residuals ranked in an increasing order, where \\(\\hat{\\epsilon}_{[1]}\\) is the minimum and \\(\\hat{\\epsilon}_{[n]}\\) is the maximum. These points define the sample percentiles (or quantiles) of the distribution of the residuals.\n\n\n\n\nNext, calculate the theoretical percentiles of a (standard) Normal distribution calculated using Python."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-16",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-16",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "The normal QQ plot displays the (sample) percentiles of the residuals versus the percentiles of a normal distribution.\nIf these percentiles agree with each other, then they would approximate a straight line.\nThe straight line is usually determined visually, with emphasis on the central values rather than the extremes.\nFor a nice explanation, see this YouTube video."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#qq-plot-in-python",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#qq-plot-in-python",
    "title": "Model Evaluation and Inference",
    "section": "QQ plot in Python",
    "text": "QQ plot in Python\n\nTo construct a QQ plot, we use the function qqplot() from the statsmodels library.\n\n# QQ plot to assess normality of residuals\nplt.figure(figsize=(5, 3))\nsm.qqplot(residuals, fit = True, line = '45')\nplt.title('QQ Plot of Residuals')\nplt.show()"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-17",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-17",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Substantial departures from a straight line indicate that the distribution is not normal.\n\n\n&lt;Figure size 672x384 with 0 Axes&gt;"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-18",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-18",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "This plot suggests that the residuals are consistent with a Normal curve.\n\n\n&lt;Figure size 672x384 with 0 Axes&gt;"
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#comments",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#comments",
    "title": "Model Evaluation and Inference",
    "section": "Comments",
    "text": "Comments\nThese data are truly Normally distributed. But note that we still see deviations. These are entirely due to chance.\n\nWhen \\(n\\) is relatively small, you tend to see deviations, particularly in the tails."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-19",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-19",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Normal probability plots for data sets following various distributions. 100 observations in each data set."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#consequences-of-faulty-assumptions",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#consequences-of-faulty-assumptions",
    "title": "Model Evaluation and Inference",
    "section": "Consequences of faulty assumptions",
    "text": "Consequences of faulty assumptions\n\n\nIf the model structure is incorrect, the estimated coefficients \\(\\hat{\\beta}_j\\) will be incorrect and the predictions \\(\\hat{Y}_i\\) will be inaccurate.\n\n\n\nIf the residuals do not follow a normal distribution, we have two cases:\n\n\nIf sample size is large, we still get accurate p-values for the t-tests for the coefficients thanks to the Central Limit Theorem.\nOtherwiese, the t-tests and p-values are unreliable."
  },
  {
    "objectID": "LinearRegression/ModelInferenceEvaluation.slides.html#section-20",
    "href": "LinearRegression/ModelInferenceEvaluation.slides.html#section-20",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "If the residuals do not have constant variance, then the linear model is incorrect and everything falls apart!\nIf the residuals are dependent, then the linear model is incorrect and everything falls apart!\n\nThere are methods to correct the faulty assumptions, but we will not discus them here."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#load-the-libraries",
    "href": "Classification/KNearestNeighbours.slides.html#load-the-libraries",
    "title": "K-nearest neighbors",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#k-nearest-neighbors-knn",
    "href": "Classification/KNearestNeighbours.slides.html#k-nearest-neighbors-knn",
    "title": "K-nearest neighbors",
    "section": "K-nearest neighbors (KNN)",
    "text": "K-nearest neighbors (KNN)\nKNN a supervised learning algorithm that uses proximity to make classifications or predictions about the clustering of a single data point.\n\nBasic idea: Predict a new observation using the K closest observations in the training dataset.\n\nTo predict the response for a new observation, KNN uses the K nearest neighbors (observations) in terms of the predictors!\nThe predicted response for the new observation is the most common response among the K nearest neighbors."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#the-algorithm-has-3-steps",
    "href": "Classification/KNearestNeighbours.slides.html#the-algorithm-has-3-steps",
    "title": "K-nearest neighbors",
    "section": "The algorithm has 3 steps:",
    "text": "The algorithm has 3 steps:\n\n\nChoose the number of nearest neighbors (K).\nFor a new observation, find the K closest observations in the training data (ignoring the response).\nFor the new observation, the algorithm predicts the value of the most common response among the K nearest observations."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#nearest-neighbour",
    "href": "Classification/KNearestNeighbours.slides.html#nearest-neighbour",
    "title": "K-nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#nearest-neighbour-1",
    "href": "Classification/KNearestNeighbours.slides.html#nearest-neighbour-1",
    "title": "K-nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#nearest-neighbour-2",
    "href": "Classification/KNearestNeighbours.slides.html#nearest-neighbour-2",
    "title": "K-nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#nearest-neighbour-3",
    "href": "Classification/KNearestNeighbours.slides.html#nearest-neighbour-3",
    "title": "K-nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#banknote-data",
    "href": "Classification/KNearestNeighbours.slides.html#banknote-data",
    "title": "K-nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 2 votes for “genuine” and 2 for “fake.” So we classify it as “genius.”"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#banknote-data-1",
    "href": "Classification/KNearestNeighbours.slides.html#banknote-data-1",
    "title": "K-nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 2 votes for “genuine” and 2 for “fake.” So we classify it as “genius.”"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#banknote-data-2",
    "href": "Classification/KNearestNeighbours.slides.html#banknote-data-2",
    "title": "K-nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 3 votes for “counterfeit” and 0 for “genuine.” So we classify it as “counterfeit.”"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#banknote-data-3",
    "href": "Classification/KNearestNeighbours.slides.html#banknote-data-3",
    "title": "K-nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 3 votes for “counterfeit” and 0 for “genuine.” So we classify it as “counterfeit.”\nCloseness is based on Euclidean distance."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#implementation-details",
    "href": "Classification/KNearestNeighbours.slides.html#implementation-details",
    "title": "K-nearest neighbors",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nTies\n\nIf there are more than K nearest neighbors, include them all.\nIf there is a tie in the vote, set a rule to break the tie. For example, randomly select the class."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#section",
    "href": "Classification/KNearestNeighbours.slides.html#section",
    "title": "K-nearest neighbors",
    "section": "",
    "text": "KNN uses the Euclidean distance between points. So it ignores units.\n\nExample: two predictors: height in cm and arm span in feet. Compare two people: (152.4, 1.52) and (182.88, 1.85).\nThese people are separated by 30.48 units of distance in the first variable, but only by 0.33 units in the second.\nTherefore, the first predictor plays a much more important role in classification and can bias the results to the point where the second variable becomes useless.\n\n\nTherefore, as a first step, we must transform the predictors so that they have the same units!"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#example",
    "href": "Classification/KNearestNeighbours.slides.html#example",
    "title": "K-nearest neighbors",
    "section": "Example",
    "text": "Example\nThe data is located in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#create-the-predictor-matrix-and-response-column",
    "href": "Classification/KNearestNeighbours.slides.html#create-the-predictor-matrix-and-response-column",
    "title": "K-nearest neighbors",
    "section": "Create the predictor matrix and response column",
    "text": "Create the predictor matrix and response column\nLet’s create the predictor matrix or response column\n\n# Set full matrix of predictors.\nX_full = bank_data.drop(columns = ['Status']) \n\n# Vector with responses\nY_full = bank_data.filter(['Status'])\n\nTo set the target category in the response we use the get_dummies() function.\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_full, dtype = 'int')\n\n# Select target variable.\nY_target_full = Y_dummies['Status_counterfeit']"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#lets-partition-the-dataset",
    "href": "Classification/KNearestNeighbours.slides.html#lets-partition-the-dataset",
    "title": "K-nearest neighbors",
    "section": "Let’s partition the dataset",
    "text": "Let’s partition the dataset\n\nWe use 70% for training and the rest for validation.\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n                                                      test_size = 0.3)"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#standardization-in-python",
    "href": "Classification/KNearestNeighbours.slides.html#standardization-in-python",
    "title": "K-nearest neighbors",
    "section": "Standardization in Python",
    "text": "Standardization in Python\n\nTo standardize numeric predictors, we use the StandardScaler() function. We also apply the function to variables using the fit_transform() function.\n\n\nscaler = StandardScaler()\nXs_train = scaler.fit_transform(X_train)"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#knn-in-python",
    "href": "Classification/KNearestNeighbours.slides.html#knn-in-python",
    "title": "K-nearest neighbors",
    "section": "KNN in Python",
    "text": "KNN in Python\n\nIn Python, we can use the KNeighborsClassifier() and fit() from scikit-learn to train a KNN.\nIn the KNeighborsClassifier function, we can define the number of nearest neighbors using the n_neighbors parameter.\n\n# For example, let's use KNN with three neighbours\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Now, we train the algorithm.\nknn.fit(Xs_train, Y_train)"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#evaluation",
    "href": "Classification/KNearestNeighbours.slides.html#evaluation",
    "title": "K-nearest neighbors",
    "section": "Evaluation",
    "text": "Evaluation\n\nTo evaluate KNN, we make predictions on the validation data (not used to train the KNN). To do this, we must first perform standardization operations on the predictors in the validation dataset.\n\nXs_valid = scaler.fit_transform(X_valid)\n\n\nNext, we make predictions.\n\nY_pred_knn = knn.predict(Xs_valid)"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#confusion-matrix",
    "href": "Classification/KNearestNeighbours.slides.html#confusion-matrix",
    "title": "K-nearest neighbors",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\n# Calcular matriz de confusión.\ncm = confusion_matrix(Y_valid, Y_pred_knn)\n\n# Mostrar matriz de confusión.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#finding-the-best-value-of-k",
    "href": "Classification/KNearestNeighbours.slides.html#finding-the-best-value-of-k",
    "title": "K-nearest neighbors",
    "section": "Finding the best value of K",
    "text": "Finding the best value of K\nWe can determine the best value of K for the KNN algorithm. To this end, we evaluate the performance of the KNN for different values of \\(K\\) in terms of accuracy on the validation dataset.\n\nbest_k = 1\nbest_accuracy = 0\nk_values = range(1, 50)  # Test k values from 1 to 50\nvalidation_accuracies = []\n\nfor k in k_values:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(Xs_train, Y_train)\n    val_accuracy = accuracy_score(Y_valid, model.predict(Xs_valid))\n    validation_accuracies.append(val_accuracy)\n\n    if val_accuracy &gt; best_accuracy:\n        best_accuracy = val_accuracy\n        best_k = k"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#visualize",
    "href": "Classification/KNearestNeighbours.slides.html#visualize",
    "title": "K-nearest neighbors",
    "section": "Visualize",
    "text": "Visualize\nWe can then visualize the accuracy for different values of \\(K\\) using the following graph and code.\n\n\nCode\nplt.figure(figsize=(6.3, 4.3))\nplt.plot(k_values, validation_accuracies, marker=\"o\", linestyle=\"-\")\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Validation Accuracy\")\nplt.title(\"Choosing the Best k for KNN\")\nplt.show()"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#section-1",
    "href": "Classification/KNearestNeighbours.slides.html#section-1",
    "title": "K-nearest neighbors",
    "section": "",
    "text": "Finally, we select the best number of nearest neighbors contained in the best_k object.\n\nKNN_final = KNeighborsClassifier(n_neighbors = best_k)\nKNN_final.fit(Xs_train, Y_train)\n\n\nThe accuracy of the best KNN is\n\nY_pred_KNNfinal = KNN_final.predict(Xs_valid)\nvalid_accuracy = accuracy_score(Y_valid, Y_pred_KNNfinal)\nprint(valid_accuracy)\n\n0.95"
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#discussion",
    "href": "Classification/KNearestNeighbours.slides.html#discussion",
    "title": "K-nearest neighbors",
    "section": "Discussion",
    "text": "Discussion\n\nKNN is intuitive and simple and can produce decent predictions. However, KNN has some disadvantages:\n\nWhen the training dataset is very large, KNN is computationally expensive. This is because, to predict an observation, we need to calculate the distance between that observation and all the others in the dataset. (“Lazy learner”).\nIn this case, a decision tree is more advantageous because it is easy to build, store, and make predictions with."
  },
  {
    "objectID": "Classification/KNearestNeighbours.slides.html#section-2",
    "href": "Classification/KNearestNeighbours.slides.html#section-2",
    "title": "K-nearest neighbors",
    "section": "",
    "text": "The predictive performance of KNN deteriorates as the number of predictors increases.\nThis is because the expected distance to the nearest neighbor increases dramatically with the number of predictors, unless the size of the dataset increases exponentially with this number.\nThis is known as the curse of dimensionality.\n\n\n\n\nhttps://aiaspirant.com/curse-of-dimensionality/"
  },
  {
    "objectID": "Classification/Classification.slides.html#agenda",
    "href": "Classification/Classification.slides.html#agenda",
    "title": "Classification Trees",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nClassification Trees\nClassification Algorithm Metrics"
  },
  {
    "objectID": "Classification/Classification.slides.html#load-the-libraries",
    "href": "Classification/Classification.slides.html#load-the-libraries",
    "title": "Classification Trees",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Classification/Classification.slides.html#main-data-science-problems",
    "href": "Classification/Classification.slides.html#main-data-science-problems",
    "title": "Classification Trees",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "Classification/Classification.slides.html#main-data-science-problems-1",
    "href": "Classification/Classification.slides.html#main-data-science-problems-1",
    "title": "Classification Trees",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "Classification/Classification.slides.html#terminology",
    "href": "Classification/Classification.slides.html#terminology",
    "title": "Classification Trees",
    "section": "Terminology",
    "text": "Terminology\n\nExplanatory variables or predictors:\n\n\\(X\\) represents an explanatory variable or predictor.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a collection of \\(p\\) predictors."
  },
  {
    "objectID": "Classification/Classification.slides.html#section",
    "href": "Classification/Classification.slides.html#section",
    "title": "Classification Trees",
    "section": "",
    "text": "Response:\n\n\\(Y\\) is a categorical variable that takes 2 categories or classes.\nFor example, \\(Y\\) can take 0 or 1, A or B, no or yes, spam or no spam.\nWhen classes are strings, they are usually encoded as 0 and 1.\n\nThe target class is the one for which \\(Y = 1\\).\nThe reference class is the one for which \\(Y = 0\\)."
  },
  {
    "objectID": "Classification/Classification.slides.html#classification-algorithms",
    "href": "Classification/Classification.slides.html#classification-algorithms",
    "title": "Classification Trees",
    "section": "Classification algorithms",
    "text": "Classification algorithms\n\nClassification algorithms use predictor values to predict the class of the response (target or reference).\n\nThat is, for an unseen record, they use predictor values to predict whether the record belongs to the target class or not.\n\nTechnically, they predict the probability that the record belongs to the target class."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-1",
    "href": "Classification/Classification.slides.html#section-1",
    "title": "Classification Trees",
    "section": "",
    "text": "Goal: Develop a function \\(C(\\boldsymbol{X})\\) for predicting \\(Y = \\{0, 1\\}\\) from \\(\\boldsymbol{X}\\).\n\n\nTo achieve this goal, most algorithms consider functions \\(C(\\boldsymbol{X})\\) that predict the probability that \\(Y\\) takes the value of 1.\n\n\n\nA probability for each class can be very useful for gauging the model’s confidence about the predicted classification."
  },
  {
    "objectID": "Classification/Classification.slides.html#example-1",
    "href": "Classification/Classification.slides.html#example-1",
    "title": "Classification Trees",
    "section": "Example 1",
    "text": "Example 1\nConsider a spam filter where \\(Y\\) is the email type.\n\nThe target class is spam. In this case, \\(Y=1\\).\nThe reference class is not spam. In this case, \\(Y=0\\).\n\n\n\n\n\n\n\n\n\nBoth emails would be classified as spam. However, we would have greater confidence in our classification for the second email."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-2",
    "href": "Classification/Classification.slides.html#section-2",
    "title": "Classification Trees",
    "section": "",
    "text": "Technically, \\(C(\\boldsymbol{X})\\) works with the conditional probability:\n\\[P(Y = 1 | X_1 = x_1, X_2 = x_2, \\ldots, X_p = x_p) = P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\]\nIn words, this is the probability that \\(Y\\) takes a value of 1 given that the predictors \\(\\boldsymbol{X}\\) have taken the values \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)\\).\n\n\nThe conditional probability that \\(Y\\) takes the value of 0 is\n\\[P(Y = 0 | \\boldsymbol{X} = \\boldsymbol{x}) = 1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}).\\]"
  },
  {
    "objectID": "Classification/Classification.slides.html#bayes-classifier",
    "href": "Classification/Classification.slides.html#bayes-classifier",
    "title": "Classification Trees",
    "section": "Bayes classifier",
    "text": "Bayes classifier\n\nIt turns out that, if we know the true structure of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\), we can build a good classification function called the Bayes classifier:\n\\[C(\\boldsymbol{X}) =\n    \\begin{cases}\n      1, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n    \\end{cases}.\\]\nThis function classifies to the most probable class using the conditional distribution \\(P(Y | \\boldsymbol{X} = \\boldsymbol{x})\\)."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-3",
    "href": "Classification/Classification.slides.html#section-3",
    "title": "Classification Trees",
    "section": "",
    "text": "HOWEVER, we don’t (and will never) know the true form of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\)!\n\n\nTo overcome this issue, we several standard solutions:\n\nLogistic Regression: Impose an structure on \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\). This was covered in IN1002B.\nClassification Trees: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. What we will cover today.\nEnsemble methods and K-Nearest Neighbours: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. (If time permits)."
  },
  {
    "objectID": "Classification/Classification.slides.html#two-datasets",
    "href": "Classification/Classification.slides.html#two-datasets",
    "title": "Classification Trees",
    "section": "Two datasets",
    "text": "Two datasets\n\nThe application of data science algorithms needs two data sets:\n\nTraining data is data that we use to train or construct the estimated function \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is data that we use to evaluate the predictive performance of \\(\\hat{f}(\\boldsymbol{X})\\) only."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-4",
    "href": "Classification/Classification.slides.html#section-4",
    "title": "Classification Trees",
    "section": "",
    "text": "A random sample of \\(n\\) observations.\nUse it to construct \\(\\hat{f}(\\boldsymbol{X})\\).\n\n\n\n\n\nAnother random sample of \\(n_t\\) observations, which is independent of the training data.\nUse it to evaluate \\(\\hat{f}(\\boldsymbol{X})\\)."
  },
  {
    "objectID": "Classification/Classification.slides.html#validation-dataset",
    "href": "Classification/Classification.slides.html#validation-dataset",
    "title": "Classification Trees",
    "section": "Validation dataset",
    "text": "Validation dataset\nIn many practical situations, a test dataset is not available. To overcome this issue, we use a validation dataset.\n\n\nIdea: Apply model to your validation dataset to mimic what will happen when you apply it to test dataset."
  },
  {
    "objectID": "Classification/Classification.slides.html#example-2-identifying-counterfeit-banknotes",
    "href": "Classification/Classification.slides.html#example-2-identifying-counterfeit-banknotes",
    "title": "Classification Trees",
    "section": "Example 2: Identifying Counterfeit Banknotes",
    "text": "Example 2: Identifying Counterfeit Banknotes"
  },
  {
    "objectID": "Classification/Classification.slides.html#dataset",
    "href": "Classification/Classification.slides.html#dataset",
    "title": "Classification Trees",
    "section": "Dataset",
    "text": "Dataset\nThe data is located in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7"
  },
  {
    "objectID": "Classification/Classification.slides.html#how-do-we-generate-validation-data",
    "href": "Classification/Classification.slides.html#how-do-we-generate-validation-data",
    "title": "Classification Trees",
    "section": "How do we generate validation data?",
    "text": "How do we generate validation data?\nWe split the current dataset into a training and a validation dataset. To this end, we use the function train_test_split() from scikit-learn.\n\nThe function has three main inputs:\n\nA pandas dataframe with the predictor columns only.\nA pandas dataframe with the response column only.\nThe parameter test_size which sets the portion of the dataset that will go to the validation set."
  },
  {
    "objectID": "Classification/Classification.slides.html#create-the-predictor-matrix",
    "href": "Classification/Classification.slides.html#create-the-predictor-matrix",
    "title": "Classification Trees",
    "section": "Create the predictor matrix",
    "text": "Create the predictor matrix\nWe use the function .drop() from pandas. This function drops one or more columns from a data frame. Let’s drop the response column Status and store the result in X_full.\n\n# Set full matrix of predictors.\nX_full = bank_data.drop(columns = ['Status']) \nX_full.head(4)\n\n\n\n\n\n\n\n\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\n131.0\n131.1\n9.0\n9.7\n\n\n1\n129.7\n129.7\n8.1\n9.5\n\n\n2\n129.7\n129.7\n8.7\n9.6\n\n\n3\n129.7\n129.6\n7.5\n10.4"
  },
  {
    "objectID": "Classification/Classification.slides.html#create-the-response-column",
    "href": "Classification/Classification.slides.html#create-the-response-column",
    "title": "Classification Trees",
    "section": "Create the response column",
    "text": "Create the response column\nWe use the function .filter() from pandas to extract the column Status from the data frame. We store the result in Y_full.\n\n# Set full matrix of responses.\nY_full = bank_data.filter(['Status'])\nY_full.head(4)\n\n\n\n\n\n\n\n\nStatus\n\n\n\n\n0\ngenuine\n\n\n1\ngenuine\n\n\n2\ngenuine\n\n\n3\ngenuine"
  },
  {
    "objectID": "Classification/Classification.slides.html#set-the-target-category",
    "href": "Classification/Classification.slides.html#set-the-target-category",
    "title": "Classification Trees",
    "section": "Set the target category",
    "text": "Set the target category\nTo set the target category in the response we use the get_dummies() function.\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_full, dtype = 'int')\n\n# Select target variable.\nY_target_full = Y_dummies['Status_counterfeit']\n\n# Show target variable.\nY_target_full.head() \n\n0    0\n1    0\n2    0\n3    0\n4    0\nName: Status_counterfeit, dtype: int64"
  },
  {
    "objectID": "Classification/Classification.slides.html#lets-partition-the-dataset",
    "href": "Classification/Classification.slides.html#lets-partition-the-dataset",
    "title": "Classification Trees",
    "section": "Let’s partition the dataset",
    "text": "Let’s partition the dataset\n\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n                                                      test_size = 0.3)\n\n\nThe function makes a clever partition of the data using the empirical distribution of the response.\nTechnically, it splits the data so that the distribution of the response under the training and validation sets is similar.\nUsually, the proportion of the dataset that goes to the validation set is 20% or 30%."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-5",
    "href": "Classification/Classification.slides.html#section-5",
    "title": "Classification Trees",
    "section": "",
    "text": "The predictors and response in the training dataset are in the objects X_train and Y_train, respectively. We compile these objects into a single dataset using the function .concat() from pandas. The argument axis = 1 tells .concat() to concatenate the datasets by their rows.\n\ntraining_dataset = pd.concat([X_train, Y_train], axis = 1)\ntraining_dataset.head(4)\n\n\n\n\n\n\n\n\nLeft\nRight\nBottom\nTop\nStatus_counterfeit\n\n\n\n\n166\n130.7\n130.4\n10.0\n10.1\n1\n\n\n31\n129.7\n129.7\n8.6\n9.1\n0\n\n\n175\n130.4\n130.2\n11.9\n10.7\n1\n\n\n101\n130.5\n130.2\n11.0\n11.5\n1"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-6",
    "href": "Classification/Classification.slides.html#section-6",
    "title": "Classification Trees",
    "section": "",
    "text": "Equivalently, the predictors and response in the validation dataset are in the objects X_valid and Y_valid, respectively.\n\nvalidation_dataset = pd.concat([X_valid, Y_valid], axis = 1)\nvalidation_dataset.head()\n\n\n\n\n\n\n\n\nLeft\nRight\nBottom\nTop\nStatus_counterfeit\n\n\n\n\n182\n130.5\n130.1\n11.0\n11.4\n1\n\n\n8\n129.4\n129.7\n8.2\n11.0\n0\n\n\n5\n130.8\n130.5\n9.0\n10.1\n0\n\n\n118\n130.5\n130.2\n11.0\n11.0\n1\n\n\n28\n130.0\n130.0\n7.4\n10.5\n0"
  },
  {
    "objectID": "Classification/Classification.slides.html#work-on-your-training-dataset",
    "href": "Classification/Classification.slides.html#work-on-your-training-dataset",
    "title": "Classification Trees",
    "section": "Work on your training dataset",
    "text": "Work on your training dataset\nAfter we have partitioned the data, we work on the training data to develop our predictive pipeline.\nThe pipeline has two main steps:\n\nData preprocessing.\nModel development.\n\nWe will now discuss preprocessing techniques applied to the predictor columns in the training dataset.\nNote that all preprocessing techniques will also be applied to the validation and test datasets to prepare it for your model!"
  },
  {
    "objectID": "Classification/Classification.slides.html#decision-tree",
    "href": "Classification/Classification.slides.html#decision-tree",
    "title": "Classification Trees",
    "section": "Decision tree",
    "text": "Decision tree\nIt is a supervised learning algorithm that predicts or classifies observations using a hierarchical tree structure.\n\nMain characteristics:\n\nSimple and useful for interpretation.\nCan handle numerical and categorical predictors and responses.\nComputationally efficient.\nNonparametric technique."
  },
  {
    "objectID": "Classification/Classification.slides.html#basic-idea-of-a-decision-tree",
    "href": "Classification/Classification.slides.html#basic-idea-of-a-decision-tree",
    "title": "Classification Trees",
    "section": "Basic idea of a decision tree",
    "text": "Basic idea of a decision tree\nStratify or segment the prediction space into several simpler regions."
  },
  {
    "objectID": "Classification/Classification.slides.html#how-do-you-build-a-decision-tree",
    "href": "Classification/Classification.slides.html#how-do-you-build-a-decision-tree",
    "title": "Classification Trees",
    "section": "How do you build a decision tree?",
    "text": "How do you build a decision tree?\n\nBuilding decision trees involves two main procedures:\n\nGrow a large tree.\nPrune the tree to prevent overfitting.\n\nAfter building a “good” tree, we can predict new observations that are not in the data set we used to build it."
  },
  {
    "objectID": "Classification/Classification.slides.html#how-do-we-grow-a-tree",
    "href": "Classification/Classification.slides.html#how-do-we-grow-a-tree",
    "title": "Classification Trees",
    "section": "How do we grow a tree?",
    "text": "How do we grow a tree?\n\n\nUsing the CART algorithm!\n\nThe algorithm uses a recursive binary splitting strategy that builds the tree using a greedy top-down approach.\nBasically, at a given node, it considers all variables and all possible splits of that variable. Then, for classification, it chooses the best variable and splits it that minimizes the so-called impurity."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-21",
    "href": "Classification/Classification.slides.html#section-21",
    "title": "Classification Trees",
    "section": "",
    "text": "We repeat the partitioning process until the terminal nodes have no less than, say, 5 observations."
  },
  {
    "objectID": "Classification/Classification.slides.html#what-is-impurity",
    "href": "Classification/Classification.slides.html#what-is-impurity",
    "title": "Classification Trees",
    "section": "What is impurity?",
    "text": "What is impurity?\nNode impurity refers to the homogeneity of the response classes at that node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe CART algorithm minimizes impurity between tree nodes."
  },
  {
    "objectID": "Classification/Classification.slides.html#how-do-we-measure-impurity",
    "href": "Classification/Classification.slides.html#how-do-we-measure-impurity",
    "title": "Classification Trees",
    "section": "How do we measure impurity?",
    "text": "How do we measure impurity?\n\n\n\nThere are three different metrics for impurity:\n\nRisk of misclassification.\nCross entropy.\nGini impurity index.\n\n\n \n\nProportion of elements in a class"
  },
  {
    "objectID": "Classification/Classification.slides.html#pruning-the-tree",
    "href": "Classification/Classification.slides.html#pruning-the-tree",
    "title": "Classification Trees",
    "section": "Pruning the tree",
    "text": "Pruning the tree\nTo avoid overfitting, we pruned some of the tree’s branches. More specifically, we collapsed two internal (non-terminal) nodes."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-24",
    "href": "Classification/Classification.slides.html#section-24",
    "title": "Classification Trees",
    "section": "",
    "text": "To prune a tree, we use an advanced algorithm to measure the contribution of the tree’s branches.\nThe algorithm has a tuning parameter called \\(\\alpha\\), which places greater weight on the number of tree nodes (or size):\n\nLarge values of \\(\\alpha\\) result in small trees with few nodes.\nSmall values of \\(\\alpha\\) result in large trees with many nodes."
  },
  {
    "objectID": "Classification/Classification.slides.html#in-python",
    "href": "Classification/Classification.slides.html#in-python",
    "title": "Classification Trees",
    "section": "In Python",
    "text": "In Python\n\nIn Python, we can use the DecisionTreeClassifier() and fit() functions from scikit-learn to train a decision tree.\n\n# We tell Python we want a classification tree\nclf = DecisionTreeClassifier(max_depth=5, random_state=507134)\n\n# We train the classification tree using the training data.\nclf.fit(X_train, Y_train)\n\nThe parameters max_depth of DecisionTreeClassifier() controls the depth of the tree. The parameter random_state allows you to reproduce the same tree in different runs of the Python code."
  },
  {
    "objectID": "Classification/Classification.slides.html#plotting-the-tree",
    "href": "Classification/Classification.slides.html#plotting-the-tree",
    "title": "Classification Trees",
    "section": "Plotting the tree",
    "text": "Plotting the tree\n\nTo see the decision tree, we use the plot_tree function from scikit-learn and some commands from matplotlib. Specifically, we use the plt.figure() functions to define the size of the figure and plt.show() to display the figure.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf, filled=True, rounded=True)\nplt.show()"
  },
  {
    "objectID": "Classification/Classification.slides.html#implementation-details",
    "href": "Classification/Classification.slides.html#implementation-details",
    "title": "Classification Trees",
    "section": "Implementation details",
    "text": "Implementation details\n\nCategorical predictors with unordered levels \\(\\{A, B, C\\}\\). We order the levels in a specific way (works for binary and regression problems).\nPredictors with missing values. For quantitative predictors, we use multiple imputation. For categorical predictors, we create a new “NA” level.\nTertiary or quartary splits. There is not much improvement.\nDiagonal splits (using a linear combination for partitioning). These can lead to improvement, but they impair interpretability."
  },
  {
    "objectID": "Classification/Classification.slides.html#apply-penalty-for-large-trees",
    "href": "Classification/Classification.slides.html#apply-penalty-for-large-trees",
    "title": "Classification Trees",
    "section": "Apply penalty for large trees",
    "text": "Apply penalty for large trees\nNow, let’s illustrate the pruning technique to find a small decision tree that performs well. To do this, we will train several decision trees using different values of the \\(\\alpha\\) parameter, which weighs the contribution of the tree branches.\n\nIn Python, this is achieved using the cost_complexity_pruning_path() function with the following syntax.\n\npath = clf.cost_complexity_pruning_path(X_train, Y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-26",
    "href": "Classification/Classification.slides.html#section-26",
    "title": "Classification Trees",
    "section": "",
    "text": "The ccp_alphas and impurities objects contain the different values of the \\(\\alpha\\) parameter used, as well as the impurity performance of the generated trees.\nTo train a decision tree using different alpha values, we use the following code that iterates over the values contained in ccp_alphas.\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=507134, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, Y_train)\n    clfs.append(clf)\n\nIn the next section, we will evaluate the performance of these decision trees."
  },
  {
    "objectID": "Classification/Classification.slides.html#evaluation",
    "href": "Classification/Classification.slides.html#evaluation",
    "title": "Classification Trees",
    "section": "Evaluation",
    "text": "Evaluation\n\nWe evaluate a classification tree by classifying observations that were not used for training.\nThat is, we use the classifier to predict categories in the validation data set using only the predictor values from this set.\nIn Python, we use the commands:\n\n# Predict classes.\npredicted_class = clf.predict(X_valid)"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-27",
    "href": "Classification/Classification.slides.html#section-27",
    "title": "Classification Trees",
    "section": "",
    "text": "The predict() function generates actual classes to which each observation was assigned.\n\npredicted_class\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-28",
    "href": "Classification/Classification.slides.html#section-28",
    "title": "Classification Trees",
    "section": "",
    "text": "The predict_proba() function generates the probabilities used by the algorithm to assign the classes.\n\n# Predict probabilities.\npredicted_probability = clf.predict_proba(X_valid)\npredicted_probability\n\narray([[0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571],\n       [0.43571429, 0.56428571]])"
  },
  {
    "objectID": "Classification/Classification.slides.html#confusion-matrix",
    "href": "Classification/Classification.slides.html#confusion-matrix",
    "title": "Classification Trees",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nTable to evaluate the performance of a classifier.\nCompares actual values with the predicted values of a classifier.\nUseful for binary and multiclass classification problems."
  },
  {
    "objectID": "Classification/Classification.slides.html#in-python-1",
    "href": "Classification/Classification.slides.html#in-python-1",
    "title": "Classification Trees",
    "section": "In Python",
    "text": "In Python\n\nWe calculate the confusion matrix using the homonymous function scikit-learn.\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Show confusion matrix.\nprint(cm)\n\n[[ 0 39]\n [ 0 21]]"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-29",
    "href": "Classification/Classification.slides.html#section-29",
    "title": "Classification Trees",
    "section": "",
    "text": "We can display the confusion matrix using the ConfusionMatrixDisplay() function.\n\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Classification/Classification.slides.html#accuracy",
    "href": "Classification/Classification.slides.html#accuracy",
    "title": "Classification Trees",
    "section": "Accuracy",
    "text": "Accuracy\nA simple metric for summarizing the information in the confusion matrix is accuracy. It is the proportion of correct classifications for both classes, out of the total classifications performed.\nIn Python, we calculate accuracy using the scikit-learn accuracy_score() function.\n\naccuracy = accuracy_score(Y_valid, predicted_class)\nprint( round(accuracy, 2) )\n\n0.35\n\n\nThe higher the accuracy, the better the performance of the classifier."
  },
  {
    "objectID": "Classification/Classification.slides.html#lets-get-back-to-the-penalized-trees",
    "href": "Classification/Classification.slides.html#lets-get-back-to-the-penalized-trees",
    "title": "Classification Trees",
    "section": "Let’s get back to the penalized trees",
    "text": "Let’s get back to the penalized trees\n\nNow, for each of those trees contained in clfs, we evaluate the performance in terms of accuracy for the training and validation data.\n\ntrain_scores = [clf.score(X_train, Y_train) for clf in clfs]\nvalidation_scores = [clf.score(X_valid, Y_valid) for clf in clfs]"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-30",
    "href": "Classification/Classification.slides.html#section-30",
    "title": "Classification Trees",
    "section": "",
    "text": "We visualize the results using the following plot.\n\n\nCode\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Accuracy\")\nax.set_title(\"Accuracy vs alpha for training and validation data\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, validation_scores, marker=\"o\", label=\"validation\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()"
  },
  {
    "objectID": "Classification/Classification.slides.html#choosing-the-best-tree",
    "href": "Classification/Classification.slides.html#choosing-the-best-tree",
    "title": "Classification Trees",
    "section": "Choosing the best tree",
    "text": "Choosing the best tree\nWe can see that the best \\(\\alpha\\) value for the validation dataset is 0.01.\nTo train our new reduced tree, we use the DecisionTreeClassifier() function again with the ccp_alpha parameter set to the best \\(\\alpha\\) value. The object containing this new tree is clf_simple.\n\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0.01)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-31",
    "href": "Classification/Classification.slides.html#section-31",
    "title": "Classification Trees",
    "section": "",
    "text": "Once this is done, we can visualize the small tree using the plot_tree() function.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf_simple, filled=True, rounded=True)\nplt.show()"
  },
  {
    "objectID": "Classification/Classification.slides.html#evaluation-1",
    "href": "Classification/Classification.slides.html#evaluation-1",
    "title": "Classification Trees",
    "section": "Evaluation",
    "text": "Evaluation\nLet’s compute the accuracy of the pruned tree.\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.98"
  },
  {
    "objectID": "Classification/Classification.slides.html#comments",
    "href": "Classification/Classification.slides.html#comments",
    "title": "Classification Trees",
    "section": "Comments",
    "text": "Comments\n\n\nAccuracy is easy to calculate and interpret.\nIt works well when the data set has a balanced class distribution (i.e., cases 1 and 0 are approximately equal).\nHowever, there are situations in which identifying the target class is more important than the reference class.\nFor example, it is not ideal for unbalanced data sets. When one class is much more frequent than the other, accuracy can be misleading."
  },
  {
    "objectID": "Classification/Classification.slides.html#an-example",
    "href": "Classification/Classification.slides.html#an-example",
    "title": "Classification Trees",
    "section": "An example",
    "text": "An example\n\nLet’s say we want to create a classifier that tells us whether a mobile phone company’s customer will churn next month.\nCustomers who churn significantly decrease the company’s revenue. That’s why it’s important to retain these customers.\nTo retain that customer, the company will send them a text message with an offer for a low-cost mobile plan.\nIdeally, our classifier correctly identifies customers who will churn, so they get the offer and, hopefully, stay."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-32",
    "href": "Classification/Classification.slides.html#section-32",
    "title": "Classification Trees",
    "section": "",
    "text": "In other words, we want to avoid making wrong decisions about customers who will churn.\nWrong decisions about loyal customers aren’t as relevant.\nBecause if we classify a loyal customer as one who will churn, the customer will get a good deal. They’ll probably pay less but stay anyway."
  },
  {
    "objectID": "Classification/Classification.slides.html#another-example",
    "href": "Classification/Classification.slides.html#another-example",
    "title": "Classification Trees",
    "section": "Another example",
    "text": "Another example\n\nAnother example is developing an algorithm (classifier) that can quickly identify patients who may have a rare disease and need a more extensive and expensive medical evaluation.\nThe classifier must make correct decisions about patients with the rare disease, so they can be evaluated and eventually treated.\nA healthy patient who is misclassified with the disease will only incur a few extra dollars to pay for the next test, only to discover that the patient does not have the disease."
  },
  {
    "objectID": "Classification/Classification.slides.html#classification-specific-metrics",
    "href": "Classification/Classification.slides.html#classification-specific-metrics",
    "title": "Classification Trees",
    "section": "Classification-specific metrics",
    "text": "Classification-specific metrics\n\nTo overcome this limitation of accuracy, there are several class-specific metrics. The most popular are:\n\nSensitivity or recall\nPrecision\nType I error\n\nThese metrics are calculated from the confusion matrix."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-33",
    "href": "Classification/Classification.slides.html#section-33",
    "title": "Classification Trees",
    "section": "",
    "text": "Sensitivity or recall = OO/(OO + OR) “How many records of the target class did we predict correctly?”"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-34",
    "href": "Classification/Classification.slides.html#section-34",
    "title": "Classification Trees",
    "section": "",
    "text": "Precision = OO/(OO + RO) How many of the records we predicted as target class were classified correctly?"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-35",
    "href": "Classification/Classification.slides.html#section-35",
    "title": "Classification Trees",
    "section": "",
    "text": "In Python, we compute sensitivity and precision using the following commands:\n\nrecall = recall_score(Y_valid, predicted_class)\nprint( round(recall, 2) )\n\n1.0\n\n\n\n\nprecision = precision_score(Y_valid, predicted_class)\nprint( round(precision, 2) )\n\n0.35"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-36",
    "href": "Classification/Classification.slides.html#section-36",
    "title": "Classification Trees",
    "section": "",
    "text": "Type I error = RO/(RO + RR) “How many of the reference records did we incorrectly predict as targets?”"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-37",
    "href": "Classification/Classification.slides.html#section-37",
    "title": "Classification Trees",
    "section": "",
    "text": "Unfortunately, there is no simple command to calculate the type-I error in sklearn. To overcome this issue, we must calculate it manually.\n\n# Confusion matrix to compute Type-I error\ntn, fp, fn, tp = confusion_matrix(Y_valid, predicted_class).ravel()\n\n# Type-I error rate = False Positive Rate = FP / (FP + TN)\ntype_I_error = fp / (fp + tn)\nprint( round(type_I_error, 2) )\n\n1.0"
  },
  {
    "objectID": "Classification/Classification.slides.html#discussion",
    "href": "Classification/Classification.slides.html#discussion",
    "title": "Classification Trees",
    "section": "Discussion",
    "text": "Discussion\n\n\nThere is generally a trade-off between sensitivity and Type I error.\nIntuitively, increasing the sensitivity of a classifier is likely to increase Type I error, because more observations are predicted as positive.\nPossible trade-offs between sensitivity and Type I error may be appropriate when there are different penalties or costs associated with each type of error."
  },
  {
    "objectID": "Classification/Classification.slides.html#activity-2.1-classification-and-metrics-solo-mode",
    "href": "Classification/Classification.slides.html#activity-2.1-classification-and-metrics-solo-mode",
    "title": "Classification Trees",
    "section": "Activity 2.1: Classification and Metrics (solo mode)",
    "text": "Activity 2.1: Classification and Metrics (solo mode)\nUsing the data in the “weight-height.csv” table, apply the CART procedure to build a decision tree useful for predicting a person’s sex based on their weight and height.\nIn this example, the predictor variables are continuous, and the predictor variable is binary.\nInterpret the Precision, Accuracy, Sensitivity, and Type 1 Error values for the validation set. If the software doesn’t report them, perform the calculations using the confusion matrix. Use “Female” as the target class. Discuss the effectiveness of the resulting model."
  },
  {
    "objectID": "Classification/Classification.slides.html#disadvantages-of-decision-trees",
    "href": "Classification/Classification.slides.html#disadvantages-of-decision-trees",
    "title": "Classification Trees",
    "section": "Disadvantages of decision trees",
    "text": "Disadvantages of decision trees\n\nDecision trees have high variance. A small change in the training data can result in a very different tree.\nIt has trouble identifying simple data structures."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#agenda",
    "href": "Classification/EnsembleMethods.slides.html#agenda",
    "title": "Ensemble Methods",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction to Ensemble Methods\nBagging\nRandom Forests"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#load-the-libraries",
    "href": "Classification/EnsembleMethods.slides.html#load-the-libraries",
    "title": "Ensemble Methods",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#decision-trees",
    "href": "Classification/EnsembleMethods.slides.html#decision-trees",
    "title": "Ensemble Methods",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\nSimple and useful for interpretations.\nCan handle continuous and categorical predictors and responses. So, they can be applied to both classification and regression problems.\nComputationally efficient."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#limitations-of-decision-trees",
    "href": "Classification/EnsembleMethods.slides.html#limitations-of-decision-trees",
    "title": "Ensemble Methods",
    "section": "Limitations of decision trees",
    "text": "Limitations of decision trees\n\n\nIn general, decision trees do not work well for classification and regression problems.\nHowever, decision trees can be combined to build effective algorithms for these problems."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#ensamble-methods-1",
    "href": "Classification/EnsembleMethods.slides.html#ensamble-methods-1",
    "title": "Ensemble Methods",
    "section": "Ensamble methods",
    "text": "Ensamble methods\n\nEnsemble methods refer to frameworks to combine decision trees.\nHere, we will cover a popular ensamble method:\n\nBagging. Ensemble many deep trees.\n\nQuintessential method: Random Forests."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#bootstrap-samples",
    "href": "Classification/EnsembleMethods.slides.html#bootstrap-samples",
    "title": "Ensemble Methods",
    "section": "Bootstrap samples",
    "text": "Bootstrap samples\n\nBootstrap samples are samples obtained with replacement from the original sample. So, an observation can occur more than one in a bootstrap sample.\nBootstrap samples are the building block of the bootstrap method, which is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#bagging-1",
    "href": "Classification/EnsembleMethods.slides.html#bagging-1",
    "title": "Ensemble Methods",
    "section": "Bagging",
    "text": "Bagging\nGiven a training dataset, bagging averages the predictions from decision trees over a collection of bootstrap samples."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#predictions",
    "href": "Classification/EnsembleMethods.slides.html#predictions",
    "title": "Ensemble Methods",
    "section": "Predictions",
    "text": "Predictions\n\nLet \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)\\) be a vector of new predictor values. For classification problems with 2 classes:\n\nEach classification tree outputs the probability for class 1 and 2 depending on the region \\(\\mathbf{x}\\) falls in.\nFor the b-th tree, we denote the probabilities as \\(\\hat{p}^{b}_0(\\boldsymbol{x})\\) and \\(\\hat{p}^{b}_1(\\boldsymbol{x})\\) for class 0 and 1, respectively."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#section",
    "href": "Classification/EnsembleMethods.slides.html#section",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Using the probabilities, a standard tree follows the Bayes Classifier to output the actual class:\n\n\\[\\hat{T}_{b}(\\boldsymbol{x}) =\n    \\begin{cases}\n      1, & \\hat{p}^{b}_1(\\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\hat{p}^{b}_1(\\boldsymbol{x}) \\leq 0.5\n    \\end{cases}\\]"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#section-1",
    "href": "Classification/EnsembleMethods.slides.html#section-1",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Compute the proportion of trees that output a 0 as\n\n\\[p_{bag, 0} = \\frac{1}{B} \\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 0).\\]\n\nCompute the proportion of trees that output a 1 as:\n\n\\[p_{bag, 1} = \\frac{1}{B}\\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 1).\\]"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#section-2",
    "href": "Classification/EnsembleMethods.slides.html#section-2",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Classify according to a majority vote between \\(p_{bag, 0}\\) and \\(p_{bag, 1}\\)."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#implementation",
    "href": "Classification/EnsembleMethods.slides.html#implementation",
    "title": "Ensemble Methods",
    "section": "Implementation",
    "text": "Implementation\n\n\nHow many trees? No risk of overfitting, so use plenty.\nNo pruning necessary to build the trees. However, one can still decide to apply some pruning or early stopping mechanism.\nThe size of bootstrap samples is the same as the size of the training dataset, but we can use a different size."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#example-1",
    "href": "Classification/EnsembleMethods.slides.html#example-1",
    "title": "Ensemble Methods",
    "section": "Example 1",
    "text": "Example 1\n\nThe data “AdultReduced.xlsx” comes from the UCI Machine Learning Repository and is derived from US census records. In this data, the goal is to predict whether a person’s income was high (defined in 1994 as more than $50,000) or low.\nPredictors include education level, job type (e.g., never worked and local government), capital gains/losses, hours worked per week, country of origin, etc. The data contains 7,508 records."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#read-the-dataset",
    "href": "Classification/EnsembleMethods.slides.html#read-the-dataset",
    "title": "Ensemble Methods",
    "section": "Read the dataset",
    "text": "Read the dataset\n\n# Load the data\nAdult_data = pd.read_excel('AdultReduced.xlsx')\n\n# Preview the data.\nAdult_data.head(3)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation.num\nmarital.status\noccupation\nrelationship\nrace\nsex\ncapital.gain\ncapital.loss\nhours.per.week\nnative.country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\nsmall\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\nsmall\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\nsmall"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#selected-predictors.",
    "href": "Classification/EnsembleMethods.slides.html#selected-predictors.",
    "title": "Ensemble Methods",
    "section": "Selected predictors.",
    "text": "Selected predictors.\n\nage: Age of the individual.\nsex: Sex of the individual (male or female).\nrace: Race of the individual (W, B, Amer-Indian, Amer-Pacific, or Other).\neducation.num: number of years of education.\nhours.per.week: Number of work hours per week.\n\n\n# Choose the predictors.\nX_full = Adult_data.filter(['age', 'sex', 'race', 'education.num', \n                            'hours.per.week'])"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#pre-processing-for-categorical-predictors",
    "href": "Classification/EnsembleMethods.slides.html#pre-processing-for-categorical-predictors",
    "title": "Ensemble Methods",
    "section": "Pre-processing for categorical predictors",
    "text": "Pre-processing for categorical predictors\n\nUnfortunately, bagging does not work with categorical predictors. We must transform them into dummy variables using the code below.\n\n# Turn categorical predictors into dummy variables.\nX_dummies = pd.get_dummies(X_full[['sex', 'race']])\n\n# Drop original predictors from the test.\nX_other = X_full.drop(['sex', 'race'], axis=1)\n\n# Update the predictor matrix.\nX_full = pd.concat([X_other, X_dummies], axis=1)"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#set-the-target-class",
    "href": "Classification/EnsembleMethods.slides.html#set-the-target-class",
    "title": "Ensemble Methods",
    "section": "Set the target class",
    "text": "Set the target class\nLet’s set the target class and the reference class using the get_dummies() function.\n\n# Select answer\nY = Adult_data.filter(['income'])\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y, dtype = 'int')\n\n# Show.\nY_dummies.head(4)\n\n\n\n\n\n\n\n\nincome_large\nincome_small\n\n\n\n\n0\n0\n1\n\n\n1\n0\n1\n\n\n2\n0\n1\n\n\n3\n0\n1"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#section-3",
    "href": "Classification/EnsembleMethods.slides.html#section-3",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Here we’ll use the large target class. So, let’s use the corresponding column as our response variable.\n\n# Choose target category.\nY_target = Y_dummies['income_large']\n\nY_target.head()\n\n0    0\n1    0\n2    0\n3    0\n4    1\nName: income_large, dtype: int64"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#training-and-validation-datasets",
    "href": "Classification/EnsembleMethods.slides.html#training-and-validation-datasets",
    "title": "Ensemble Methods",
    "section": "Training and validation datasets",
    "text": "Training and validation datasets\n\nTo evaluate a model’s performance on unobserved data, we split the current dataset into training and validation datasets. To do this, we use train_test_split() from scikit-learn.\n\n# Split into training and validation\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target, \n                                                      test_size = 0.2)\n\nWe use 80% of the dataset for training and the rest for validation."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#bagging-in-python",
    "href": "Classification/EnsembleMethods.slides.html#bagging-in-python",
    "title": "Ensemble Methods",
    "section": "Bagging in Python",
    "text": "Bagging in Python\n\nWe define a bagging algorithm for classification using the BaggingClassifier function from scikit-learn.\nThe n_estimators argument is the number of decision trees to generate in bagging. Ideally, it should be high, around 500.\n\n# Set the bagging algorithm.\nBaggingalgorithm = BaggingClassifier(n_estimators = 500, \n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nBaggingalgorithm.fit(X_train, Y_train)\n\nrandom_state allows us to obtain the same bagging algorithm in different runs of the algorithm."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#predictions-1",
    "href": "Classification/EnsembleMethods.slides.html#predictions-1",
    "title": "Ensemble Methods",
    "section": "Predictions",
    "text": "Predictions\nPredict the classes using bagging.\n\npredicted_class = Baggingalgorithm.predict(X_valid)\n\npredicted_class\n\narray([0, 0, 0, ..., 0, 0, 0])"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#confusion-matrix",
    "href": "Classification/EnsembleMethods.slides.html#confusion-matrix",
    "title": "Ensemble Methods",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#accuracy",
    "href": "Classification/EnsembleMethods.slides.html#accuracy",
    "title": "Ensemble Methods",
    "section": "Accuracy",
    "text": "Accuracy\n\nThe accuracy of the bagging classifier is 78%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, predicted_class)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.78"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#a-single-deep-tree",
    "href": "Classification/EnsembleMethods.slides.html#a-single-deep-tree",
    "title": "Ensemble Methods",
    "section": "A single deep tree",
    "text": "A single deep tree\nTo compare the bagging, let’s use a single deep tree.\n\n\nCode\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0.0,\n                                    random_state=507134)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)\n\n\nLet’s compute the accuracy of the pruned tree.\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.77"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#advantages",
    "href": "Classification/EnsembleMethods.slides.html#advantages",
    "title": "Ensemble Methods",
    "section": "Advantages",
    "text": "Advantages\n\n\nBagging will have lower prediction errors than a single classification tree.\nThe fact that, for each tree, not all of the original observations were used, can be exploited to produce an estimate of the accuracy for classification."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#limitations",
    "href": "Classification/EnsembleMethods.slides.html#limitations",
    "title": "Ensemble Methods",
    "section": "Limitations",
    "text": "Limitations\n\n\nLoss of interpretability: the final bagged classifier is not a tree, and so we forfeit the clear interpretative ability of a classification tree.\nComputational complexity: we are essentially multiplying the work of growing (and possibly pruning) a single tree by B.\nFundamental issue: bagging a good model can improve predictive accuracy, but bagging a bad one can seriously degrade predictive accuracy."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#other-issues",
    "href": "Classification/EnsembleMethods.slides.html#other-issues",
    "title": "Ensemble Methods",
    "section": "Other issues",
    "text": "Other issues\n\n\nSuppose a variable is very important and decisive.\n\nIt will probably appear near the top of a large number of trees.\nAnd these trees will tend to vote the same way.\nIn some sense, then, many of the trees are “correlated”.\nThis will degrade the performance of bagging."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#section-4",
    "href": "Classification/EnsembleMethods.slides.html#section-4",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Bagging is unable to capture simple decision boundaries"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#random-forest-1",
    "href": "Classification/EnsembleMethods.slides.html#random-forest-1",
    "title": "Ensemble Methods",
    "section": "Random Forest",
    "text": "Random Forest\n\nExactly as bagging, but…\n\nWhen splitting the nodes using the CART algorithm, instead of going through all possible splits for all possible variables, we go through all possible splits on a random sample of a small number of variables \\(m\\), where \\(m &lt; p\\).\n\nRandom forests can reduce variability further."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#why-does-it-work",
    "href": "Classification/EnsembleMethods.slides.html#why-does-it-work",
    "title": "Ensemble Methods",
    "section": "Why does it work?",
    "text": "Why does it work?\n\n\nNot so dominant predictors will get a chance to appear by themselves and show “their stuff”.\nThis adds more diversity to the trees.\nThe fact that the trees in the forest are not (strongly) correlated means lower variability in the predictions and so, a bettter performance overall."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#tuning-parameter",
    "href": "Classification/EnsembleMethods.slides.html#tuning-parameter",
    "title": "Ensemble Methods",
    "section": "Tuning parameter",
    "text": "Tuning parameter\n\nHow do we set \\(m\\)?\n\nFor classification, use \\(m = \\lfloor \\sqrt{p} \\rfloor\\) and the minimum node size is 1.\n\nIn practice, sometimes the best values for these parameters will depend on the problem. So, we can treat \\(m\\) as a tuning parameter.\n\nNote that if \\(m = p\\), we get bagging."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#the-final-product-is-a-black-box",
    "href": "Classification/EnsembleMethods.slides.html#the-final-product-is-a-black-box",
    "title": "Ensemble Methods",
    "section": "The final product is a black box",
    "text": "The final product is a black box\n\n\nA black box. Inside the box are several hundred trees, each slightly different.\nYou put an observation into the black box, and the black box classifies it or predicts it for you."
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#random-forest-in-python",
    "href": "Classification/EnsembleMethods.slides.html#random-forest-in-python",
    "title": "Ensemble Methods",
    "section": "Random Forest in Python",
    "text": "Random Forest in Python\n\nIn Python, we define a RandomForest algorithm for classification using the RandomForestClassifier function from scikit-learn. The n_estimators argument is the number of decision trees to generate in the RandomForest, and random_state allows you to reprudce the results.\n\n# Set the bagging algorithm.\nRFalgorithm = RandomForestClassifier(n_estimators = 500,\n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nRFalgorithm.fit(X_train, Y_train)"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#confusion-matrix-1",
    "href": "Classification/EnsembleMethods.slides.html#confusion-matrix-1",
    "title": "Ensemble Methods",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nEvaluate the performance of random forest.\n\n\nCode\n# Predict class.\nRF_predicted = RFalgorithm.predict(X_valid)\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, RF_predicted)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Classification/EnsembleMethods.slides.html#accuracy-1",
    "href": "Classification/EnsembleMethods.slides.html#accuracy-1",
    "title": "Ensemble Methods",
    "section": "Accuracy",
    "text": "Accuracy\n\nThe accuracy of the random forest classifier is 79%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, RF_predicted)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.79"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#agenda",
    "href": "Classification/IntrotoDataScience.slides.html#agenda",
    "title": "Introduction to Data Science",
    "section": "Agenda",
    "text": "Agenda\n\n\nData Science\nSupervised and Unsupervised Learning"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#data-science-is",
    "href": "Classification/IntrotoDataScience.slides.html#data-science-is",
    "title": "Introduction to Data Science",
    "section": "Data Science is …",
    "text": "Data Science is …\na multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from vast amounts of structured and unstructured data."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#other-similar-concepts",
    "href": "Classification/IntrotoDataScience.slides.html#other-similar-concepts",
    "title": "Introduction to Data Science",
    "section": "Other Similar Concepts",
    "text": "Other Similar Concepts\n\n\nData mining is a process of discovering patterns in large data sets using methods at the intersection of statistics and database systems.\nPredictive modeling is the process of developing a model so that we can understand and quantify the accuracy of the model’s prediction in yet-to-be-seen future data sets.\nStatistical learning refers to a set of tools (statistical models and data mining methods) for modeling and understanding complex data sets."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#in-2004",
    "href": "Classification/IntrotoDataScience.slides.html#in-2004",
    "title": "Introduction to Data Science",
    "section": "In 2004…",
    "text": "In 2004…\nHurricane Frances battered the Caribbean and threatened to directly affect Florida’s Atlantic coast.\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidents headed for higher ground, but in Arkansas, Walmart executives saw a big opportunity for one of their newest data-driven weapons: predictive technology."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#section",
    "href": "Classification/IntrotoDataScience.slides.html#section",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "A week before the storm made landfall, Linda M. Dillman, Wal-Mart’s chief information officer, pressured her staff to create forecasts based on what had happened when Hurricane Charley hit the area several weeks earlier.\n\nBacked by trillions of bytes of purchase history stored in Walmart’s data warehouse, he said, the company could “start predicting what’s going to happen, rather than waiting for it to happen,” as he put it."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#the-result",
    "href": "Classification/IntrotoDataScience.slides.html#the-result",
    "title": "Introduction to Data Science",
    "section": "The result",
    "text": "The result\n\n\nThe New York Times reported\n\n\n“… Experts analyzed the data and found that stores would indeed need certain products, not just the typical flashlights.”\n\n\n\nDillman said\n\n\n“We didn’t know in the past that strawberry Pop-Tarts increase their sales, like seven times their normal sales rate, before a hurricane.”"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#cross-industry-standard-process-crisp-for-data-science",
    "href": "Classification/IntrotoDataScience.slides.html#cross-industry-standard-process-crisp-for-data-science",
    "title": "Introduction to Data Science",
    "section": "Cross-Industry Standard Process (CRISP) for Data Science",
    "text": "Cross-Industry Standard Process (CRISP) for Data Science"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#crisp-model",
    "href": "Classification/IntrotoDataScience.slides.html#crisp-model",
    "title": "Introduction to Data Science",
    "section": "CRISP Model",
    "text": "CRISP Model\n\n\nBusiness Understanding: What does the business need?\nData Understanding: What data do we have or need? Is it clean?\nData Preparation: How do we organize the data for modeling?\nModeling: What modeling techniques should we apply?\nEvaluation: Which model best meets business objectives?\nImplementation: How do stakeholders access the results?"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#business-understanding",
    "href": "Classification/IntrotoDataScience.slides.html#business-understanding",
    "title": "Introduction to Data Science",
    "section": "Business Understanding",
    "text": "Business Understanding\n\n\nBusiness understanding refers to defining the business problem you are trying to solve.\nThe goal is to reframe the business problem as a data science problem.\nReframing the problem and designing a solution is often an iterative process."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#problems-in-data-science",
    "href": "Classification/IntrotoDataScience.slides.html#problems-in-data-science",
    "title": "Introduction to Data Science",
    "section": "Problems in Data Science",
    "text": "Problems in Data Science\n\nClassification (or class probability estimation) attempts to predict, for each individual in a population, which of a (small) set of classes that individual belongs to. For example, “Among all T-Mobile customers, which ones are likely to respond to a given offer?”\n\nRegression attempts to estimate or predict, for each individual, the numerical value of some variable for that individual. For example, “How much will a given customer use the service?”"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#section-1",
    "href": "Classification/IntrotoDataScience.slides.html#section-1",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Clustering attempts to group individuals in a population based on their similarity, but not for any specific purpose. For example, “Do our customers form natural groups or segments?”"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#discussion",
    "href": "Classification/IntrotoDataScience.slides.html#discussion",
    "title": "Introduction to Data Science",
    "section": "Discussion",
    "text": "Discussion\n\n\nOften, reframing the problem and designing a solution is an iterative process.\nThe initial formulation may not be complete or optimal, so multiple iterations may be necessary to formulate an acceptable solution.\nThe key to great success is creative problem formulation by an analyst on how to frame the business problem as one or more data science problems."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#data-understanding-i",
    "href": "Classification/IntrotoDataScience.slides.html#data-understanding-i",
    "title": "Introduction to Data Science",
    "section": "Data Understanding I",
    "text": "Data Understanding I\n\n\nIf the goal is to solve a business problem, data constitutes the raw material available from which the solution will be built.\nThe available data rarely matches the problem.\nFor example, historical data is often collected for purposes unrelated to the current business problem or without any explicit purpose."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#data-understanding-ii",
    "href": "Classification/IntrotoDataScience.slides.html#data-understanding-ii",
    "title": "Introduction to Data Science",
    "section": "Data Understanding II",
    "text": "Data Understanding II\n\n\nData costs vary. Some data will be available for free, while others will require effort to obtain.\n\n\nA key part of the data understanding phase is estimating the costs and benefits of each data source and deciding whether further investment is justified.\nEven after acquiring all the data sets, compiling them may require additional effort."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#example",
    "href": "Classification/IntrotoDataScience.slides.html#example",
    "title": "Introduction to Data Science",
    "section": "Example",
    "text": "Example\n\nIn the 1980s, credit cards were essentially priced uniformly because companies didn’t have adequate information systems to deal with differential pricing on a massive scale.\n\nAround 1990, Richard Fairbanks and Nigel Morris realized that information technology was powerful enough to enable more sophisticated predictive models and offer different terms (today: pricing, credit limits, low introductory rate balance transfers, cash back, and loyalty points)."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#section-2",
    "href": "Classification/IntrotoDataScience.slides.html#section-2",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Signet Bank’s management was convinced that modeling profitability, not just the probability of default, was the right strategy.\nThey knew that a small proportion of customers actually account for more than 100% of a bank’s profit from credit card transactions (because the rest are either breaking even or losing money).\nIf they could model profitability, they could make better offers to the best customers and “skim the cream” of the big banks’ clientele."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#section-3",
    "href": "Classification/IntrotoDataScience.slides.html#section-3",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "But Signet Bank had a really big problem implementing this strategy.\nThey didn’t have the right data to model profitability for offering different terms to different customers!\nSince the bank offered credit with a specific set of terms and a specific default model, they had the data to model profitability (1) for the terms they actually offered in the past, and (2) for the type of customer actually offered credit."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#section-4",
    "href": "Classification/IntrotoDataScience.slides.html#section-4",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "What could Signet Bank do? They put into play a fundamental data science strategy: acquire the necessary data at a cost!\nIn this case, data on customer profitability with different credit terms could be generated by conducting experiments. Different terms were randomly offered to different customers.\nThis might seem silly outside the context of data analytics thinking: you’re likely to lose money!\nThis is true. In this case, the losses are the cost of data acquisition."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#what-happened",
    "href": "Classification/IntrotoDataScience.slides.html#what-happened",
    "title": "Introduction to Data Science",
    "section": "What happened?",
    "text": "What happened?\nAs expected, Signet’s number of bad accounts skyrocketed.\nThe losses continued for several years while data scientists worked to build predictive models from the data, evaluate them, and implement them to improve profits.\nBecause the company viewed these losses as investments in data, they persisted despite complaints from stakeholders.\nEventually, Signet’s credit card business turned around and became so profitable that it was spun off to separate it from the bank’s other operations, which were now overshadowing the success of its consumer lending business."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#richard-fairbanks-and-nigel-morris",
    "href": "Classification/IntrotoDataScience.slides.html#richard-fairbanks-and-nigel-morris",
    "title": "Introduction to Data Science",
    "section": "Richard Fairbanks and Nigel Morris",
    "text": "Richard Fairbanks and Nigel Morris\n\n\n\n\n\n\n\nFounders of"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#most-used-data-science-tools",
    "href": "Classification/IntrotoDataScience.slides.html#most-used-data-science-tools",
    "title": "Introduction to Data Science",
    "section": "Most Used Data Science Tools",
    "text": "Most Used Data Science Tools\n\nPython\nR\nSAS\nExcel\nPower BI\nTableau\nApache Spark\n\nhttps://hackr.io/blog/top-data-analytics-tools"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#other-tools-used",
    "href": "Classification/IntrotoDataScience.slides.html#other-tools-used",
    "title": "Introduction to Data Science",
    "section": "Other Tools Used",
    "text": "Other Tools Used\n\nRapidMiner (https://rapidminer.com/products/studio/)\nJMP (https://www.jmp.com/es_mx/home.html)\nMinitab (https://www.minitab.com/es-mx/products/minitab/)\nTrifacta (https://www.trifacta.com/)\nBigML (https://bigml.com/)\nMLBase (http://www.mlbase.org/)\nGoogle Cloud AutoML (https://cloud.google.com/automl/)"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#terminology",
    "href": "Classification/IntrotoDataScience.slides.html#terminology",
    "title": "Introduction to Data Science",
    "section": "Terminology",
    "text": "Terminology\n\nPredictors. They are represented using the notation \\(X_1\\) for the first predictor, \\(X_p\\) for the second predictor, …, and \\(X_p\\) for the p-th predictor.\nResponse. \\(Y\\) represents the response variable, which we will attempt to predict.\n\n\nWe want to establish the following relationship\n\\[\nY = f(X_1, X_2, \\ldots, X_p) + \\epsilon,\n\\]\nwhere \\(f\\) is a function of the predictors and \\(\\epsilon\\) is a natural (random) error."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#types-of-learning",
    "href": "Classification/IntrotoDataScience.slides.html#types-of-learning",
    "title": "Introduction to Data Science",
    "section": "Types of Learning",
    "text": "Types of Learning\n\nIn data science (and machine learning), there are two main types of learning:\n\nSupervised learning\nUnsupervised learning"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#supervised-learning",
    "href": "Classification/IntrotoDataScience.slides.html#supervised-learning",
    "title": "Introduction to Data Science",
    "section": "Supervised Learning…",
    "text": "Supervised Learning…\nIncludes algorithms that learn by example. The user provides the supervised algorithm with a known data set that includes the corresponding known inputs and outputs. The algorithm must find a method to determine how to reach those inputs and outputs.\nWhile the user knows the correct answers to the problem, the algorithm identifies patterns in the data, learns from observations, and makes predictions.\nThe algorithm makes predictions that can be corrected by the user, and this process continues until the algorithm reaches a high level of accuracy and performance."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#popular-supervised-algorithms",
    "href": "Classification/IntrotoDataScience.slides.html#popular-supervised-algorithms",
    "title": "Introduction to Data Science",
    "section": "Popular Supervised Algorithms",
    "text": "Popular Supervised Algorithms"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#unsupervised-learning",
    "href": "Classification/IntrotoDataScience.slides.html#unsupervised-learning",
    "title": "Introduction to Data Science",
    "section": "Unsupervised Learning…",
    "text": "Unsupervised Learning…\n\nstudies data to identify patterns. There is no answer key or human operator to provide instruction. The machine determines correlations and relationships by analyzing the available data.\nIn this process, the unsupervised algorithm is left to interpret large data sets. The algorithm attempts to organize that data in some way to describe its structure.\nAs it evaluates more data, its ability to make decisions about it gradually improves and becomes more refined."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#popular-unsupervised-algorithms",
    "href": "Classification/IntrotoDataScience.slides.html#popular-unsupervised-algorithms",
    "title": "Introduction to Data Science",
    "section": "Popular Unsupervised Algorithms",
    "text": "Popular Unsupervised Algorithms"
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#two-data-sets",
    "href": "Classification/IntrotoDataScience.slides.html#two-data-sets",
    "title": "Introduction to Data Science",
    "section": "Two Data Sets",
    "text": "Two Data Sets\n\nIn supervised learning, there are several types of data.\nTraining data is the data used to construct \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is the data that was NOT used in the fitting process, but is used to test the model’s performance on unanalyzed data."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#yogi-berra",
    "href": "Classification/IntrotoDataScience.slides.html#yogi-berra",
    "title": "Introduction to Data Science",
    "section": "Yogi Berra",
    "text": "Yogi Berra\n\n\nIt’s though to make predictions, especially about the future."
  },
  {
    "objectID": "Classification/IntrotoDataScience.slides.html#lets-play",
    "href": "Classification/IntrotoDataScience.slides.html#lets-play",
    "title": "Introduction to Data Science",
    "section": "Let’s Play",
    "text": "Let’s Play\n\nLet’s play with supervised models.\n\nhttps://quickdraw.withgoogle.com/\nhttps://tenso.rs/demos/rock-paper-scissors/\nhttps://teachablemachine.withgoogle.com/"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#load-the-libraries",
    "href": "Classification/NeuralNetworks.slides.html#load-the-libraries",
    "title": "K-nearest neighbors",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#k-nearest-neighbors-knn",
    "href": "Classification/NeuralNetworks.slides.html#k-nearest-neighbors-knn",
    "title": "K-nearest neighbors",
    "section": "K-nearest neighbors (KNN)",
    "text": "K-nearest neighbors (KNN)\nKNN a supervised learning algorithm that uses proximity to make classifications or predictions about the clustering of a single data point.\n\nBasic idea: Predict a new observation using the K closest observations in the training dataset.\n\nTo predict the response for a new observation, KNN uses the K nearest neighbors (observations) in terms of the predictors!\nThe predicted response for the new observation is the most common response among the K nearest neighbors."
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#the-algorithm-has-3-steps",
    "href": "Classification/NeuralNetworks.slides.html#the-algorithm-has-3-steps",
    "title": "K-nearest neighbors",
    "section": "The algorithm has 3 steps:",
    "text": "The algorithm has 3 steps:\n\n\nChoose the number of nearest neighbors (K).\nFor a new observation, find the K closest observations in the training data (ignoring the response).\nFor the new observation, the algorithm predicts the value of the most common response among the K nearest observations."
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#nearest-neighbour",
    "href": "Classification/NeuralNetworks.slides.html#nearest-neighbour",
    "title": "K-nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#nearest-neighbour-1",
    "href": "Classification/NeuralNetworks.slides.html#nearest-neighbour-1",
    "title": "K-nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#nearest-neighbour-2",
    "href": "Classification/NeuralNetworks.slides.html#nearest-neighbour-2",
    "title": "K-nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#nearest-neighbour-3",
    "href": "Classification/NeuralNetworks.slides.html#nearest-neighbour-3",
    "title": "K-nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#banknote-data",
    "href": "Classification/NeuralNetworks.slides.html#banknote-data",
    "title": "K-nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 2 votes for “genuine” and 2 for “fake.” So we classify it as “genius.”"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#banknote-data-1",
    "href": "Classification/NeuralNetworks.slides.html#banknote-data-1",
    "title": "K-nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 2 votes for “genuine” and 2 for “fake.” So we classify it as “genius.”"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#banknote-data-2",
    "href": "Classification/NeuralNetworks.slides.html#banknote-data-2",
    "title": "K-nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 3 votes for “counterfeit” and 0 for “genuine.” So we classify it as “counterfeit.”"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#banknote-data-3",
    "href": "Classification/NeuralNetworks.slides.html#banknote-data-3",
    "title": "K-nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 3 votes for “counterfeit” and 0 for “genuine.” So we classify it as “counterfeit.”\nCloseness is based on Euclidean distance."
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#implementation-details",
    "href": "Classification/NeuralNetworks.slides.html#implementation-details",
    "title": "K-nearest neighbors",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nTies\n\nIf there are more than K nearest neighbors, include them all.\nIf there is a tie in the vote, set a rule to break the tie. For example, randomly select the class."
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#section",
    "href": "Classification/NeuralNetworks.slides.html#section",
    "title": "K-nearest neighbors",
    "section": "",
    "text": "KNN uses the Euclidean distance between points. So it ignores units.\n\nExample: two predictors: height in cm and arm span in feet. Compare two people: (152.4, 1.52) and (182.88, 1.85).\nThese people are separated by 30.48 units of distance in the first variable, but only by 0.33 units in the second.\nTherefore, the first predictor plays a much more important role in classification and can bias the results to the point where the second variable becomes useless.\n\n\nTherefore, as a first step, we must transform the predictors so that they have the same units!"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#example",
    "href": "Classification/NeuralNetworks.slides.html#example",
    "title": "K-nearest neighbors",
    "section": "Example",
    "text": "Example\nThe data is located in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#create-the-predictor-matrix-and-response-column",
    "href": "Classification/NeuralNetworks.slides.html#create-the-predictor-matrix-and-response-column",
    "title": "K-nearest neighbors",
    "section": "Create the predictor matrix and response column",
    "text": "Create the predictor matrix and response column\nLet’s create the predictor matrix or response column\n\n# Set full matrix of predictors.\nX_full = bank_data.drop(columns = ['Status']) \n\n# Vector with responses\nY_full = bank_data.filter(['Status'])\n\nTo set the target category in the response we use the get_dummies() function.\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_full, dtype = 'int')\n\n# Select target variable.\nY_target_full = Y_dummies['Status_counterfeit']"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#lets-partition-the-dataset",
    "href": "Classification/NeuralNetworks.slides.html#lets-partition-the-dataset",
    "title": "K-nearest neighbors",
    "section": "Let’s partition the dataset",
    "text": "Let’s partition the dataset\n\nWe use 70% for training and the rest for validation.\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n                                                      test_size = 0.3)"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#standardization-in-python",
    "href": "Classification/NeuralNetworks.slides.html#standardization-in-python",
    "title": "K-nearest neighbors",
    "section": "Standardization in Python",
    "text": "Standardization in Python\n\nTo standardize numeric predictors, we use the StandardScaler() function. We also apply the function to variables using the fit_transform() function.\n\n\nscaler = StandardScaler()\nXs_train = scaler.fit_transform(X_train)"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#knn-in-python",
    "href": "Classification/NeuralNetworks.slides.html#knn-in-python",
    "title": "K-nearest neighbors",
    "section": "KNN in Python",
    "text": "KNN in Python\n\nIn Python, we can use the KNeighborsClassifier() and fit() from scikit-learn to train a KNN.\nIn the KNeighborsClassifier function, we can define the number of nearest neighbors using the n_neighbors parameter.\n\n# For example, let's use KNN with three neighbours\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Now, we train the algorithm.\nknn.fit(Xs_train, Y_train)"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#evaluation",
    "href": "Classification/NeuralNetworks.slides.html#evaluation",
    "title": "K-nearest neighbors",
    "section": "Evaluation",
    "text": "Evaluation\n\nTo evaluate KNN, we make predictions on the validation data (not used to train the KNN). To do this, we must first perform standardization operations on the predictors in the validation dataset.\n\nXs_valid = scaler.fit_transform(X_valid)\n\n\nNext, we make predictions.\n\nY_pred_knn = knn.predict(Xs_valid)"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#confusion-matrix",
    "href": "Classification/NeuralNetworks.slides.html#confusion-matrix",
    "title": "K-nearest neighbors",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\n# Calcular matriz de confusión.\ncm = confusion_matrix(Y_valid, Y_pred_knn)\n\n# Mostrar matriz de confusión.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#finding-the-best-value-of-k",
    "href": "Classification/NeuralNetworks.slides.html#finding-the-best-value-of-k",
    "title": "K-nearest neighbors",
    "section": "Finding the best value of K",
    "text": "Finding the best value of K\nWe can determine the best value of K for the KNN algorithm. To this end, we evaluate the performance of the KNN for different values of \\(K\\) in terms of accuracy on the validation dataset.\n\nbest_k = 1\nbest_accuracy = 0\nk_values = range(1, 50)  # Test k values from 1 to 50\nvalidation_accuracies = []\n\nfor k in k_values:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(Xs_train, Y_train)\n    val_accuracy = accuracy_score(Y_valid, model.predict(Xs_valid))\n    validation_accuracies.append(val_accuracy)\n\n    if val_accuracy &gt; best_accuracy:\n        best_accuracy = val_accuracy\n        best_k = k"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#visualize",
    "href": "Classification/NeuralNetworks.slides.html#visualize",
    "title": "K-nearest neighbors",
    "section": "Visualize",
    "text": "Visualize\nWe can then visualize the accuracy for different values of \\(K\\) using the following graph and code.\n\n\nCode\nplt.figure(figsize=(6.3, 4.3))\nplt.plot(k_values, validation_accuracies, marker=\"o\", linestyle=\"-\")\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Validation Accuracy\")\nplt.title(\"Choosing the Best k for KNN\")\nplt.show()"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#section-1",
    "href": "Classification/NeuralNetworks.slides.html#section-1",
    "title": "K-nearest neighbors",
    "section": "",
    "text": "Finally, we select the best number of nearest neighbors contained in the best_k object.\n\nKNN_final = KNeighborsClassifier(n_neighbors = best_k)\nKNN_final.fit(Xs_train, Y_train)\n\n\nThe accuracy of the best KNN is\n\nY_pred_KNNfinal = KNN_final.predict(Xs_valid)\nvalid_accuracy = accuracy_score(Y_valid, Y_pred_KNNfinal)\nprint(valid_accuracy)\n\n0.9833333333333333"
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#discussion",
    "href": "Classification/NeuralNetworks.slides.html#discussion",
    "title": "K-nearest neighbors",
    "section": "Discussion",
    "text": "Discussion\n\nKNN is intuitive and simple and can produce decent predictions. However, KNN has some disadvantages:\n\nWhen the training dataset is very large, KNN is computationally expensive. This is because, to predict an observation, we need to calculate the distance between that observation and all the others in the dataset. (“Lazy learner”).\nIn this case, a decision tree is more advantageous because it is easy to build, store, and make predictions with."
  },
  {
    "objectID": "Classification/NeuralNetworks.slides.html#section-2",
    "href": "Classification/NeuralNetworks.slides.html#section-2",
    "title": "K-nearest neighbors",
    "section": "",
    "text": "The predictive performance of KNN deteriorates as the number of predictors increases.\nThis is because the expected distance to the nearest neighbor increases dramatically with the number of predictors, unless the size of the dataset increases exponentially with this number.\nThis is known as the curse of dimensionality.\n\n\n\n\nhttps://aiaspirant.com/curse-of-dimensionality/"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#agenda",
    "href": "LinearRegression/AdditionalTopics.slides.html#agenda",
    "title": "Additional Topics",
    "section": "Agenda",
    "text": "Agenda\n\n\nLinear Models with Categorical Predictors\nAnother Example"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#load-the-libraries",
    "href": "LinearRegression/AdditionalTopics.slides.html#load-the-libraries",
    "title": "Additional Topics",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nLet’s import scikit-learn into Python together with the other relevant libraries.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import root_mean_squared_error\n\nWe will not use all the functions from the scikit-learn library. Instead, we will use specific functions from the sub-libraries preprocessing, model_selection, and metrics."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#categorical-predictors",
    "href": "LinearRegression/AdditionalTopics.slides.html#categorical-predictors",
    "title": "Additional Topics",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nA categorical predictor takes on values that are categories, say, names or labels.\nTheir use in regression requires dummy variables, which are quantitative variables.\nWhen a categorical predictor has more than two levels, a single dummy variable cannot represent all possible categories.\nIn general, a categorical predictor with \\(k\\) categories requires \\(k-1\\) dummy variables."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#dummy-coding",
    "href": "LinearRegression/AdditionalTopics.slides.html#dummy-coding",
    "title": "Additional Topics",
    "section": "Dummy coding",
    "text": "Dummy coding\n\nTraditionally, dummy variables are binary variables which can only take the values 0 and 1.\nThis approach implies a reference category. Specifically, the category that results when all dummy variables equal 0.\nThis coding impacts the interpretation of the model coefficients:\n\n\\(\\beta_0\\) is the mean response under the reference category.\n\\(\\beta_j\\) is the amount of increase in the mean response when we change from the reference category to another category."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#example-1",
    "href": "LinearRegression/AdditionalTopics.slides.html#example-1",
    "title": "Additional Topics",
    "section": "Example 1",
    "text": "Example 1\n\nThe auto data set includes a categorical variable “Origin” which shows the origin of each car.\n\n# Load the Excel file into a pandas DataFrame.\nauto_data = pd.read_excel(\"auto.xlsx\")\n\n# Set categorical variables.\nauto_data['origin'] = pd.Categorical(auto_data['origin'])\n\nIn this section, we will consider the auto_data as our training dataset."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#dataset",
    "href": "LinearRegression/AdditionalTopics.slides.html#dataset",
    "title": "Additional Topics",
    "section": "Dataset",
    "text": "Dataset\n\n(auto_data\n.filter(['mpg', 'origin'])\n).head(6)\n\n\n\n\n\n\n\n\nmpg\norigin\n\n\n\n\n0\n18.0\nAmerican\n\n\n1\n15.0\nAmerican\n\n\n2\n18.0\nAmerican\n\n\n3\n16.0\nAmerican\n\n\n4\n17.0\nAmerican\n\n\n5\n15.0\nAmerican"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#dummy-variables",
    "href": "LinearRegression/AdditionalTopics.slides.html#dummy-variables",
    "title": "Additional Topics",
    "section": "Dummy variables",
    "text": "Dummy variables\n\nOrigin has 3 categories: European, American, Japanese.\nSo, 2 dummy variables are required:\n\\[d_1 =\n\\begin{cases}\n1 \\text{ if car is European}\\\\\n0 \\text{ if car is not European}\n\\end{cases} \\text{ and }\\]\n\\[d_2 =\n\\begin{cases}\n1 \\text{ if car is Japanese}\\\\\n0 \\text{ if car is not Japanese}\n\\end{cases}.\\]\n“American” acts as the reference category."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#section",
    "href": "LinearRegression/AdditionalTopics.slides.html#section",
    "title": "Additional Topics",
    "section": "",
    "text": "The dataset with the dummy variables looks like this:\n\n\n\norigin\n\\(d_1\\)\n\\(d_2\\)\n\n\n\n\nAmerican\n0\n0\n\n\nAmerican\n0\n0\n\n\nEuropean\n1\n0\n\n\nEuropean\n1\n0\n\n\nAmerican\n0\n0\n\n\nJapanese\n0\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#linear-regression-model",
    "href": "LinearRegression/AdditionalTopics.slides.html#linear-regression-model",
    "title": "Additional Topics",
    "section": "Linear regression model",
    "text": "Linear regression model\n\n\\[\\hat{Y}_i= \\hat{\\beta}_0 + \\hat{\\beta}_1 d_{1i} + \\hat{\\beta}_2 d_{2i}  \\ \\text{for} \\ i=1,\\ldots,n.\\]\n\n\\(\\hat{Y}_i\\) is the i-th predicted response.\n\\(d_{1i}\\) is 1 if the i-th observation is from a European car, and 0 otherwise.\n\\(d_{2i}\\) is 1 if the i-th observation is from a Japanese car, and 0 otherwise."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#model-coefficients",
    "href": "LinearRegression/AdditionalTopics.slides.html#model-coefficients",
    "title": "Additional Topics",
    "section": "Model coefficients",
    "text": "Model coefficients\n\n\\[\n\\hat{Y}_i= \\hat{\\beta}_0 + \\hat{\\beta}_1 d_{1i} + \\hat{\\beta}_2 d_{2i}\n\\]\n\n\\(\\hat{\\beta}_0\\) is the mean response (mpg) for American cars.\n\\(\\hat{\\beta}_1\\) is the amount of increase in the mean response when changing from an American to a European car.\n\\(\\hat{\\beta}_2\\) is the amount of increase in the mean response when changing from an American to a Japanese car."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#section-1",
    "href": "LinearRegression/AdditionalTopics.slides.html#section-1",
    "title": "Additional Topics",
    "section": "",
    "text": "Alternatively, we can write the regression model as:\n\\[\n\\hat{Y}_i= \\hat{\\beta}_0+\\hat{\\beta}_1 d_{1i} +\\hat{\\beta}_2 d_{2i} = \\begin{cases}\n\\hat{\\beta}_0+\\hat{\\beta}_1 +\\hat{\\beta}_i \\text{ if car is European}\\\\\n\\hat{\\beta}_0+\\hat{\\beta}_2  \\text{ if car is Japanese} \\\\\n\\hat{\\beta}_0 \\;\\;\\;\\;\\;\\;\\ \\text{ if car is American}\n\\end{cases}\n\\]\nGiven this model representation:\n\n\n\\(\\hat{\\beta}_0\\) is the mean mpg for American cars,\n\\(\\hat{\\beta}_1\\) is the difference in the mean mpg between European and American cars, and\n\\(\\hat{\\beta}_2\\) is the difference in the mean mpg between Japanese and American cars."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#in-python",
    "href": "LinearRegression/AdditionalTopics.slides.html#in-python",
    "title": "Additional Topics",
    "section": "In Python",
    "text": "In Python\n\nWe follow three steps to fit a linear model with a categorical predictor. First, we compute the dummy variables.\n\n# Create linear regression object\ndummy_X_train = pd.get_dummies(auto_data['origin'], drop_first = True, \n                            dtype = 'int')\n\nNext, we construct the response column.\n\n# Create response vector\nY_train = auto_data['mpg']"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#section-2",
    "href": "LinearRegression/AdditionalTopics.slides.html#section-2",
    "title": "Additional Topics",
    "section": "",
    "text": "Finally, we fit the model using scikit-learn.\n\n# 1. Create the linear regression model\nLRmodel = LinearRegression()\n\n# 2. Fit the model.\nLRmodel.fit(dummy_X_train, Y_train)\n\n# 3. Show estimated coefficients.\nprint(\"Intercept:\", LRmodel.intercept_)\nprint(\"Coefficients:\", LRmodel.coef_)\n\nIntercept: 20.033469387755094\nCoefficients: [ 7.56947179 10.41716352]"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#analysis-of-covariance",
    "href": "LinearRegression/AdditionalTopics.slides.html#analysis-of-covariance",
    "title": "Additional Topics",
    "section": "Analysis of covariance",
    "text": "Analysis of covariance\n\nModels that mix categorical and numerical predictors are sometimes referred to as analysis of covariance (ANCOVA) models.\nExample (cont): Consider the predictor weight (\\(X\\)).\n\\[\\hat{Y}_i= \\hat{\\beta}_0 + \\hat{\\beta}_1 d_{1i} + \\hat{\\beta}_2 d_{2i} + \\hat{\\beta}_3 X_{i},\\]\nwhere \\(X_i\\) denotes the i-th observed value of weight and \\(\\hat{\\beta}_3\\) is the coefficient of this predictor."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#ancova-model",
    "href": "LinearRegression/AdditionalTopics.slides.html#ancova-model",
    "title": "Additional Topics",
    "section": "ANCOVA model",
    "text": "ANCOVA model\nThe components of the ANCOVA model are individual functions of the coefficients.\nTo gain insight into the model, we write it as follows:\n\\[\n\\begin{align}\n\\hat{Y}_i &= \\hat{\\beta}_0+\\hat{\\beta}_1 d_{1i} +\\hat{\\beta}_2 d_{2i} + \\hat{\\beta}_3 X_{i} \\\\ &= \\begin{cases}\n(\\hat{\\beta}_0+\\hat{\\beta}_1)  + \\hat{\\beta}_3 X_{i} & \\text{ if car is European} \\\\\n(\\hat{\\beta}_0+\\hat{\\beta}_2) + \\hat{\\beta}_3 X_{i} & \\text{ if car is Japanese} \\\\\n\\hat{\\beta}_0 + \\hat{\\beta}_3 X_{i} \\;\\;\\;\\;\\;\\;\\;\\;\\ & \\text{ if car is American} \\\\\n\\end{cases}.\n\\end{align}\n\\]\nThe models have different intercepts but the same slope."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#section-3",
    "href": "LinearRegression/AdditionalTopics.slides.html#section-3",
    "title": "Additional Topics",
    "section": "",
    "text": "To estimate \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\) and \\(\\hat{\\beta}_3\\), we use least squares.\nWe could make individual inferences on \\(\\beta_1\\) and \\(\\beta_2\\) using t-tests in the statsmodels library.\nHowever, better tests are possible such as overall and partial F-tests (not discussed here)."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#in-python-1",
    "href": "LinearRegression/AdditionalTopics.slides.html#in-python-1",
    "title": "Additional Topics",
    "section": "In Python",
    "text": "In Python\n\nTo fit an ANCOVA model, we use similar steps as before. The only extra step is to concatenate the data with the dummy variables and the numerical predictor using the function concat() from pandas.\n\n# Concatenate the two data sets.\nX_train = pd.concat([dummy_X_train, auto_data['weight']], axis = 1)\n\n# 1. Create the linear regression model\nLRmodel_ancova = LinearRegression()\n\n# 2. Fit the model.\nLRmodel_ancova.fit(X_train, Y_train)"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#section-4",
    "href": "LinearRegression/AdditionalTopics.slides.html#section-4",
    "title": "Additional Topics",
    "section": "",
    "text": "The estimated model is shown below.\n\n# 3. Show estimated coefficients.\nprint(\"Intercept:\", LRmodel_ancova.intercept_)\nprint(\"Coefficients:\", LRmodel_ancova.coef_)\n\nIntercept: 43.732236151632975\nCoefficients: [ 0.97090559  2.3271499  -0.00702708]"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#example-2",
    "href": "LinearRegression/AdditionalTopics.slides.html#example-2",
    "title": "Additional Topics",
    "section": "Example 2",
    "text": "Example 2\n\nThe “BostonHousing.xlsx” contains data collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes data on 506 census housing tracts in the Boston area in 1970s.\nThe goal is to predict the median house price in new tracts based on information such as crime rate, pollution, and number of rooms.\nThe response is the median value of owner-occupied homes in $1000s, contained in the column MEDV."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#the-predictors",
    "href": "LinearRegression/AdditionalTopics.slides.html#the-predictors",
    "title": "Additional Topics",
    "section": "The predictors",
    "text": "The predictors\n\n\nCRIM: per capita crime rate by town.\nZN: proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS: proportion of non-retail business acres per town.\nCHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\nNOX: nitrogen oxides concentration (parts per 10 million).\nRM: average number of rooms per dwelling.\nAGE: proportion of owner-occupied units built prior to 1940.\nDIS: weighted mean of distances to five Boston employment centers\nRAD: index of accessibility to radial highways.\nTAX: full-value property-tax rate per $10,000.\nPTRATIO: pupil-teacher ratio by town.\nLSTAT: lower status of the population (percent)."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#read-the-dataset",
    "href": "LinearRegression/AdditionalTopics.slides.html#read-the-dataset",
    "title": "Additional Topics",
    "section": "Read the dataset",
    "text": "Read the dataset\nWe read the dataset and set the variable CHAS as categorical.\n\n# Load Excel file (make sure the file is in your Colab)\nBoston_data = pd.read_excel('BostonHousing.xlsx')\n\n# Drop the categorical variable.\nBoston_data['CHAS'] = pd.Categorical(Boston_data['CHAS'])\nBoston_data['RAD'] = pd.Categorical(Boston_data['RAD'])\n\n# Preview the dataset.\nBoston_data.head(3)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\nNo\n0.538\n6.575\n65.2\n4.0900\nLow\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\nNo\n0.469\n6.421\n78.9\n4.9671\nLow\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\nNo\n0.469\n7.185\n61.1\n4.9671\nLow\n242\n17.8\n4.03\n34.7"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#train-and-validation-data",
    "href": "LinearRegression/AdditionalTopics.slides.html#train-and-validation-data",
    "title": "Additional Topics",
    "section": "Train and validation data",
    "text": "Train and validation data\nWe split the dataset into a training and a validation dataset using the function train_test_split() from scikit-learn.\n\n# Set full matrix of predictors.\nX_full = Boston_data.drop(columns = ['MEDV']) \n\n# Set full matrix of responses.\nY_full = Boston_data['MEDV']\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n                                                      test_size=0.3,\n                                                      random_state = 59227)\n\nThe parameter test_size sets the portion of the dataset that will go to the validation set. It is usually 0.3 or 0.2."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#transform-the-categorical-predictors",
    "href": "LinearRegression/AdditionalTopics.slides.html#transform-the-categorical-predictors",
    "title": "Additional Topics",
    "section": "Transform the categorical predictors",
    "text": "Transform the categorical predictors\n\nBefore we continue, let’s turn the categorical predictors into dummy variables\n\n# Transform categorical predictors into dummy variables\nBoston_X_train = pd.get_dummies(X_train, drop_first = True, dtype = 'int')\nBoston_X_train.head(3)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nNOX\nRM\nAGE\nDIS\nTAX\nPTRATIO\nLSTAT\nCHAS_Yes\nRAD_Low\nRAD_Medium\n\n\n\n\n452\n5.09017\n0.0\n18.10\n0.713\n6.297\n91.8\n2.3682\n666\n20.2\n17.27\n0\n0\n0\n\n\n317\n0.24522\n0.0\n9.90\n0.544\n5.782\n71.7\n4.0317\n304\n18.4\n15.94\n0\n0\n1\n\n\n352\n0.07244\n60.0\n1.69\n0.411\n5.884\n18.5\n10.7103\n411\n18.3\n7.79\n0\n0\n1"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#fit-a-model-using-training-data",
    "href": "LinearRegression/AdditionalTopics.slides.html#fit-a-model-using-training-data",
    "title": "Additional Topics",
    "section": "Fit a model using training data",
    "text": "Fit a model using training data\n\nWe train a linear regression model to predict the MEDV in terms of the 12 predictors using functions from scikit-learn.\n\n# 1. Create the linear regression model\nLRmodel = LinearRegression()\n\n# 2. Fit the model.\nLRmodel.fit(Boston_X_train, Y_train)\n\n# 3. Show estimated coefficients.\nprint(\"Intercept:\", LRmodel.intercept_)\nprint(\"Coefficients:\", LRmodel.coef_)\n\nIntercept: 37.516763561108434\nCoefficients: [-5.31702151e-02  4.42408175e-02  8.46402739e-02 -1.19782458e+01\n  3.82114415e+00  1.77709868e-02 -1.25752076e+00 -7.33041781e-03\n -9.14794029e-01 -7.17544266e-01  5.25221827e+00 -2.04793579e+00\n -2.47739812e+00]"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#brief-residual-analysis",
    "href": "LinearRegression/AdditionalTopics.slides.html#brief-residual-analysis",
    "title": "Additional Topics",
    "section": "Brief Residual Analysis",
    "text": "Brief Residual Analysis\nWe evaluate the model using a “Residual versus Fitted Values” plot. The plot does not show concerning patterns in the residuals. So, we assume that the model satisfies the assumption of constant variance."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#validation-mean-squared-error",
    "href": "LinearRegression/AdditionalTopics.slides.html#validation-mean-squared-error",
    "title": "Additional Topics",
    "section": "Validation Mean Squared Error",
    "text": "Validation Mean Squared Error\nWhen the response is numeric, the most common evaluation metric is the validation Root Mean Squared Error (RMSE):\n\\[\n\\left(\\frac{1}{n_{v}} \\sum_{i=1}^{n_{v}} \\left( Y_i - \\hat{Y}_i \\right)^2 \\right)^{1/2}\n\\]\nwhere \\((Y_1, \\boldsymbol{X}_1), \\ldots, (Y_{n_{v} }, \\boldsymbol{X}_{n_{v}} )\\) are the \\(n_{v}\\) observations in the validation dataset, and \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\hat{\\beta}_2 X_{i2} + \\cdots + \\hat{\\beta}_p X_{ip}\\) is the model prediction of the i-th response."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#recall-that",
    "href": "LinearRegression/AdditionalTopics.slides.html#recall-that",
    "title": "Additional Topics",
    "section": "Recall that …",
    "text": "Recall that …\n\n\nThe RMSE is in the same units as the response.\nThe RMSE value is interpreted as either how far (on average) the residuals are from zero.\nIt can also be interpreted as the average distance between the observed response values and the model predictions."
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#in-python-2",
    "href": "LinearRegression/AdditionalTopics.slides.html#in-python-2",
    "title": "Additional Topics",
    "section": "In Python",
    "text": "In Python\nWe first compute the predictions of our model on the validation dataset. To this end, we create the dummy variables of the categorical variables using the validation dataset.\n\nBoston_X_val = pd.get_dummies(X_valid, drop_first = True, dtype = 'int')\nBoston_X_val.head(3)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nNOX\nRM\nAGE\nDIS\nTAX\nPTRATIO\nLSTAT\nCHAS_Yes\nRAD_Low\nRAD_Medium\n\n\n\n\n482\n5.73116\n0.0\n18.1\n0.532\n7.061\n77.0\n3.4106\n666\n20.2\n7.01\n0\n0\n0\n\n\n367\n13.52220\n0.0\n18.1\n0.631\n3.863\n100.0\n1.5106\n666\n20.2\n13.33\n0\n0\n0\n\n\n454\n9.51363\n0.0\n18.1\n0.713\n6.728\n94.1\n2.4961\n666\n20.2\n18.71\n0\n0\n0"
  },
  {
    "objectID": "LinearRegression/AdditionalTopics.slides.html#section-5",
    "href": "LinearRegression/AdditionalTopics.slides.html#section-5",
    "title": "Additional Topics",
    "section": "",
    "text": "Now, we use the values of the predictors in this dataset and use it as input to our model. Our model then computes the prediction of the response for each combination of values of the predictors.\n\n# Predict responses using validation data.\npredicted_medv_val = LRmodel.predict(Boston_X_val)\n\nWe compute the validation RMSE using scikit-learn.\n\nrmse = root_mean_squared_error(Y_valid, predicted_medv_val)\nprint( round(rmse, 3) )\n\n6.02\n\n\nThe lower the validation RMSE, the more accurate our model.\nInterpretation: On average, our predictions are off by \\(\\pm\\) 5,981 dollars."
  },
  {
    "objectID": "Tools/Tools3.slides.html#agenda",
    "href": "Tools/Tools3.slides.html#agenda",
    "title": "Data Types and Visualization",
    "section": "Agenda",
    "text": "Agenda\n\n\nReview of data types and summary statistics\nData visualizations"
  },
  {
    "objectID": "Tools/Tools3.slides.html#types-of-data-i",
    "href": "Tools/Tools3.slides.html#types-of-data-i",
    "title": "Data Types and Visualization",
    "section": "Types of data I",
    "text": "Types of data I\n\nWhen a numerical quantity designating how much or how many is assigned to each item in the sample, the resulting set of values is numerical or quantitative.\n\nHeight (in ft).\nWeight (in lbs).\nAge (in years)."
  },
  {
    "objectID": "Tools/Tools3.slides.html#types-of-data-ii",
    "href": "Tools/Tools3.slides.html#types-of-data-ii",
    "title": "Data Types and Visualization",
    "section": "Types of data II",
    "text": "Types of data II\n\nWhen sample items are placed into categories and category names are assigned to the sample items, the data are categorical or qualitative.\n\nHair color.\nCountry of origin.\nZIP code."
  },
  {
    "objectID": "Tools/Tools3.slides.html#data-types",
    "href": "Tools/Tools3.slides.html#data-types",
    "title": "Data Types and Visualization",
    "section": "Data types",
    "text": "Data types"
  },
  {
    "objectID": "Tools/Tools3.slides.html#example-1",
    "href": "Tools/Tools3.slides.html#example-1",
    "title": "Data Types and Visualization",
    "section": "Example 1",
    "text": "Example 1\nLet’s load the data in “penguins.xlsx”.\n\n# Load pandas.\nimport pandas as pd\n\n# Load the Excel file into a pandas DataFrame.\npenguins_data = pd.read_excel(\"penguins.xlsx\")\n\n# Print the first 3 rows of the dataset.\npenguins_data.head(3)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007"
  },
  {
    "objectID": "Tools/Tools3.slides.html#section",
    "href": "Tools/Tools3.slides.html#section",
    "title": "Data Types and Visualization",
    "section": "",
    "text": "In Python, we check the type of each variable in a dataset using the function info().\n\npenguins_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB"
  },
  {
    "objectID": "Tools/Tools3.slides.html#general-python-formats",
    "href": "Tools/Tools3.slides.html#general-python-formats",
    "title": "Data Types and Visualization",
    "section": "General Python formats",
    "text": "General Python formats\n\n\nfloat64 format for numerical variables with decimals.\nint64 format for numerical variables with integers.\nobject format for general variables with characters."
  },
  {
    "objectID": "Tools/Tools3.slides.html#define-categorical-variables",
    "href": "Tools/Tools3.slides.html#define-categorical-variables",
    "title": "Data Types and Visualization",
    "section": "Define categorical variables",
    "text": "Define categorical variables\n\nTechnically, the variable sex in penguins_data is categorical. To explicitly tell this to Python, we use the following code.\n\npenguins_data['sex'] = pd.Categorical(penguins_data['sex'])\n\nSetting sex to categorical allows us to use effective visualization for this data.\nWe do the same for the other categorical variables species and island.\n\npenguins_data['species'] = pd.Categorical(penguins_data['species'])\npenguins_data['island'] = pd.Categorical(penguins_data['island'])"
  },
  {
    "objectID": "Tools/Tools3.slides.html#section-1",
    "href": "Tools/Tools3.slides.html#section-1",
    "title": "Data Types and Visualization",
    "section": "",
    "text": "Let’s check the type of variables again.\n\npenguins_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   species            344 non-null    category\n 1   island             344 non-null    category\n 2   bill_length_mm     342 non-null    float64 \n 3   bill_depth_mm      342 non-null    float64 \n 4   flipper_length_mm  342 non-null    float64 \n 5   body_mass_g        342 non-null    float64 \n 6   sex                333 non-null    category\n 7   year               344 non-null    int64   \ndtypes: category(3), float64(4), int64(1)\nmemory usage: 14.9 KB"
  },
  {
    "objectID": "Tools/Tools3.slides.html#summary-statistics",
    "href": "Tools/Tools3.slides.html#summary-statistics",
    "title": "Data Types and Visualization",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nA sample is often a long list of numbers. To help make the important features of a sample stand out, we compute summary statistics.\nFor numerical data, the most popular summary statistics are:\n\nSample mean\nSample variance and sample standard deviation\nSample quartiles\nSample maximum and minimum"
  },
  {
    "objectID": "Tools/Tools3.slides.html#sample-mean",
    "href": "Tools/Tools3.slides.html#sample-mean",
    "title": "Data Types and Visualization",
    "section": "Sample mean",
    "text": "Sample mean\n\nLet \\(y_1, y_2, \\ldots, y_n\\) be an observed sample of size \\(n\\).\nThe sample mean is\n\\[\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i = \\frac{y_1 + y_2 + \\cdots + y_n}{n}.\\]\nThe sample mean gives an indication of the center of the data."
  },
  {
    "objectID": "Tools/Tools3.slides.html#in-python",
    "href": "Tools/Tools3.slides.html#in-python",
    "title": "Data Types and Visualization",
    "section": "In Python",
    "text": "In Python\n\nThe sample mean is calculated using the function .agg() with “mean”.\n\nbill_length_mean = (penguins_data\n                    .filter(['bill_length_mm'], axis = 1)\n                    .agg(\"mean\")\n                    )\nprint(bill_length_mean)\n\nbill_length_mm    43.92193\ndtype: float64\n\n\nWe use the function print to show the number. Otherwise, Python will show the computer type of value stored in bill_length_mean."
  },
  {
    "objectID": "Tools/Tools3.slides.html#section-2",
    "href": "Tools/Tools3.slides.html#section-2",
    "title": "Data Types and Visualization",
    "section": "",
    "text": "You can also round the result to, say, three decimals.\n\nprint( round(bill_length_mean, 3) )\n\nbill_length_mm    43.922\ndtype: float64"
  },
  {
    "objectID": "Tools/Tools3.slides.html#sample-variance",
    "href": "Tools/Tools3.slides.html#sample-variance",
    "title": "Data Types and Visualization",
    "section": "Sample variance",
    "text": "Sample variance\n\nLet \\(y_1, y_2, \\ldots, y_n\\) be an observed sample of size \\(n\\). The sample mean is\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\frac{(y_1 - \\bar{y})^2  + \\cdots + (y_n - \\bar{y})^2}{n-1}\n\\]\n\nThe sample variance is like an average of the squared differences between each observation and the sample mean.\nIt gives an indication of how spread out the data are."
  },
  {
    "objectID": "Tools/Tools3.slides.html#section-3",
    "href": "Tools/Tools3.slides.html#section-3",
    "title": "Data Types and Visualization",
    "section": "",
    "text": "In Python, the sample variance is calculated using the function agg() with “var”.\n\nbill_length_var = (penguins_data\n                    .filter(['bill_length_mm'], axis = 1)\n                    .agg(\"var\")\n                    )\nprint( round(bill_length_var, 3) )\n\nbill_length_mm    29.807\ndtype: float64"
  },
  {
    "objectID": "Tools/Tools3.slides.html#sample-standard-deviation",
    "href": "Tools/Tools3.slides.html#sample-standard-deviation",
    "title": "Data Types and Visualization",
    "section": "Sample standard deviation",
    "text": "Sample standard deviation\nA drawback of the sample variance is that it is not on the same scale as the actual observations.\nTo obtain a measure of spread whose units are the same as those of the sample, we simply take the squared root of the sample variance\n\\[\ns = \\left(\\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right)^{1/2}\n\\]\nThis quantity is known as the sample standard deviation. It is in the same units as the observations."
  },
  {
    "objectID": "Tools/Tools3.slides.html#section-4",
    "href": "Tools/Tools3.slides.html#section-4",
    "title": "Data Types and Visualization",
    "section": "",
    "text": "In Python, the sample variance is calculated using the function agg() with “std”.\n\nbill_length_std = (penguins_data\n                    .filter(['bill_length_mm'], axis = 1)\n                    .agg(\"std\")\n                    )\nprint( round(bill_length_std, 3) )\n\nbill_length_mm    5.46\ndtype: float64"
  },
  {
    "objectID": "Tools/Tools3.slides.html#sample-quartiles",
    "href": "Tools/Tools3.slides.html#sample-quartiles",
    "title": "Data Types and Visualization",
    "section": "Sample quartiles",
    "text": "Sample quartiles\n\nThe sample median is the middle number of the ordered data values.\n\nSample quartiles divide the data as nearly as possible into quarters:\n\nFirst quartile (\\(Q_1\\)) is the median of the lower half of the data.\nSecond quartile (\\(Q_2\\)) is the median of the data.\nThird quartile (\\(Q_3\\)) is the median of the upper half of the data."
  },
  {
    "objectID": "Tools/Tools3.slides.html#section-6",
    "href": "Tools/Tools3.slides.html#section-6",
    "title": "Data Types and Visualization",
    "section": "",
    "text": "In Python, the quartiles are calculated using the function quantile().\n\n# Set the quantiles.\nset_quantiles = [0.25, 0.5, 0.75]\n# Compute the quantiles.\n(penguins_data\n .filter(['bill_length_mm'], axis = 1)\n .agg(\"quantile\", q = set_quantiles)\n)\n\n\n\n\n\n\n\n\nbill_length_mm\n\n\n\n\n0.25\n39.225\n\n\n0.50\n44.450\n\n\n0.75\n48.500"
  },
  {
    "objectID": "Tools/Tools3.slides.html#sample-maximum-and-minimum",
    "href": "Tools/Tools3.slides.html#sample-maximum-and-minimum",
    "title": "Data Types and Visualization",
    "section": "Sample maximum and minimum",
    "text": "Sample maximum and minimum\nOther relevant summary statistics are the maximum and minimum, which are calculated using the functions max() and min(), respectively.\n\nbill_length_max = (penguins_data\n                   .filter(['bill_length_mm'], axis = 1)\n                   .agg(\"max\")\n                  )\nprint(bill_length_max)\n\nbill_length_mm    59.6\ndtype: float64\n\n\n\nbill_length_min = (penguins_data\n                   .filter(['bill_length_mm'], axis = 1)\n                   .agg(\"min\")\n                  )\nprint(bill_length_min)\n\nbill_length_mm    32.1\ndtype: float64"
  },
  {
    "objectID": "Tools/Tools3.slides.html#summary-statistics-for-categorical-data",
    "href": "Tools/Tools3.slides.html#summary-statistics-for-categorical-data",
    "title": "Data Types and Visualization",
    "section": "Summary statistics for categorical data",
    "text": "Summary statistics for categorical data\n\nThe most commonly used statistical summaries for categorical data are:\n\nThe frequency of a category is the number of observations that belong to that category.\nThe relative frequency is the frequency divided by the total number of observations."
  },
  {
    "objectID": "Tools/Tools3.slides.html#frequency-table",
    "href": "Tools/Tools3.slides.html#frequency-table",
    "title": "Data Types and Visualization",
    "section": "Frequency table",
    "text": "Frequency table\nSummarizes a categorical variable by counting the values per category.\n\n\n\n(penguins_data\n  .filter(['species'], axis = 1)\n  .value_counts()\n)  \n\nspecies  \nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\n\n\n\n\n\nSpecie\nFrequency\n\n\n\n\nAdelie\n152\n\n\nChinstrap\n68\n\n\nGentoo\n124\n\n\nTotal\n344\n\n\n\n\n\nFrequency: Number of observations in each category.\nTotal: Total sum of observations.\n\n\n\nVentajas de las frequencias.\nResumen claro y conciso de los datos categóricos.\nFacilita la identificación de patrones y tendencias.\nAyuda en la toma de decisiones informadas."
  },
  {
    "objectID": "Tools/Tools3.slides.html#relative-frequency-table",
    "href": "Tools/Tools3.slides.html#relative-frequency-table",
    "title": "Data Types and Visualization",
    "section": "Relative Frequency Table",
    "text": "Relative Frequency Table\nSummarizes a categorical variable by calculating the proportion of values per category.\n\n\n\n(penguins_data\n .filter(['species'], axis = 1)\n .value_counts(normalize = True)\n)\n\nspecies  \nAdelie       0.441860\nGentoo       0.360465\nChinstrap    0.197674\nName: proportion, dtype: float64\n\n\n\n\n\n\nSpecie\nRelative Frequency\n\n\n\n\nAdelie\n0.4418605\n\n\nChinstrap\n0.1976744\n\n\nGentoo\n0.3604651\n\n\nSum\n1\n\n\n\n\n\nRelative frequency: Number of observations in each category divided by the total.\n\n\nLa ventaja de la frequencia relativa es que se puede interpretar como una probabilidad. Lo que da mas información."
  },
  {
    "objectID": "Tools/Tools3.slides.html#example-2",
    "href": "Tools/Tools3.slides.html#example-2",
    "title": "Data Types and Visualization",
    "section": "Example 2",
    "text": "Example 2\n\nA criminologist is developing a rule-based system to classify the types of glasses encountered in criminal investigations.\nThe data consist of 214 glass samples labeled as one of seven class categories.\nThere are nine predictors, including refractive index and percentages of eight elements: Na, Mg, AL, Is, K, Ca, Ba, and Fe. The response is the type of glass."
  },
  {
    "objectID": "Tools/Tools3.slides.html#section-7",
    "href": "Tools/Tools3.slides.html#section-7",
    "title": "Data Types and Visualization",
    "section": "",
    "text": "The dataset is in the file “glass.xlsx”. Let’s load it using pandas.\n\n# Load the Excel file into a pandas DataFrame.\nglass_data = pd.read_excel(\"glass.xlsx\")\n\n\nThe variable Type is categorical. So, let’s ensure Python knows this using the code below.\n\nglass_data['Type'] = pd.Categorical(glass_data['Type'])"
  },
  {
    "objectID": "Tools/Tools3.slides.html#matplotlib-library",
    "href": "Tools/Tools3.slides.html#matplotlib-library",
    "title": "Data Types and Visualization",
    "section": "matplotlib library",
    "text": "matplotlib library\n\nmatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python\nIt is widely used in the data science community for plotting data in various formats\nIdeal for creating simple visualizations like line plots, bar charts, scatter plots, and more\nhttps://matplotlib.org/"
  },
  {
    "objectID": "Tools/Tools3.slides.html#seaborn-library",
    "href": "Tools/Tools3.slides.html#seaborn-library",
    "title": "Data Types and Visualization",
    "section": "seaborn library",
    "text": "seaborn library\n\nseaborn is a Python library built on top of Matplotlib\nDesigned to make statistical data visualization easy and beautiful\nIdeal for creating informative and attractive visualizations with minimal code\nhttps://seaborn.pydata.org/index.html"
  },
  {
    "objectID": "Tools/Tools3.slides.html#importing-the-libraries",
    "href": "Tools/Tools3.slides.html#importing-the-libraries",
    "title": "Data Types and Visualization",
    "section": "Importing the libraries",
    "text": "Importing the libraries\n\nThe matplotlib and seaborn libraries are pre-installed in Google Colab. However, we need to inform Google Colab that we want to use them and its functions using the following command:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nSimilar to pandas, the command as sns allows us to have a short name for seaborn. Similarly, we rename matplotlib as plt."
  },
  {
    "objectID": "Tools/Tools3.slides.html#histogram",
    "href": "Tools/Tools3.slides.html#histogram",
    "title": "Data Types and Visualization",
    "section": "Histogram",
    "text": "Histogram\n\nGraphical display that gives an idea of the “shape” of the sample, indicating regions where sample points are concentrated and regions where they are sparse.\n\nThe bars of the histogram touch each other. A space indicates that there are no observations in that interval."
  },
  {
    "objectID": "Tools/Tools3.slides.html#histogram-of-na",
    "href": "Tools/Tools3.slides.html#histogram-of-na",
    "title": "Data Types and Visualization",
    "section": "Histogram of Na",
    "text": "Histogram of Na\nTo create a histogram, we use the function histplot() from seabron.\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for figure.\nsns.histplot(data = glass_data, x = 'Na') # Create the histogram.\nplt.title(\"Histogram of Na\") # Plot title.\nplt.xlabel(\"Na\") # X label\nplt.show() # Display the plot"
  },
  {
    "objectID": "Tools/Tools3.slides.html#box-plot",
    "href": "Tools/Tools3.slides.html#box-plot",
    "title": "Data Types and Visualization",
    "section": "Box plot",
    "text": "Box plot\n\nA box plot is a graphic that presents the median, the first and third quartiles, and any “outliers” present in the sample.\n\nThe interquartile range (IQR) is the difference between the third quartile and the first quartile (\\(Q_3 - Q_1\\)). This is the distance needed to span the middle half of the data."
  },
  {
    "objectID": "Tools/Tools3.slides.html#anatomy-of-a-box-plot",
    "href": "Tools/Tools3.slides.html#anatomy-of-a-box-plot",
    "title": "Data Types and Visualization",
    "section": "Anatomy of a box plot",
    "text": "Anatomy of a box plot\n\nSee also https://towardsdatascience.com/why-1-5-in-iqr-method-of-outlier-detection-5d07fdc82097"
  },
  {
    "objectID": "Tools/Tools3.slides.html#box-plot-of-na",
    "href": "Tools/Tools3.slides.html#box-plot-of-na",
    "title": "Data Types and Visualization",
    "section": "Box plot of Na",
    "text": "Box plot of Na\nTo create a boxplot, we use the function boxplot() from seabron.\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for the figure.\nsns.boxplot(data = glass_data, y = 'Na') # Create boxplot.\nplt.title(\"Box plot of Na\") # Add title.\nplt.show() # Show the plot."
  },
  {
    "objectID": "Tools/Tools3.slides.html#outliers",
    "href": "Tools/Tools3.slides.html#outliers",
    "title": "Data Types and Visualization",
    "section": "Outliers",
    "text": "Outliers\n\nOutliers are points that are much larger or smaller than the rest of the sample points.\nOutliers may be data entry errors or they may be points that really are different from the rest.\nOutliers should not be deleted without considerable thought—sometimes calculations and analyses will be done with and without outliers and then compared."
  },
  {
    "objectID": "Tools/Tools3.slides.html#scatter-plot",
    "href": "Tools/Tools3.slides.html#scatter-plot",
    "title": "Data Types and Visualization",
    "section": "Scatter plot",
    "text": "Scatter plot\n\nData for which items consists of a pair of numeric values is called bivariate. The graphical summary for bivariate data is a scatterplot.\nThe variables \\(X\\) and \\(Y\\) are placed on the horizontal and vertical axes, respectively. Each point on the graph marks the position of a pair of values of \\(X\\) and \\(Y\\).\nA scatterplot allows us to explore lineal and nonlinear relationships between two variables."
  },
  {
    "objectID": "Tools/Tools3.slides.html#scatter-plot-of-na-versus-ri",
    "href": "Tools/Tools3.slides.html#scatter-plot-of-na-versus-ri",
    "title": "Data Types and Visualization",
    "section": "Scatter plot of Na versus RI",
    "text": "Scatter plot of Na versus RI\nTo create a scatter plot, we use the function scatter() from seabron. In this function, you must state the\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for the plot.\nsns.scatterplot(data = glass_data, x = 'Na', y = 'RI') # Show the plot.\nplt.title(\"Scatter plot of Na vs RI\") # Set plot title.\nplt.xlabel(\"Na\") # Set label for X axis.\nplt.ylabel(\"RI\") # Set label for Y axis.\nplt.show() # Show plot."
  },
  {
    "objectID": "Tools/Tools3.slides.html#bar-charts",
    "href": "Tools/Tools3.slides.html#bar-charts",
    "title": "Data Types and Visualization",
    "section": "Bar charts",
    "text": "Bar charts\nBar charts are commonly used to describe qualitative data classified into various categories based on sector, region, different time periods, or other such factors.\nDifferent sectors, different regions, or different time periods are then labeled as specific categories.\nA bar chart is constructed by creating categories that are represented by labeling each category and which are represented by intervals of equal length on a horizontal axis.\nThe count or frequency within the corresponding category is represented by a bar of height proportional to the frequency."
  },
  {
    "objectID": "Tools/Tools3.slides.html#section-8",
    "href": "Tools/Tools3.slides.html#section-8",
    "title": "Data Types and Visualization",
    "section": "",
    "text": "We create the bar chart using the function countplot() from seaborn.\n\n\nCode\n# Create plot.\nplt.figure(figsize=(7,4)) # Create space for the plot.\nsns.countplot(data = glass_data, x = 'Type') # Show the plot.\nplt.title(\"Bar chart of Type of Glasses\") # Set plot title.\nplt.ylabel(\"Frequency\") # Set label for Y axis.\nplt.show() # Show plot."
  },
  {
    "objectID": "Tools/Tools3.slides.html#saving-plots",
    "href": "Tools/Tools3.slides.html#saving-plots",
    "title": "Data Types and Visualization",
    "section": "Saving plots",
    "text": "Saving plots\n\nWe save a figure using the save.fig function from matplotlib. The dpi argument of this function sets the resolution of the image. The higher the dpi, the better the resolution.\n\nplt.figure(figsize=(5, 7))\nsns.countplot(data = glass_data, x = 'Type')\nplt.title('Frequency of Each Category')\nplt.ylabel('Frequency')\nplt.xlabel('Category')\nplt.savefig('bar_chart.png',dpi=300)"
  },
  {
    "objectID": "Tools/Tools3.slides.html#improving-the-figure",
    "href": "Tools/Tools3.slides.html#improving-the-figure",
    "title": "Data Types and Visualization",
    "section": "Improving the figure",
    "text": "Improving the figure\n\nWe can also use other functions to improve the aspect of the figure:\n\nplt.title(fontsize): Font size of the title.\nplt.ylabel(fontsize): Font size of y axis title.\nplt.xlabel(fontsize): Font size of x axis title.\nplt.yticks(fontsize): Font size of the y axis labels.\nplt.xticks(fontsize): Font size of the x axis labels."
  },
  {
    "objectID": "Tools/Tools3.slides.html#section-9",
    "href": "Tools/Tools3.slides.html#section-9",
    "title": "Data Types and Visualization",
    "section": "",
    "text": "plt.figure(figsize=(5, 5))\nsns.countplot(data = glass_data, x = 'Type')\nplt.title('Relative Frequency of Each Category', fontsize = 12)\nplt.ylabel('Relative Frequency', fontsize = 12)\nplt.xlabel('Category', fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.savefig('bar_chart.png',dpi=300)"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#agenda",
    "href": "IntroductionDS/Introduction.slides.html#agenda",
    "title": "Introduction to Data Science",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction to data science\nBasic notation and terminology"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#data-science-is",
    "href": "IntroductionDS/Introduction.slides.html#data-science-is",
    "title": "Introduction to Data Science",
    "section": "Data science is …",
    "text": "Data science is …\na multidisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from vast amounts of structured and unstructured data."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#in-2004",
    "href": "IntroductionDS/Introduction.slides.html#in-2004",
    "title": "Introduction to Data Science",
    "section": "In 2004 …",
    "text": "In 2004 …\nHurricane Frances was sweeping through the Caribbean and threatening to make a direct hit on Florida’s Atlantic coast.\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidents headed for higher ground, but in Arkansas, Wal-Mart executives saw a big opportunity for one of their newest data-driven weapons: predictive technology."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#section",
    "href": "IntroductionDS/Introduction.slides.html#section",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "A week before the storm made landfall, Linda M. Dillman, Wal-Mart’s chief information officer, pressed her staff to create forecasts based on what had happened when Hurricane Charley hit several weeks earlier.\n\nBacked by trillions of bytes of shopper history stored in Wal-Mart’s data warehouse, she said, the company could “start predicting what’s going to happen, rather than waiting for it to happen,” as she put it."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#the-result",
    "href": "IntroductionDS/Introduction.slides.html#the-result",
    "title": "Introduction to Data Science",
    "section": "The result",
    "text": "The result\n\n\nThe New York Times reported\n\n\n“… Experts analyzed the data and found that stores would indeed need certain products, and not just the usual flashlights.”\n\n\n\nDillman said\n\n\n“We didn’t know in the past that strawberry Pop-Tarts increase their sales, like seven times their normal sales rate, before a hurricane.”"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#the-scheme-of-data-science",
    "href": "IntroductionDS/Introduction.slides.html#the-scheme-of-data-science",
    "title": "Introduction to Data Science",
    "section": "The scheme of data science",
    "text": "The scheme of data science"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#business-understanding",
    "href": "IntroductionDS/Introduction.slides.html#business-understanding",
    "title": "Introduction to Data Science",
    "section": "Business understanding",
    "text": "Business understanding\n\n\nBusiness understanding refers to defining the business problem to be solved.\nThe goal is to reframe the business problem as a data science problem.\nOften, reframing the problem and designing a solution is an iterative process."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#common-data-science-problems",
    "href": "IntroductionDS/Introduction.slides.html#common-data-science-problems",
    "title": "Introduction to Data Science",
    "section": "Common data science problems",
    "text": "Common data science problems\n\nRegression attempts to estimate or predict, for each individual, the numerical value of some variable for that individual. For example, “How much will a given customer use the service?”\n\nClassification (or class probability estimation) attempts to predict, for each individual in a population, which of a (small) set of classes this individual belongs to. For example, “Among all customers of T-Mobile, which are likely to respond to a given offer?”"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#other-problems",
    "href": "IntroductionDS/Introduction.slides.html#other-problems",
    "title": "Introduction to Data Science",
    "section": "Other problems",
    "text": "Other problems\n\nClustering attempts to group individuals in a population together by their similarity, but not driven by any specific purpose. For example, “Do our customers form natural groups or segments?”"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#discussion",
    "href": "IntroductionDS/Introduction.slides.html#discussion",
    "title": "Introduction to Data Science",
    "section": "Discussion",
    "text": "Discussion\n\n\nOften, recasting the problem and designing a solution is an iterative process.\nThe initial formulation may not be complete or optimal, so multiple iterations may be necessary for an acceptable solution formulation.\nThey key to a great success is a creative problem formulation by some analyst regarding how to cast the business problem as one or more data science problems."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#data-understanding-i",
    "href": "IntroductionDS/Introduction.slides.html#data-understanding-i",
    "title": "Introduction to Data Science",
    "section": "Data understanding I",
    "text": "Data understanding I\n\n\nIf the goal is to solve a business problem, the data that makes up the raw material available from which the solution will be built.\nThe available data rarely matches the problem.\nFor example, historical data is often collected for purposes unrelated to the current business problem or for no explicit purpose at all."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#data-understanding-ii",
    "href": "IntroductionDS/Introduction.slides.html#data-understanding-ii",
    "title": "Introduction to Data Science",
    "section": "Data understanding II",
    "text": "Data understanding II\n\n\nThe costs of data vary. Some data will be available for free while others will require effort to obtain.\n\n\nA critical part of the data understanding phase is estimating the costs and benefits of each data source and deciding wether further investment is merited.\nEven after all datasets are acquired, collating them may require additional effort."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#our-goal",
    "href": "IntroductionDS/Introduction.slides.html#our-goal",
    "title": "Introduction to Data Science",
    "section": "Our goal",
    "text": "Our goal\n\n\nOur goal is to turn data into information that answers useful questions."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#data-classes",
    "href": "IntroductionDS/Introduction.slides.html#data-classes",
    "title": "Introduction to Data Science",
    "section": "Data classes",
    "text": "Data classes\n\n\n\nText\n\n\nImages\n\nVideo\n\n\nAudio"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#numerical-data",
    "href": "IntroductionDS/Introduction.slides.html#numerical-data",
    "title": "Introduction to Data Science",
    "section": "Numerical data",
    "text": "Numerical data\n\nData science methodology is based on numerical data given in tables.\n\n\nIn fact, texts, images, videos or audios are transformed into this format to process them.\n\n\nIn this course, we will assume that the data is in a table."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#notation-and-terminology",
    "href": "IntroductionDS/Introduction.slides.html#notation-and-terminology",
    "title": "Introduction to Data Science",
    "section": "Notation and terminology",
    "text": "Notation and terminology\n\nExplanatory variables or predictors:\n\n\\(X\\) represents an explanatory variable or predictor.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a whole collection of \\(p\\) predictors.\n\nOutcome or response:\n\n\\(Y\\) represents the response variable, which we’ll try to predict."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#two-main-problems",
    "href": "IntroductionDS/Introduction.slides.html#two-main-problems",
    "title": "Introduction to Data Science",
    "section": "Two main problems",
    "text": "Two main problems\n\nRegression problems. The response \\(Y\\) is quantitative. For example, a person’s income, the value of a house, the blood pressure of a patient.\nClassification problems. The response \\(Y\\) is qualitative and has \\(K\\) different categories. For example, the brand of a product purchased (A, B, C), or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be qualitative or quantitative."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#example",
    "href": "IntroductionDS/Introduction.slides.html#example",
    "title": "Introduction to Data Science",
    "section": "Example",
    "text": "Example\n\nWhat variables predict the presence of Type II diabetes on a person?\n\n\\(Y\\) is a 1 if a person has Type II diabetes, a 0 if not.\nThe predictors (\\(\\boldsymbol{X}\\)) might include: income, zip code, age, weight, height, gender and race."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#a-statistical-model",
    "href": "IntroductionDS/Introduction.slides.html#a-statistical-model",
    "title": "Introduction to Data Science",
    "section": "A statistical model",
    "text": "A statistical model\n\nA statistical model is a mathematical equation that embodies statistical assumptions concerning the generation of data.\nTechnically, it has the following form:\n\\[Y = f(\\boldsymbol{X}) + \\epsilon \\]\nwhere \\(Y\\) is a quantitative response, \\(f(\\boldsymbol{X})\\) is the function that relates the predictors, \\(\\boldsymbol{X}\\), to the \\(Y\\), and \\(\\epsilon\\) is the (random) error term."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#true-and-estimated-models",
    "href": "IntroductionDS/Introduction.slides.html#true-and-estimated-models",
    "title": "Introduction to Data Science",
    "section": "True and estimated models",
    "text": "True and estimated models\n\n\\(f(\\boldsymbol{X})\\) represents the TRUTH. The true relationship between \\(\\boldsymbol{X}\\) and \\(Y\\).\n\nUnknown\nVery complex\n\n\n\\(\\hat{f}(\\boldsymbol{X})\\) represents an approximation or estimate of the true model constructed using data.\n\nIdeally, interpretable (but not necessarily)"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#two-datasets",
    "href": "IntroductionDS/Introduction.slides.html#two-datasets",
    "title": "Introduction to Data Science",
    "section": "Two datasets",
    "text": "Two datasets\n\n“Training” data are data used to construct \\(\\hat{f}(\\boldsymbol{X})\\).\n“Testing” data are data that were NOT used in the fitting process, but are used to test how well your model performs on unseen data."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#yogi-berra",
    "href": "IntroductionDS/Introduction.slides.html#yogi-berra",
    "title": "Introduction to Data Science",
    "section": "Yogi Berra",
    "text": "Yogi Berra\n\n\nIt’s though to make predictions, especially about the future."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#agenda",
    "href": "UnsupervisedLearning/PCA.slides.html#agenda",
    "title": "Principal Component Analysis",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nDispersion in one or more dimensions\nPrincipal component analysis"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#load-the-libraries",
    "href": "UnsupervisedLearning/PCA.slides.html#load-the-libraries",
    "title": "Principal Component Analysis",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nBefore we start, let’s import the data science libraries into Python.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, and sklearn libraries in Python."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#types-of-learning",
    "href": "UnsupervisedLearning/PCA.slides.html#types-of-learning",
    "title": "Principal Component Analysis",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#unsupervised-learning-methods",
    "href": "UnsupervisedLearning/PCA.slides.html#unsupervised-learning-methods",
    "title": "Principal Component Analysis",
    "section": "Unsupervised learning methods",
    "text": "Unsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#dispersion-in-one-dimension",
    "href": "UnsupervisedLearning/PCA.slides.html#dispersion-in-one-dimension",
    "title": "Principal Component Analysis",
    "section": "Dispersion in one dimension",
    "text": "Dispersion in one dimension\n\nThe concept of principal components requires an understanding of the dispersion or variability of the data.\nSuppose we have data for a single predictor."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#dispersion-in-two-dimensions",
    "href": "UnsupervisedLearning/PCA.slides.html#dispersion-in-two-dimensions",
    "title": "Principal Component Analysis",
    "section": "Dispersion in two dimensions",
    "text": "Dispersion in two dimensions"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#capturing-dispersion",
    "href": "UnsupervisedLearning/PCA.slides.html#capturing-dispersion",
    "title": "Principal Component Analysis",
    "section": "Capturing dispersion",
    "text": "Capturing dispersion\nIn some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#capturing-dispersion-1",
    "href": "UnsupervisedLearning/PCA.slides.html#capturing-dispersion-1",
    "title": "Principal Component Analysis",
    "section": "Capturing dispersion",
    "text": "Capturing dispersion\nIn some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension.\n\n\n\n\n\n\n\n\n\n\nA single predictor \\(X_2\\) captures much of the spread in the data."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#lets-see-another-example",
    "href": "UnsupervisedLearning/PCA.slides.html#lets-see-another-example",
    "title": "Principal Component Analysis",
    "section": "Let’s see another example",
    "text": "Let’s see another example"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#lets-see-another-example-1",
    "href": "UnsupervisedLearning/PCA.slides.html#lets-see-another-example-1",
    "title": "Principal Component Analysis",
    "section": "Let’s see another example",
    "text": "Let’s see another example\n\n\n\n\n\n\n\n\n\n\nA single predictor captures much of the dispersion in the data. In this case, the new predictor has the form \\(Z_1 = a X_1 + b X_2 + c.\\)"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section",
    "href": "UnsupervisedLearning/PCA.slides.html#section",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Alternatively, we can use two alternative dimensions to capture the dispersion."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#a-new-coordinate-system",
    "href": "UnsupervisedLearning/PCA.slides.html#a-new-coordinate-system",
    "title": "Principal Component Analysis",
    "section": "A new coordinate system",
    "text": "A new coordinate system\n\n\n\n\n\nThe new coordinate axis is given by two new predictors, \\(Z_1\\) and \\(Z_2\\). Both are given by linear equations of the new predictors.\nThe first axis, \\(Z_1\\), captures a large portion of the dispersion, while \\(Z_2\\) captures a small portion from another angle.\nThe new axes, \\(Z_1\\) and \\(Z_2\\), are called principal components."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#dimension-reduction",
    "href": "UnsupervisedLearning/PCA.slides.html#dimension-reduction",
    "title": "Principal Component Analysis",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\nPrincipal Components Analysis (PCA) helps us reduce the dimension of the data.\n\nIt creates a new coordinate axis in two (or more) dimensions.\nTechnically, it creates new predictors by combining highly correlated predictors. The new predictors are uncorrelated."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#setup",
    "href": "UnsupervisedLearning/PCA.slides.html#setup",
    "title": "Principal Component Analysis",
    "section": "Setup",
    "text": "Setup\n\nStep 1. We start with a database with \\(n\\) observations and \\(p\\) predictors.\n\n\n\nPredictor 1\nPredictor 2\nPredictor 3\n\n\n\n\n15\n14\n5\n\n\n2\n1\n6\n\n\n10\n3\n17\n\n\n8\n18\n9\n\n\n12\n16\n11"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-1",
    "href": "UnsupervisedLearning/PCA.slides.html#section-1",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 2. We standardize each predictor individually.\n\\[{\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2 }}\\]\n\n\n\n\nPredictor 1\nPredictor 2\nPredictor 3\n\n\n\n\n\n1.15\n0.46\n-0.96\n\n\n\n-1.52\n-1.20\n-0.75\n\n\n\n0.12\n-0.95\n1.55\n\n\n\n-0.29\n0.97\n-0.13\n\n\n\n0.53\n0.72\n0.29\n\n\nSum\n0\n0\n0\n\n\nVariance\n1\n1\n1"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-2",
    "href": "UnsupervisedLearning/PCA.slides.html#section-2",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 3. We assume that the standardized database is an \\(n\\times p\\) matrix \\(\\mathbf{X}\\).\n\\[\\mathbf{X} = \\begin{pmatrix}\n1.15    &   0.46    &   -0.96   \\\\\n-1.52   &   -1.20   &   -0.75   \\\\\n0.12    &   -0.95   &   1.55    \\\\\n-0.29   &   0.97    &   -0.13   \\\\\n0.53    &   0.72    &   0.29    \\\\\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#algorithm",
    "href": "UnsupervisedLearning/PCA.slides.html#algorithm",
    "title": "Principal Component Analysis",
    "section": "Algorithm",
    "text": "Algorithm\n\nThe PCA algorithm has its origins in linear algebra.\n\nIts basic idea is:\n\nCreate a matrix \\(\\mathbf{C}\\) with the correlations between the predictors of the matrix \\(\\mathbf{X}\\).\nSplit the matrix \\(\\mathbf{C}\\) into three parts, which give us the new coordinate axis and the importance of each axis."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#correlation-matrix",
    "href": "UnsupervisedLearning/PCA.slides.html#correlation-matrix",
    "title": "Principal Component Analysis",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\nContinuing with our example, the correlation matrix contains the correlations between two columns of \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#partitioning-the-correlation-matrix",
    "href": "UnsupervisedLearning/PCA.slides.html#partitioning-the-correlation-matrix",
    "title": "Principal Component Analysis",
    "section": "Partitioning the correlation matrix",
    "text": "Partitioning the correlation matrix\n\nThe \\(\\mathbf{C}\\) matrix is partitioned using the eigenvalue and eigenvector decomposition method."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-3",
    "href": "UnsupervisedLearning/PCA.slides.html#section-3",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The columns of \\(\\mathbf{B}\\) define the axes of the new coordinate system. These axes are called principal components.\nThe diagonal values in \\(\\mathbf{A}\\) define the individual importance of each principal component (axis)."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#proportion-of-the-dispersion-explained-by-the-component",
    "href": "UnsupervisedLearning/PCA.slides.html#proportion-of-the-dispersion-explained-by-the-component",
    "title": "Principal Component Analysis",
    "section": "Proportion of the dispersion explained by the component",
    "text": "Proportion of the dispersion explained by the component\n\n\n\n\n\n\\[\\mathbf{A} = \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.33    \\\\\n\\end{pmatrix}\\]\n\n\n\n\nThe proportion of the dispersion in the data that is captured by the first component is \\(\\frac{a_{1,1}}{p} = \\frac{1.60}{3} = 0.53\\)."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-4",
    "href": "UnsupervisedLearning/PCA.slides.html#section-4",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "\\[\\mathbf{A} = \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.33    \\\\\n\\end{pmatrix}\\]\n\n\n\n\nThe proportion captured by the second component is \\(\\frac{a_{2,2}}{p} = \\frac{1.07}{3} = 0.36\\).\nThe proportion captured by the third component is \\(\\frac{a_{3,3}}{p} = \\frac{0.33}{3} = 0.11.\\)"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#comments",
    "href": "UnsupervisedLearning/PCA.slides.html#comments",
    "title": "Principal Component Analysis",
    "section": "Comments",
    "text": "Comments\n\nPrincipal components can be used to approximate a matrix.\nFor example, we can approximate the matrix \\(\\mathbf{C}\\) by setting the third component equal to zero.\n\n\\[\\begin{pmatrix}\n-0.68   &   0.35    &   0.00    \\\\\n-0.72   &   -0.13   &   0.00    \\\\\n0.16    &   0.93    &   0.00\\\\\n\\end{pmatrix} \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.00    \\\\\n\\end{pmatrix} \\begin{pmatrix}\n-0.68   &   -0.72   &   0.16    \\\\\n0.35    &   -0.13   &   0.93    \\\\\n0.00    &   0.00    &   0.00    \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n0.86    &   0.73    &   0.18    \\\\\n0.73    &   0.85    &   -0.30   \\\\\n0.18    &   -0.30   &   0.96    \\\\\n\\end{pmatrix}\\]\n\n\n\\[\\approx \\begin{pmatrix}\n1.00    &   0.58    &   0.11    \\\\\n0.58    &   1.00    &   -0.23   \\\\\n0.11    &   -0.23   &   1.00    \\\\\n\\end{pmatrix} = \\mathbf{C}\\]"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-5",
    "href": "UnsupervisedLearning/PCA.slides.html#section-5",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Approximations are useful for storing large matrices.\nThis is because we only need to store the largest eigenvalues and their corresponding eigenvectors to recover a high-quality approximation of the entire matrix.\nThis is the idea behind image compression."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#example-1",
    "href": "UnsupervisedLearning/PCA.slides.html#example-1",
    "title": "Principal Component Analysis",
    "section": "Example 1",
    "text": "Example 1\n\nConsider a database of the 100 most popular songs on TikTok. The data is in the file “TikTok 2020 reduced.xlsx”. There are observations of several predictors, such as:\n\nDanceability describes how suitable a track is for dancing based on a combination of musical elements.\nEnergy is a measure from 0 to 1 and represents a perceptual measure of intensity and activity.\nThe overall volume of a track in decibels (dB). Loudness values are averaged across the entire track."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-6",
    "href": "UnsupervisedLearning/PCA.slides.html#section-6",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Other predictors are:\n\nSpeech detects the presence of spoken words in a track. The more exclusively speech-like the recording is.\nA confidence measure from 0 to 1 about whether the track is acoustic.\nDetects the presence of an audience in the recording.\nA measure from 0 to 1 that describes the musical positivity a track conveys."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#the-data",
    "href": "UnsupervisedLearning/PCA.slides.html#the-data",
    "title": "Principal Component Analysis",
    "section": "The data",
    "text": "The data\n\ntiktok_data = pd.read_excel(\"TikTok_Songs_2020_Reduced.xlsx\")\ntiktok_data.head()\n\n\n\n\n\n\n\n\ntrack_name\nartist_name\nalbum\ndanceability\nenergy\nloudness\nspeechiness\nacousticness\nliveness\nvalence\ntempo\n\n\n\n\n0\nSay So\nDoja Cat\nHot Pink\n0.787\n0.673\n-4.583\n0.1590\n0.26400\n0.0904\n0.779\n110.962\n\n\n1\nBlinding Lights\nThe Weeknd\nAfter Hours\n0.514\n0.730\n-5.934\n0.0598\n0.00146\n0.0897\n0.334\n171.005\n\n\n2\nSupalonely (feat. Gus Dapperton)\nBENEE\nHey u x\n0.862\n0.631\n-4.746\n0.0515\n0.29100\n0.1230\n0.841\n128.978\n\n\n3\nSavage\nMegan Thee Stallion\nSuga\n0.843\n0.741\n-5.609\n0.3340\n0.02520\n0.0960\n0.680\n168.983\n\n\n4\nMoral of the Story\nAshe\nMoral of the Story\n0.572\n0.406\n-8.624\n0.0427\n0.58700\n0.1020\n0.265\n119.812"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#standardize-the-data",
    "href": "UnsupervisedLearning/PCA.slides.html#standardize-the-data",
    "title": "Principal Component Analysis",
    "section": "Standardize the data",
    "text": "Standardize the data\n\nRemember that PCA works with distances, so we must standardize the quantitative predictors to have an accurate analysis.\n\n# Select the predictors\nfeatures = ['danceability', 'energy', 'loudness', 'speechiness',\n            'acousticness', 'liveness', 'valence', 'tempo']\nX_tiktok = tiktok_data.filter(features)  \n\n# Standardize the data\nscaler = StandardScaler()\nXs_tiktok = scaler.fit_transform(X_tiktok)"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#pca-in-python",
    "href": "UnsupervisedLearning/PCA.slides.html#pca-in-python",
    "title": "Principal Component Analysis",
    "section": "PCA in Python",
    "text": "PCA in Python\n\nWe tell Python that we want to apply PCA using the function PCA() from sklearn. Next, we run the algorithm using .fit_transform().\n\npca = PCA()\nPCA_tiktok = pca.fit_transform(Xs_tiktok)"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-7",
    "href": "UnsupervisedLearning/PCA.slides.html#section-7",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The Screen or Summary Plot tells you the variability captured by each component. This variability is given by the Eigenvalue. From 1 to 8 components.\nThe first component covers most of the data dispersion.\nThis graph is used to define the total number of components to use."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-8",
    "href": "UnsupervisedLearning/PCA.slides.html#section-8",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The code to generate a scree plot is below.\n\nexplained_var = pca.explained_variance_ratio_\n\nplt.figure(figsize=(5, 5))\nplt.plot(range(1, len(explained_var) + 1), explained_var, \n         marker='o', linestyle='-')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.xticks(range(1, len(explained_var) + 1))\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#biplot",
    "href": "UnsupervisedLearning/PCA.slides.html#biplot",
    "title": "Principal Component Analysis",
    "section": "Biplot",
    "text": "Biplot\n\n\n\n\nDisplays the graphical observations on the new coordinate axis given by the first two components.\nHelps visualize data for three or more predictors using a two-dimensional scatter plot.\nA red line indicates the growth direction of the labeled variable."
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-9",
    "href": "UnsupervisedLearning/PCA.slides.html#section-9",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The code to generate the biplot is lenghty but it can be broken into three steps.\nStep 1. Create a DataFrame with the PCA results\n\npca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\npca_df.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\n\n\n\n\n0\n1.103065\n0.558086\n-0.800688\n0.446496\n0.605944\n-0.044089\n0.287325\n-0.413604\n\n\n1\n0.805080\n-0.766973\n1.580513\n-2.215856\n0.359655\n0.708123\n-0.882761\n0.113058\n\n\n2\n1.330433\n0.728161\n-0.288982\n0.376298\n0.786185\n-1.134308\n0.178388\n-0.242497\n\n\n3\n1.496277\n2.095014\n1.351398\n-0.621691\n0.390949\n0.494101\n0.024648\n-0.080720\n\n\n4\n-1.973362\n-0.966108\n-0.302071\n-1.266269\n0.414639\n-0.335677\n-0.076711\n-0.140126"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-10",
    "href": "UnsupervisedLearning/PCA.slides.html#section-10",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 2. Create biplot of first two principal components\n\n\nCode\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.show()"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-11",
    "href": "UnsupervisedLearning/PCA.slides.html#section-11",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 3. Add more information to the biplot.\n\n\nCode\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n\n# Add variable vectors\nloadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n              color='red', alpha=0.5, head_width=0.05)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n\nplt.show()"
  },
  {
    "objectID": "UnsupervisedLearning/PCA.slides.html#section-12",
    "href": "UnsupervisedLearning/PCA.slides.html#section-12",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "With some extra lines of code, we label the points in the plot.\n\n\nCode\npca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\npca_df = (pca_df\n          .assign(songs = tiktok_data['track_name'])\n          )\n\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n\n# Add labels for each song\nfor i in range(pca_df.shape[0]):\n    plt.text(pca_df['PC1'][i] + 0.1, pca_df['PC2'][i] + 0.1,\n             pca_df['songs'][i], fontsize=8, alpha=0.7)\n\n\n# Add variable vectors\nloadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n              color='red', alpha=0.5, head_width=0.05)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n\nplt.show()"
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#before-we-start",
    "href": "PreProcessing/TrainTestData.slides.html#before-we-start",
    "title": "Train, Test, and Validation Datasets",
    "section": "Before we start,",
    "text": "Before we start,\n\nLet’s import scikit-learn into Python together with the other relevant libraries.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nWe will not use all the functions from the scikit-learn library. Instead, we will use specific functions from the sub-library model_selection."
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#recall-that",
    "href": "PreProcessing/TrainTestData.slides.html#recall-that",
    "title": "Train, Test, and Validation Datasets",
    "section": "Recall that …",
    "text": "Recall that …\n\nIn data science, we assume that\n\\[Y = f(\\boldsymbol{X}) + \\epsilon\\]\nwhere \\(f(\\boldsymbol{X})\\) represents the true relationship between \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) and \\(Y\\).\n\n\\(f(\\boldsymbol{X})\\) is unknown and very complex!"
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#two-datasets",
    "href": "PreProcessing/TrainTestData.slides.html#two-datasets",
    "title": "Train, Test, and Validation Datasets",
    "section": "Two datasets",
    "text": "Two datasets\n\nThe application of data science models needs two data sets:\n\nTraining data is data that we use to train or construct the estimated function \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is data that we use to evaluate the predictive performance of \\(\\hat{f}(\\boldsymbol{X})\\) only."
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#section",
    "href": "PreProcessing/TrainTestData.slides.html#section",
    "title": "Train, Test, and Validation Datasets",
    "section": "",
    "text": "A random sample of \\(n\\) observations.\nUse it to construct \\(\\hat{f}(\\boldsymbol{X})\\).\n\n\n\n\n\nAnother random sample of \\(n_t\\) observations, which is independent of the training data.\nUse it to evaluate \\(\\hat{f}(\\boldsymbol{X})\\)."
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#validation-dataset",
    "href": "PreProcessing/TrainTestData.slides.html#validation-dataset",
    "title": "Train, Test, and Validation Datasets",
    "section": "Validation Dataset",
    "text": "Validation Dataset\nIn many practical situations, a test dataset is not available. To overcome this issue, we use a validation dataset.\n\n\nIdea: Apply model to your validation dataset to mimic what will happen when you apply it to test dataset."
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#example",
    "href": "PreProcessing/TrainTestData.slides.html#example",
    "title": "Train, Test, and Validation Datasets",
    "section": "Example",
    "text": "Example\n\nThe “BostonHousing.xlsx” contains data collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes data on 506 census housing tracts in the Boston area in 1970s.\nThe goal is to predict the median house price in new tracts based on information such as crime rate, pollution, and number of rooms.\nThe response is the median value of owner-occupied homes in $1000s, contained in the column MEDV."
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#the-predictors",
    "href": "PreProcessing/TrainTestData.slides.html#the-predictors",
    "title": "Train, Test, and Validation Datasets",
    "section": "The predictors",
    "text": "The predictors\n\n\nCRIM: per capita crime rate by town.\nZN: proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS: proportion of non-retail business acres per town.\nCHAS: Charles River (‘Yes’ if tract bounds river; ‘No’ otherwise).\nNOX: nitrogen oxides concentration (parts per 10 million).\nRM: average number of rooms per dwelling.\nAGE: proportion of owner-occupied units built prior to 1940.\nDIS: weighted mean of distances to five Boston employment centers\nRAD: index of accessibility to radial highways (‘Low’, ‘Medium’, ‘High’).\nTAX: full-value property-tax rate per $10,000.\nPTRATIO: pupil-teacher ratio by town.\nLSTAT: lower status of the population (percent)."
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#read-the-dataset",
    "href": "PreProcessing/TrainTestData.slides.html#read-the-dataset",
    "title": "Train, Test, and Validation Datasets",
    "section": "Read the dataset",
    "text": "Read the dataset\n\nWe read the dataset and set the variable CHAS and RAD as categorical.\n\nBoston_data = pd.read_excel('BostonHousing.xlsx')\n\nBoston_data['CHAS'] = pd.Categorical(Boston_data['CHAS'])\nBoston_data['RAD'] = pd.Categorical(Boston_data['RAD'], \n                                      categories=[\"Low\", \"Medium\", \"High\"], \n                                      ordered=True)"
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#section-1",
    "href": "PreProcessing/TrainTestData.slides.html#section-1",
    "title": "Train, Test, and Validation Datasets",
    "section": "",
    "text": "Boston_data.head()\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\nNo\n0.538\n6.575\n65.2\n4.0900\nLow\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\nNo\n0.469\n6.421\n78.9\n4.9671\nLow\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\nNo\n0.469\n7.185\n61.1\n4.9671\nLow\n242\n17.8\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\nNo\n0.458\n6.998\n45.8\n6.0622\nLow\n222\n18.7\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\nNo\n0.458\n7.147\n54.2\n6.0622\nLow\n222\n18.7\n5.33\n36.2"
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#how-do-we-generate-validation-data",
    "href": "PreProcessing/TrainTestData.slides.html#how-do-we-generate-validation-data",
    "title": "Train, Test, and Validation Datasets",
    "section": "How do we generate validation data?",
    "text": "How do we generate validation data?\nWe split the current dataset into a training and a validation dataset. To this end, we use the function train_test_split() from scikit-learn.\n\nThe function has three main inputs:\n\nA pandas dataframe with the predictor columns only.\nA pandas dataframe with the response column only.\nThe parameter test_size which sets the portion of the dataset that will go to the validation set."
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#create-the-predictor-matrix",
    "href": "PreProcessing/TrainTestData.slides.html#create-the-predictor-matrix",
    "title": "Train, Test, and Validation Datasets",
    "section": "Create the predictor matrix",
    "text": "Create the predictor matrix\nWe use the function .drop() from pandas. This function drops one or more columns from a data frame. Let’s drop the response column MEDV and store the result in X_full.\n\n# Set full matrix of predictors.\nX_full = Boston_data.drop(columns = ['MEDV']) \nX_full.head(4)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\n\n\n\n\n0\n0.00632\n18.0\n2.31\nNo\n0.538\n6.575\n65.2\n4.0900\nLow\n296\n15.3\n4.98\n\n\n1\n0.02731\n0.0\n7.07\nNo\n0.469\n6.421\n78.9\n4.9671\nLow\n242\n17.8\n9.14\n\n\n2\n0.02729\n0.0\n7.07\nNo\n0.469\n7.185\n61.1\n4.9671\nLow\n242\n17.8\n4.03\n\n\n3\n0.03237\n0.0\n2.18\nNo\n0.458\n6.998\n45.8\n6.0622\nLow\n222\n18.7\n2.94"
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#create-the-response-column",
    "href": "PreProcessing/TrainTestData.slides.html#create-the-response-column",
    "title": "Train, Test, and Validation Datasets",
    "section": "Create the response column",
    "text": "Create the response column\nWe use the function .filter() from pandas to extract the column MEDV from the data frame. We store the result in Y_full.\n\n# Set full matrix of responses.\nY_full = Boston_data.filter(['MEDV'])\nY_full.head(4)\n\n\n\n\n\n\n\n\nMEDV\n\n\n\n\n0\n24.0\n\n\n1\n21.6\n\n\n2\n34.7\n\n\n3\n33.4"
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#lets-partition-the-dataset",
    "href": "PreProcessing/TrainTestData.slides.html#lets-partition-the-dataset",
    "title": "Train, Test, and Validation Datasets",
    "section": "Let’s partition the dataset",
    "text": "Let’s partition the dataset\n\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n                                                      test_size = 0.3)\n\n\nThe function makes a clever partition of the data using the empirical distribution of the response.\nTechnically, it splits the data so that the distribution of the response under the training and validation sets is similar.\nUsually, the proportion of the dataset that goes to the validation set is 20% or 30%."
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#section-2",
    "href": "PreProcessing/TrainTestData.slides.html#section-2",
    "title": "Train, Test, and Validation Datasets",
    "section": "",
    "text": "The predictors and response in the training dataset are in the objects X_train and Y_train, respectively. We compile these objects into a single dataset using the function .concat() from pandas. The argument axis = 1 tells .concat() to concatenate the datasets by their rows.\n\ntraining_dataset = pd.concat([X_train, Y_train], axis = 1)\ntraining_dataset.head(4)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\n\n\n\n\n394\n13.35980\n0.0\n18.10\nNo\n0.693\n5.887\n94.7\n1.7821\nHigh\n666\n20.2\n16.35\n12.7\n\n\n67\n0.05789\n12.5\n6.07\nNo\n0.409\n5.878\n21.4\n6.4980\nMedium\n345\n18.9\n8.10\n22.0\n\n\n50\n0.08873\n21.0\n5.64\nNo\n0.439\n5.963\n45.7\n6.8147\nMedium\n243\n16.8\n13.45\n19.7\n\n\n505\n0.04741\n0.0\n11.93\nNo\n0.573\n6.030\n80.8\n2.5050\nLow\n273\n21.0\n7.88\n11.9"
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#section-3",
    "href": "PreProcessing/TrainTestData.slides.html#section-3",
    "title": "Train, Test, and Validation Datasets",
    "section": "",
    "text": "Equivalently, the predictors and response in the validation dataset are in the objects X_valid and Y_valid, respectively.\n\nvalidation_dataset = pd.concat([X_valid, Y_valid], axis = 1)\nvalidation_dataset.head()\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\n\n\n\n\n99\n0.06860\n0.0\n2.89\nNo\n0.445\n7.416\n62.5\n3.4952\nLow\n276\n18.0\n6.19\n33.2\n\n\n82\n0.03659\n25.0\n4.86\nNo\n0.426\n6.302\n32.2\n5.4007\nMedium\n281\n19.0\n6.72\n24.8\n\n\n235\n0.33045\n0.0\n6.20\nNo\n0.507\n6.086\n61.5\n3.6519\nHigh\n307\n17.4\n10.88\n24.0\n\n\n251\n0.21409\n22.0\n5.86\nNo\n0.431\n6.438\n8.9\n7.3967\nHigh\n330\n19.1\n3.59\n24.8\n\n\n448\n9.32909\n0.0\n18.10\nNo\n0.713\n6.185\n98.7\n2.2616\nHigh\n666\n20.2\n18.13\n14.1"
  },
  {
    "objectID": "PreProcessing/TrainTestData.slides.html#work-on-your-training-dataset",
    "href": "PreProcessing/TrainTestData.slides.html#work-on-your-training-dataset",
    "title": "Train, Test, and Validation Datasets",
    "section": "Work on your training dataset",
    "text": "Work on your training dataset\nAfter we have partitioned the data, we work on the training data to develop our predictive pipeline.\nThe pipeline has two main steps:\n\nData preprocessing.\nModel development.\n\nWe will now discuss preprocessing techniques applied to the predictor columns in the training dataset.\nNote that all preprocessing techniques will also be applied to the validation dataset and test dataset to prepare it for your model!"
  }
]